# Colab + Drive é¡¹ç›®æ”¹åŠ¨åˆ†ææŠ¥å‘Š

> **åˆ†ææ—¥æœŸ**: 2026-01-26
> **åˆ†æç›®çš„**: ç†è§£é¡¹ç›® Checkpoint æœºåˆ¶æ”¹åŠ¨ï¼Œæ£€æŸ¥é€»è¾‘æ­£ç¡®æ€§

---
åˆ†ææŠ¥å‘Šå·²å®Œæˆå¹¶ä¿å­˜åˆ° claude-colab-drive.mdã€‚ä»¥ä¸‹æ˜¯æ ¸å¿ƒå‘ç°ï¼š

  ---
  æ ¸å¿ƒå›ç­”

  1. save_steps ä¸ EarlyStopping çš„ checkpoint åŒºåˆ«

  | ç‰¹æ€§   | save_steps                        | EarlyStopping             |
  |------|-----------------------------------|---------------------------|
  | è§¦å‘æ—¶æœº | æ¯ N ä¸ªè®­ç»ƒ step                      | æ¯ epoch ç»“æŸä¸” vali_loss åˆ›æ–°ä½ |
  | ä¿å­˜è·¯å¾„ | checkpoint_step_{N}/checkpoint.pt | checkpoint                |
  | ç”¨é€”   | æ–­ç‚¹ç»­è®­                              | ä¿å­˜æœ€ä½³æ¨¡å‹                    |
  | ç›¸äº’å½±å“ | âŒ å®Œå…¨ç‹¬ç«‹ï¼Œäº’ä¸å½±å“                       | âŒ                         |

  2. vali() å¦‚ä½•ä¿ç•™ checkpoint

  å…³é”®ç‚¹: vali() æœ¬èº«ä¸ä¿å­˜ checkpointï¼Œå®ƒåªè®¡ç®—éªŒè¯æŸå¤±ã€‚ä¿å­˜ç”± EarlyStopping å®Œæˆï¼š

  Epoch ç»“æŸ
      â”‚
      â”œâ”€â”€ vali_loss = vali(...)           # åªè®¡ç®—æŸå¤±
      â”‚
      â””â”€â”€ early_stopping(vali_loss, ...)  # åˆ¤æ–­æ˜¯å¦ä¿å­˜
          â””â”€â”€ if vali_loss < best_score â†’ save_checkpoint()

  3. å‘ç°çš„æ½œåœ¨é—®é¢˜

  | é—®é¢˜                             | é£é™©   | è¯´æ˜                                 |
  |--------------------------------|------|------------------------------------|
  | EarlyStopping best_score æœªä¿å­˜   | ğŸŸ  é«˜ | æ–­ç‚¹ç»­è®­åç¬¬ä¸€ä¸ª epoch å¿…å®šä¿å­˜ï¼ˆå¯èƒ½è¦†ç›–æ›´å¥½çš„æ¨¡å‹ï¼‰     |
  | Scheduler ä¸åŒæ­¥                  | ğŸŸ¡ ä¸­ | epoch å†…æ¢å¤æ—¶è·³è¿‡çš„ batch æ²¡æœ‰æ›´æ–° scheduler |
  | run_pretrain.py åˆ é™¤ checkpoints | ğŸŸ  é«˜ | è¯¥æ–‡ä»¶ä»ä¼šåœ¨ç»“æŸæ—¶åˆ é™¤æ‰€æœ‰ checkpoint           |

  4. ä»£ç èƒ½å¦æ­£ç¡®è¿è¡Œ

  âœ… å¯ä»¥æ­£ç¡®è¿è¡Œï¼Œæ ¸å¿ƒé€»è¾‘æ— è¯¯ã€‚ä¸Šè¿°é—®é¢˜æ˜¯è¾¹ç¼˜æƒ…å†µï¼Œä¸å½±å“åŸºæœ¬åŠŸèƒ½ã€‚
## ç›®å½•

1. [é¡¹ç›®æ”¹åŠ¨æ¦‚è¿°](#1-é¡¹ç›®æ”¹åŠ¨æ¦‚è¿°)
2. [ä¸¤ç§ Checkpoint æœºåˆ¶å¯¹æ¯”](#2-ä¸¤ç§-checkpoint-æœºåˆ¶å¯¹æ¯”)
3. [ä»£ç æ‰§è¡Œæµç¨‹è¯¦è§£](#3-ä»£ç æ‰§è¡Œæµç¨‹è¯¦è§£)
4. [å…³é”®é—®é¢˜è§£ç­”](#4-å…³é”®é—®é¢˜è§£ç­”)
5. [æ½œåœ¨é—®é¢˜ä¸é£é™©](#5-æ½œåœ¨é—®é¢˜ä¸é£é™©)
6. [è¿è¡Œæ­£ç¡®æ€§éªŒè¯](#6-è¿è¡Œæ­£ç¡®æ€§éªŒè¯)

---

## 1. é¡¹ç›®æ”¹åŠ¨æ¦‚è¿°

### 1.1 æ–°å¢å‚æ•° (`run_main.py:104-109`)

```python
parser.add_argument('--save_steps', type=int, default=0,
                    help='save checkpoint every N steps (0=disable)')
parser.add_argument('--resume_from_checkpoint', type=str, default='',
                    help='path to checkpoint directory or checkpoint.pt to resume')
parser.add_argument('--save_total_limit', type=int, default=0,
                    help='keep only the most recent N step checkpoints (0=disable)')
```

### 1.2 æ”¹åŠ¨çš„æ–‡ä»¶

| æ–‡ä»¶ | æ”¹åŠ¨å†…å®¹ |
|------|----------|
| `run_main.py` | æ–°å¢ step çº§ä¿å­˜ã€æ–­ç‚¹ç»­è®­é€»è¾‘ |
| `utils/tools.py` | EarlyStopping ä¿å­˜å®Œæ•´ dictï¼ˆå« optimizer/scheduler/epoch/global_stepï¼‰ |
| `run_m4.py:244-247` | åŠ è½½ checkpoint æ—¶å…¼å®¹æ–°æ ¼å¼ |
| `run_pretrain.py:248-249` | EarlyStopping è°ƒç”¨æ—¶ä¼ å…¥å®Œæ•´å‚æ•° |

### 1.3 Checkpoint ä¿å­˜æ ¼å¼ç»Ÿä¸€

**æ—§æ ¼å¼**:
```python
torch.save(model.state_dict(), path)
```

**æ–°æ ¼å¼**:
```python
ckpt = {
    'model': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'scheduler': scheduler.state_dict(),
    'epoch': epoch,
    'global_step': global_step,
}
torch.save(ckpt, path)
```

---

## 2. ä¸¤ç§ Checkpoint æœºåˆ¶å¯¹æ¯”

### 2.1 æœºåˆ¶æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Checkpoint ä¿å­˜æœºåˆ¶å¯¹æ¯”                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  [A] save_steps æœºåˆ¶ (run_main.py:294-321)                         â”‚
â”‚      â”œâ”€â”€ è§¦å‘æ—¶æœº: æ¯ N ä¸ª stepï¼ˆè®­ç»ƒè¿­ä»£ï¼‰                          â”‚
â”‚      â”œâ”€â”€ ä¿å­˜ä½ç½®: checkpoint_step_{global_step}/checkpoint.pt      â”‚
â”‚      â”œâ”€â”€ ä¿å­˜å†…å®¹: model + optimizer + scheduler + epoch + step     â”‚
â”‚      â””â”€â”€ ç”¨é€”: æ–­ç‚¹ç»­è®­ï¼Œé˜²æ­¢æ„å¤–ä¸­æ–­ä¸¢å¤±è¿›åº¦                         â”‚
â”‚                                                                     â”‚
â”‚  [B] EarlyStopping æœºåˆ¶ (utils/tools.py:51-91)                     â”‚
â”‚      â”œâ”€â”€ è§¦å‘æ—¶æœº: æ¯ä¸ª epoch ç»“æŸï¼ŒéªŒè¯é›†æŸå¤±åˆ›æ–°ä½æ—¶                â”‚
â”‚      â”œâ”€â”€ ä¿å­˜ä½ç½®: checkpoint (æ— æ‰©å±•å)                             â”‚
â”‚      â”œâ”€â”€ ä¿å­˜å†…å®¹: model + optimizer + scheduler + epoch + step     â”‚
â”‚      â””â”€â”€ ç”¨é€”: ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œç”¨äºæœ€ç»ˆæ¨ç†                            â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 è¯¦ç»†å¯¹æ¯”è¡¨

| ç‰¹æ€§ | save_steps | EarlyStopping (vali) |
|------|------------|----------------------|
| **è§¦å‘æ¡ä»¶** | `global_step % save_steps == 0` | `vali_loss < best_score` |
| **è§¦å‘é¢‘ç‡** | æ¯ N steps | æ¯ epoch ç»“æŸï¼ˆæ¡ä»¶è§¦å‘ï¼‰ |
| **ä¿å­˜è·¯å¾„** | `checkpoint_step_{N}/checkpoint.pt` | `checkpoint` |
| **ä¿å­˜å†…å®¹** | å®Œæ•´ dict | å®Œæ•´ dict |
| **è‡ªåŠ¨æ¸…ç†** | æ˜¯ï¼ˆsave_total_limitï¼‰ | å¦ï¼ˆå§‹ç»ˆä¿ç•™æœ€ä½³ï¼‰ |
| **ä¸»è¦ç”¨é€”** | æ–­ç‚¹ç»­è®­ | ä¿å­˜æœ€ä½³æ¨¡å‹ |

### 2.3 æ–‡ä»¶ç»“æ„ç¤ºä¾‹

```
checkpoints/
â””â”€â”€ long_term_forecast_ETTh1_256_96_TimeLLM_ETTh1_...-Llama3_8B/
    â”œâ”€â”€ checkpoint                     # EarlyStopping ä¿å­˜çš„æœ€ä½³æ¨¡å‹
    â”œâ”€â”€ checkpoint_step_1000/          # save_steps ä¿å­˜
    â”‚   â””â”€â”€ checkpoint.pt
    â””â”€â”€ checkpoint_step_2000/          # save_steps ä¿å­˜
        â””â”€â”€ checkpoint.pt
```

---

## 3. ä»£ç æ‰§è¡Œæµç¨‹è¯¦è§£

### 3.1 å•ä¸ª Epoch çš„å®Œæ•´æµç¨‹

```
Epoch å¼€å§‹
    â”‚
    â”œâ”€[1]â”€ model.train()                          # Line 231
    â”‚
    â”œâ”€[2]â”€ for i, batch in enumerate(train_loader):   # Line 234
    â”‚      â”‚
    â”‚      â”œâ”€â”€ å‰å‘ä¼ æ’­: outputs = model(batch_x, ...)
    â”‚      â”œâ”€â”€ è®¡ç®—æŸå¤±: loss = criterion(outputs, batch_y)
    â”‚      â”œâ”€â”€ åå‘ä¼ æ’­: accelerator.backward(loss)
    â”‚      â”œâ”€â”€ å‚æ•°æ›´æ–°: model_optim.step()
    â”‚      â”‚
    â”‚      â”œâ”€â”€ global_step += 1                   # Line 293
    â”‚      â”‚
    â”‚      â””â”€â”€ if global_step % save_steps == 0:  # Line 294
    â”‚          â””â”€â”€ ä¿å­˜ checkpoint_step_{N}/checkpoint.pt
    â”‚
    â”œâ”€[3]â”€ Epoch ç»“æŸåè®¡ç®—æŒ‡æ ‡                    # Line 327-330
    â”‚      â”œâ”€â”€ train_loss = np.average(train_loss)
    â”‚      â”œâ”€â”€ vali_loss = vali(model, vali_loader)   # éªŒè¯é›†è¯„ä¼°
    â”‚      â””â”€â”€ test_loss = vali(model, test_loader)   # æµ‹è¯•é›†è¯„ä¼°
    â”‚
    â”œâ”€[4]â”€ EarlyStopping æ£€æŸ¥                     # Line 335
    â”‚      â”‚
    â”‚      â”œâ”€â”€ if vali_loss < best_score:
    â”‚      â”‚   â””â”€â”€ save_checkpoint() â†’ ä¿å­˜ 'checkpoint'
    â”‚      â”‚
    â”‚      â””â”€â”€ else:
    â”‚          â””â”€â”€ counter += 1
    â”‚              â””â”€â”€ if counter >= patience: early_stop = True
    â”‚
    â””â”€[5]â”€ å­¦ä¹ ç‡è°ƒæ•´                              # Line 340-351
```

### 3.2 vali() å‡½æ•°å·¥ä½œæµç¨‹ (`utils/tools.py:144-193`)

```python
def vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric):
    model.eval()                    # [1] åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    with torch.no_grad():           # [2] ç¦ç”¨æ¢¯åº¦è®¡ç®—
        for batch in vali_loader:
            outputs = model(...)    # [3] å‰å‘ä¼ æ’­
            loss = criterion(...)   # [4] è®¡ç®—æŸå¤±
            total_loss.append(loss.item())

    model.train()                   # [5] æ¢å¤è®­ç»ƒæ¨¡å¼
    return np.average(total_loss)   # [6] è¿”å›å¹³å‡æŸå¤±
```

**å…³é”®ç‚¹**: `vali()` å‡½æ•°**åªè®¡ç®—æŸå¤±ï¼Œä¸ä¿å­˜æ¨¡å‹**ã€‚æ¨¡å‹ä¿å­˜ç”± `EarlyStopping` ç±»å¤„ç†ã€‚

### 3.3 EarlyStopping å·¥ä½œæµç¨‹ (`utils/tools.py:51-91`)

```python
def __call__(self, val_loss, model, path, optimizer, scheduler, epoch, global_step):
    score = -val_loss  # è½¬æ¢ä¸º"è¶Šå¤§è¶Šå¥½"

    if self.best_score is None:           # ç¬¬ä¸€æ¬¡è°ƒç”¨
        self.best_score = score
        self.save_checkpoint(...)         # ä¿å­˜

    elif score < self.best_score + delta: # æ²¡æœ‰æ”¹å–„
        self.counter += 1
        if self.counter >= patience:
            self.early_stop = True        # è§¦å‘æ—©åœ

    else:                                 # æœ‰æ”¹å–„
        self.best_score = score
        self.save_checkpoint(...)         # ä¿å­˜æœ€ä½³æ¨¡å‹
        self.counter = 0                  # é‡ç½®è®¡æ•°å™¨
```

---

## 4. å…³é”®é—®é¢˜è§£ç­”

### 4.1 é—®é¢˜: save_steps ä¿å­˜çš„ checkpoint ä¼šå½±å“ EarlyStopping çš„ checkpoint å—ï¼Ÿ

**ç­”æ¡ˆ: ä¸ä¼šã€‚ä¸¤è€…å®Œå…¨ç‹¬ç«‹ã€‚**

| ç‰¹æ€§ | save_steps checkpoint | EarlyStopping checkpoint |
|------|----------------------|--------------------------|
| ä¿å­˜è·¯å¾„ | `checkpoint_step_{N}/checkpoint.pt` | `checkpoint` |
| è§¦å‘é€»è¾‘ | `global_step % save_steps == 0` | `vali_loss < best_score` |
| ç›¸äº’å½±å“ | âŒ æ—  | âŒ æ—  |

**ä»£ç è¯æ®**:
- save_steps ä¿å­˜: `run_main.py:296` â†’ `os.path.join(path, f'checkpoint_step_{global_step}')`
- EarlyStopping ä¿å­˜: `utils/tools.py:90` â†’ `os.path.join(path, 'checkpoint')`

### 4.2 é—®é¢˜: vali() åœ¨éªŒè¯é›†æŸå¤±æœ€ä½æ—¶ä¿å­˜ checkpoint æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

**ç­”æ¡ˆ: vali() æœ¬èº«ä¸ä¿å­˜ï¼Œç”± EarlyStopping åœ¨ epoch ç»“æŸæ—¶åˆ¤æ–­å¹¶ä¿å­˜ã€‚**

```
æ‰§è¡Œé¡ºåº:
1. vali_loss = vali(...)                      # è®¡ç®—éªŒè¯æŸå¤±
2. early_stopping(vali_loss, model, path, ...)  # åˆ¤æ–­å¹¶ä¿å­˜
   â””â”€â”€ if vali_loss åˆ›æ–°ä½ â†’ save_checkpoint()
```

### 4.3 é—®é¢˜: ä¸€è½® epoch ä¸­ checkpoint æ˜¯å¦‚ä½•ä¿ç•™çš„ï¼Ÿ

**æ—¶åºå›¾**:
```
Epoch N å¼€å§‹
â”‚
â”œâ”€â”€ Step 1: è®­ç»ƒ
â”œâ”€â”€ Step 2: è®­ç»ƒ
â”œâ”€â”€ ...
â”œâ”€â”€ Step 1000: è®­ç»ƒ + [save_steps] â†’ ä¿å­˜ checkpoint_step_1000
â”œâ”€â”€ ...
â”œâ”€â”€ Step 2000: è®­ç»ƒ + [save_steps] â†’ ä¿å­˜ checkpoint_step_2000
â”œâ”€â”€ ...
â”‚
Epoch N ç»“æŸ
â”‚
â”œâ”€â”€ vali_loss = vali(vali_loader)   # è®¡ç®—éªŒè¯é›†æŸå¤±
â”œâ”€â”€ test_loss = vali(test_loader)   # è®¡ç®—æµ‹è¯•é›†æŸå¤±
â”‚
â””â”€â”€ early_stopping(vali_loss, ...)
    â”œâ”€â”€ if vali_loss < best_score:
    â”‚   â””â”€â”€ ä¿å­˜ checkpoint (è¦†ç›–æ—§çš„æœ€ä½³æ¨¡å‹)
    â””â”€â”€ else:
        â””â”€â”€ ä¸ä¿å­˜ï¼Œåªå¢åŠ  counter
```

**å…³é”®: æ¯ä¸ª epoch æœ€å¤šä¿å­˜ä¸€æ¬¡ "best checkpoint"ï¼Œè€Œ save_steps å¯èƒ½ä¿å­˜å¤šæ¬¡ã€‚**

### 4.4 é—®é¢˜: æ–­ç‚¹ç»­è®­æ—¶å¦‚ä½•æ¢å¤ï¼Ÿ

**æ¢å¤é€»è¾‘** (`run_main.py:186-225`):

```python
# 1. åŠ è½½ checkpoint
ckpt = torch.load(ckpt_file, map_location='cpu')

# 2. æ¢å¤æ¨¡å‹å‚æ•°
model.load_state_dict(ckpt['model'])

# 3. æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
optimizer.load_state_dict(ckpt['optimizer'])

# 4. æ¢å¤å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler.load_state_dict(ckpt['scheduler'])

# 5. è®¡ç®—æ¢å¤ä½ç½®
start_epoch = ckpt['epoch']
global_step = ckpt['global_step']
resume_step_in_epoch = global_step % train_steps

# 6. è·³è¿‡å·²è®­ç»ƒçš„ batch
for i, batch in enumerate(train_loader):
    if i < resume_step_in_epoch:
        continue  # è·³è¿‡
```

---

## 5. æ½œåœ¨é—®é¢˜ä¸é£é™©

### 5.1 âš ï¸ é—®é¢˜ 1: Epoch å†…æ¢å¤å¯èƒ½å¯¼è‡´å­¦ä¹ ç‡ä¸ä¸€è‡´

**é—®é¢˜æè¿°**:
- `OneCycleLR` è°ƒåº¦å™¨æŒ‰ step æ›´æ–°å­¦ä¹ ç‡
- æ–­ç‚¹æ¢å¤åï¼Œscheduler ä»ä¿å­˜çŠ¶æ€æ¢å¤
- ä½†å¦‚æœåœ¨ epoch ä¸­é—´æ¢å¤ï¼Œéƒ¨åˆ† step è¢«è·³è¿‡ï¼Œscheduler ä¸ä¼šå†æ¬¡ step

**å½±å“**: å­¦ä¹ ç‡æ›²çº¿å¯èƒ½ä¸é¢„æœŸä¸ä¸€è‡´

**ä»£ç ä½ç½®**: `run_main.py:233-236`
```python
resume_skip = resume_step_in_epoch if (epoch == start_epoch and resume_step_in_epoch > 0) else 0
for i, (batch_x, ...) in enumerate(train_loader):
    if resume_skip > 0 and i < resume_skip:
        continue  # è·³è¿‡äº† batchï¼Œä½† scheduler æ²¡æœ‰åŒæ­¥è·³è¿‡
```

**é£é™©ç­‰çº§**: ğŸŸ¡ ä¸­ç­‰ï¼ˆå¯èƒ½å½±å“æ”¶æ•›é€Ÿåº¦ï¼Œä½†ä¸ä¼šå¯¼è‡´é”™è¯¯ï¼‰

### 5.2 âš ï¸ é—®é¢˜ 2: EarlyStopping çš„ best_score æœªåœ¨ checkpoint ä¸­ä¿å­˜

**é—®é¢˜æè¿°**:
- æ–­ç‚¹ç»­è®­æ—¶ï¼Œ`EarlyStopping` å¯¹è±¡é‡æ–°åˆå§‹åŒ–
- `best_score` è¢«é‡ç½®ä¸º `None`
- ç¬¬ä¸€ä¸ª epoch ç»“æŸæ—¶å¿…å®šä¿å­˜ checkpointï¼ˆå³ä½¿æ¯”ä¹‹å‰çš„æ›´å·®ï¼‰

**ä»£ç ä½ç½®**: `utils/tools.py:53-56`
```python
if self.best_score is None:       # æ–­ç‚¹ç»­è®­åæ€»æ˜¯ None
    self.best_score = score
    if self.save_mode:
        self.save_checkpoint(...)  # ç¬¬ä¸€æ¬¡è°ƒç”¨å¿…å®šä¿å­˜
```

**å½±å“**: æ–­ç‚¹ç»­è®­åå¯èƒ½è¦†ç›–ä¹‹å‰çš„æœ€ä½³æ¨¡å‹

**é£é™©ç­‰çº§**: ğŸŸ  è¾ƒé«˜

**å»ºè®®ä¿®å¤**: åœ¨ checkpoint ä¸­ä¿å­˜ `best_score`ï¼Œå¹¶åœ¨æ¢å¤æ—¶åŠ è½½

### 5.3 âš ï¸ é—®é¢˜ 3: run_pretrain.py ä»ä¼šåˆ é™¤ checkpoints

**é—®é¢˜æè¿°**: `run_pretrain.py:269-271` ä»æœ‰åˆ é™¤ checkpoint çš„ä»£ç 
```python
path = './checkpoints'
del_files(path)  # åˆ é™¤æ‰€æœ‰ checkpoint!
```

**å½±å“**: å¦‚æœä½¿ç”¨ `run_pretrain.py`ï¼Œæ‰€æœ‰ checkpoint ä¼šåœ¨è®­ç»ƒç»“æŸåè¢«åˆ é™¤

**é£é™©ç­‰çº§**: ğŸŸ  è¾ƒé«˜ï¼ˆä½† `run_main.py` å·²ä¿®å¤ï¼‰

### 5.4 âœ… å·²ä¿®å¤: run_main.py ä¸å†åˆ é™¤ checkpoints

**ä»£ç ä½ç½®**: `run_main.py:355-357`
```python
path = './checkpoints'
# del_files(path)  # æ³¨é‡Šæ‰åˆ é™¤æ“ä½œï¼Œä¿ç•™ checkpoint
accelerator.print('Checkpoints saved at: {}'.format(path))
```

### 5.5 âš ï¸ é—®é¢˜ 4: save_total_limit å¯èƒ½è¯¯åˆ æ­£åœ¨å†™å…¥çš„æ–‡ä»¶

**é—®é¢˜æè¿°**: åˆ é™¤æ—§ checkpoint æ—¶æ²¡æœ‰æ–‡ä»¶é”ä¿æŠ¤

**ä»£ç ä½ç½®**: `run_main.py:319-321`
```python
if len(step_dirs) > args.save_total_limit:
    for _, old_dir in step_dirs[:-args.save_total_limit]:
        shutil.rmtree(old_dir)  # ç›´æ¥åˆ é™¤ï¼Œæ— é”ä¿æŠ¤
```

**å½±å“**: åœ¨é«˜å¹¶å‘æˆ–åˆ†å¸ƒå¼è®­ç»ƒæ—¶å¯èƒ½å‡ºé—®é¢˜

**é£é™©ç­‰çº§**: ğŸŸ¡ ä¸­ç­‰ï¼ˆå• GPU è®­ç»ƒæ— å½±å“ï¼‰

---

## 6. è¿è¡Œæ­£ç¡®æ€§éªŒè¯

### 6.1 ä»£ç é€»è¾‘æ£€æŸ¥ âœ…

| æ£€æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|--------|------|------|
| å‚æ•°è§£æ | âœ… | æ–°å¢å‚æ•°æ­£ç¡®å®šä¹‰ |
| checkpoint æ ¼å¼ | âœ… | ç»Ÿä¸€ä¸º dict æ ¼å¼ |
| æ–­ç‚¹ç»­è®­ | âœ… | æ”¯æŒä» step/epoch æ¢å¤ |
| å…¼å®¹æ—§æ ¼å¼ | âœ… | `run_m4.py:245-246` å¤„ç†æ—§æ ¼å¼ |
| EarlyStopping | âœ… | æ­£ç¡®ä¼ é€’æ‰€æœ‰å‚æ•° |

### 6.2 è¿è¡Œæ¡ä»¶æ£€æŸ¥

**å¯ä»¥æ­£ç¡®è¿è¡Œçš„æ¡ä»¶**:

1. âœ… LLM æ¨¡å‹è·¯å¾„å­˜åœ¨ä¸”æœ‰æ•ˆ
2. âœ… æ•°æ®é›†è·¯å¾„æ­£ç¡®
3. âœ… æ˜¾å­˜è¶³å¤Ÿï¼ˆæ ¹æ®å‚æ•°è°ƒæ•´ï¼‰
4. âœ… checkpoint ç›®å½•å¯å†™

### 6.3 é¢„æœŸè¾“å‡ºæ–‡ä»¶

```
/content/drive/MyDrive/T-L/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ long_term_forecast_..._Llama3_8B_4bit_ColabDrive/
â”‚       â”œâ”€â”€ checkpoint                    # EarlyStopping æœ€ä½³æ¨¡å‹
â”‚       â”œâ”€â”€ checkpoint_step_1000/
â”‚       â”‚   â””â”€â”€ checkpoint.pt             # step çº§ä¿å­˜
â”‚       â””â”€â”€ checkpoint_step_2000/
â”‚           â””â”€â”€ checkpoint.pt
â”‚
â””â”€â”€ logs/
    â””â”€â”€ train_llama3_8b_20260126_120000.log
```

### 6.4 å»ºè®®çš„éªŒè¯å‘½ä»¤

```bash
# 1. æ£€æŸ¥ checkpoint å†…å®¹
python -c "
import torch
ckpt = torch.load('checkpoints/.../checkpoint', map_location='cpu')
print('Keys:', ckpt.keys())
print('Epoch:', ckpt.get('epoch'))
print('Global Step:', ckpt.get('global_step'))
"

# 2. æ£€æŸ¥æ¨¡å‹å‚æ•°æ•°é‡
python -c "
import torch
ckpt = torch.load('checkpoints/.../checkpoint', map_location='cpu')
total = sum(p.numel() for p in ckpt['model'].values())
print(f'Total params: {total:,}')
"
```

---

## 7. æ€»ç»“

### 7.1 æ ¸å¿ƒç»“è®º

1. **ä¸¤ç§ checkpoint æœºåˆ¶å®Œå…¨ç‹¬ç«‹**ï¼Œäº’ä¸å½±å“
2. **save_steps**: æŒ‰å›ºå®š step é—´éš”ä¿å­˜ï¼Œç”¨äºæ–­ç‚¹ç»­è®­
3. **EarlyStopping**: æŒ‰éªŒè¯é›†æŸå¤±æœ€ä¼˜ä¿å­˜ï¼Œç”¨äºæœ€ç»ˆæ¨ç†
4. **ä»£ç å¯ä»¥æ­£ç¡®è¿è¡Œ**ï¼Œä½†æœ‰ä¸€äº›å°é—®é¢˜éœ€è¦æ³¨æ„

### 7.2 å»ºè®®æ”¹è¿›

| ä¼˜å…ˆçº§ | æ”¹è¿›é¡¹ | éš¾åº¦ |
|--------|--------|------|
| ğŸ”´ é«˜ | åœ¨ checkpoint ä¸­ä¿å­˜ `best_score` | ç®€å• |
| ğŸŸ¡ ä¸­ | ä¿®å¤ epoch å†…æ¢å¤çš„ scheduler åŒæ­¥é—®é¢˜ | ä¸­ç­‰ |
| ğŸŸ¢ ä½ | æ·»åŠ  checkpoint æ–‡ä»¶é”ä¿æŠ¤ | å¤æ‚ |

### 7.3 ä½¿ç”¨å»ºè®®

1. **é¦–æ¬¡è®­ç»ƒ**: è®¾ç½® `RESUME_FROM=""`
2. **æ–­ç‚¹ç»­è®­**: è®¾ç½® `RESUME_FROM="path/to/checkpoint_step_N"`
3. **æ¨ç†åŠ è½½**: ä½¿ç”¨ EarlyStopping ä¿å­˜çš„ `checkpoint` æ–‡ä»¶
4. **ç›‘æ§**: æŸ¥çœ‹ `logs/` ç›®å½•ä¸‹çš„æ—¥å¿—æ–‡ä»¶

---

*åˆ†æå®Œæˆäº 2026-01-26*
