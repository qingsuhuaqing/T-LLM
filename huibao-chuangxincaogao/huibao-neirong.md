# 开题报告汇报发言稿

**课题名称：** 基于大模型语义融合的时间序列预测模型设计与实现

**汇报人：** 王振达

**预计时长：** 约8分钟

---

## 第一页：封面

老师好，今天我汇报的题目是"基于大模型语义融合的时间序列预测模型设计与实现"。我是王振达，接下来我将从研究背景、项目理解、研究现状、技术路线、已完成工作、创新点设计以及后续计划等方面进行汇报。

---

## 第二页：汇报目录

本次汇报主要包括以下九个部分：首先介绍研究背景，说明为什么选择这个课题；然后阐述我对项目核心思想的理解；接着分析项目的技术优势和可训练参数；之后梳理国内外研究现状；再介绍研究方法与技术路线；然后汇报当前已完成的工作；重点介绍五个创新点的设计思路；最后说明后续计划和研究成果的价值展望。

---

## 第三页：研究背景

时间序列预测是数据科学领域的核心问题，广泛应用于能源负荷预测、金融市场分析、交通流量预测、气象预报等关键场景。然而，传统统计模型如ARIMA在面对复杂非线性模式和长程依赖时表现不足。近年来，虽然深度学习模型取得了显著进展，但从零开始训练的模型往往缺乏预训练知识，泛化能力有限，且预测过程缺乏可解释性。因此，如何结合大模型的强大表示能力，实现更准确且可解释的时序预测，具有重要的研究价值。

---

## 第四页：项目理解

本课题的核心思想是跨模态知识迁移。Time-LLM这篇论文提出了一个创新性的框架，通过"重编程"技术将时序数据映射到大语言模型能够理解的表示空间。具体包括两个核心机制：一是输入重编程，通过Patching将时间序列切分为子序列，再通过投影层映射到LLM的词向量空间；二是提示重编程，将数据的统计特征如均值、方差、趋势等转化为自然语言描述，激活LLM内在的模式识别能力。整个过程中LLM的权重保持冻结，仅训练轻量级的适配层，这既保留了预训练知识，又大幅降低了计算成本。

---

## 第五页：项目优势 - 可训练参数

这个框架的一个重要优势是参数效率很高。如表格所示，整个模型只有约五千六百万个可训练参数，主要包括：PatchEmbedding层负责将时序Patch嵌入到模型维度；Mapping Layer将LLM的大词表压缩映射到一千个虚拟词；Reprogramming Layer通过交叉注意力实现时序到文本的跨模态对齐；FlattenHead负责将LLM输出投影到预测长度。而LLM本身的数十亿参数完全冻结不参与训练，这种"冻结主干加轻量微调"的设计非常高效。

---

## 第六页：国内外研究现状

在文献调研中，我梳理了四类相关工作。第一类是基于Transformer的专用时序模型，如PatchTST引入了Patch分块思想，iTransformer创新性地在变量维度做注意力，TimesNet通过FFT将时序重塑为二维处理。第二类是多尺度混合模型，如TimeMixer提出多尺度分解混合，N-BEATS实现了可解释的基扩展分析，ES-RNN是M4竞赛冠军的混合方法。第三类是LLM用于时序预测的工作，包括本课题基础的Time-LLM、动态Prompt的AutoTimes、以及引入稀疏专家混合的Time-MoE。第四类是可解释性相关工作，如Temporal Fusion Transformer提出了变量选择网络。这些工作为本课题的创新设计提供了重要参考。

---

## 第七页：研究方法与技术路线

本课题的总体方法是"冻结LLM主干加轻量可训练模块加结构化创新模块"。在硬件适配方面，由于我的GPU显存只有6GB，我采用4-bit量化技术，用Qwen 2.5 3B替代原论文的Llama-7B，将显存占用压缩到约1.5GB。在创新实现方面，我计划依次实现残差混合、变量注意力、频域增强等模块，并以可配置开关的方式集成到统一框架中，便于进行消融实验和对比分析。

---

## 第八页：当前完成工作

目前我已完成以下工作：第一，环境搭建，配置了WSL环境、4-bit量化和混合精度训练；第二，论文复现，解决了数据类型不匹配、显存溢出等多个技术问题；第三，验证运行，通过1个epoch验证项目可以正确复现；第四，创新方案设计，完成了七个创新方案的理论分析和文档撰写。整体来说，项目基础框架已经搭建完成，具备了进一步开发创新模块的条件。

---

## 第九页：创新点概述

接下来我重点介绍五个创新点的设计。如表格所示，创新点一借鉴ES-RNN和N-BEATS，采用传统模型加残差学习的架构；创新点二借鉴AMD框架，实现分段自适应融合；创新点三借鉴iTransformer，添加变量间注意力；创新点四借鉴TimesNet，引入频域FFT增强；创新点五借鉴AutoTimes，实现动态Prompt生成。下面逐一详细介绍。

---

## 第十页：创新点一 - 可解释性增强与传统模型集成

创新点一针对的问题是Time-LLM的预测过程是黑盒，缺乏可解释性。我的解决方案是采用残差学习架构：首先用传统统计模型如ARIMA或指数平滑捕获线性成分，包括趋势和季节性；然后让Time-LLM学习残差中的非线性模式；最终预测等于线性预测加非线性预测。这样既发挥了传统模型的可解释优势，又利用了深度模型捕获复杂模式的能力。该方案特别适用于周期性明显的数据，如ETT电力负荷和Traffic交通流量数据集。

---

## 第十一页：创新点二 - 分段自适应融合

创新点二解决的问题是输入序列的不同区间可能呈现不同模式，比如前半段平稳而后半段剧烈波动，固定权重的融合无法适应这种变化。我设计了分段模式检测器，对输入序列进行分段分析，检测每段的周期性、趋势性和噪声水平，然后动态分配传统模型与Time-LLM的融合权重。比如对于周期性强的段，传统模型权重高；对于复杂模式的段，Time-LLM权重高。这种自适应机制特别适用于非平稳时序和模式变化剧烈的数据。

---

## 第十二页：创新点三 - 变量间注意力增强

创新点三针对的问题是Time-LLM将多变量展平为独立样本处理，完全忽略了变量间的相关性。而实际数据中变量间往往存在强关联，比如电力系统中有功功率和无功功率的协同变化。我的方案是在Reprogramming Layer之后添加Inter-Variate Attention模块，在变量维度进行注意力计算，捕获变量间的相关性。这个设计借鉴了iTransformer的思想，特别适用于多变量预测任务和变量间有强相关性的数据。

---

## 第十三页：创新点四 - 频域增强

创新点四针对的问题是时域直接处理可能混淆趋势和周期成分。我的方案是在Patching之前通过FFT将时序分解为低频的趋势成分和高频的季节性成分，然后用不同参数的分支分别处理，最后进行特征融合。这个设计借鉴了TimesNet的频域分解思想，特别适用于周期性明显的数据，如ETTh小时级电力数据具有24小时和168小时的双周期，Weather气象数据也有明显的季节性特征。

---

## 第十四页：创新点五 - 动态Prompt生成

创新点五针对的问题是现有的静态Prompt只包含全局统计信息，无法感知序列内的局部变化，当数据分布突变时也无法及时调整。我设计了分层动态生成机制，首先进行局部统计分析，将序列分段计算趋势变化和波动性变化；然后通过可学习的Prompt Encoder直接从时序数据生成动态嵌入；最后用门控机制融合静态Prompt和动态Prompt。这个方案借鉴了AutoTimes的思想，特别适用于节假日时段、极端天气时段等分布漂移场景。

---

## 第十五页：后续计划

关于后续计划，我打算分阶段推进：第一，计划在一月底前完成论文内容的撰写，详细阐述课题意义和方法原理；第二，计划在二月中旬前完成创新模块的代码实现；第三，寒假期间争取获得更多算力资源，更换参数更大的模型进行实验；第四，完成消融实验验证各创新点的有效性，并考虑将模型应用于实际工程场景进行落地验证。

---

## 第十六页：研究成果价值展望

本课题的预期成果具有三方面价值：学术价值方面，提出可解释的混合预测框架，为大语言模型在时序预测领域的应用贡献新方法；应用价值方面，在电力负荷预测、交通流量预测等场景提供可解释的预测依据，增强模型的可信度；延伸价值方面，验证通用人工智能模型在非语言领域的适配潜力，为多模态融合和跨领域迁移学习提供技术参考。

---

## 第十七页：参考文献

本课题参考的主要文献包括：PatchTST提出的Patch时序分割方法，TimesNet的频域二维变换，iTransformer的变量维度注意力，Time-LLM的LLM重编程框架，TimeMixer的多尺度分解混合，N-BEATS的可解释基扩展，AutoTimes的动态Prompt，Time-MoE的稀疏专家混合，ES-RNN的指数平滑与RNN混合，以及TFT的时序融合Transformer。这些工作涵盖了时序预测、大模型应用和可解释性等多个方向。

---

## 第十八页：致谢

以上就是我的开题报告汇报。总结一下，本课题以Time-LLM为基础，针对其可解释性不足、忽略变量相关性、静态Prompt局限等问题，设计了五个创新模块，目前已完成环境搭建、论文复现和方案设计，后续将逐步实现并验证各创新点的有效性。

谢谢各位老师，欢迎大家批评指正！

---

## 附录：时间分配参考

| 页码 | 内容 | 建议时长 |
|------|------|----------|
| 1-2 | 封面+目录 | 30秒 |
| 3 | 研究背景 | 40秒 |
| 4 | 项目理解 | 45秒 |
| 5 | 可训练参数 | 35秒 |
| 6 | 研究现状 | 45秒 |
| 7 | 技术路线 | 35秒 |
| 8 | 完成工作 | 30秒 |
| 9 | 创新点概述 | 25秒 |
| 10-14 | 五个创新点 | 2分30秒 |
| 15 | 后续计划 | 30秒 |
| 16 | 价值展望 | 25秒 |
| 17 | 参考文献 | 20秒 |
| 18 | 致谢 | 30秒 |
| **总计** | | **约8分钟** |

---

## 附录：关键术语速查

| 术语 | 解释 | 发音提示 |
|------|------|----------|
| Time-LLM | 时间序列大语言模型 | "Time L-L-M" |
| Patching | 分块/切片 | "派驰ing" |
| Reprogramming | 重编程 | "瑞普若格瑞明" |
| FFT | 快速傅里叶变换 | "F-F-T" |
| Inter-Variate | 变量间 | "因特-维瑞特" |
| ARIMA | 自回归积分滑动平均 | "阿瑞玛" |
| Qwen | 通义千问模型 | "千问" |

---

**文档版本：** v1.0
**生成日期：** 2026年1月
**总字数：** 约1500字（正文部分）
**预计汇报时长：** 约8分钟
