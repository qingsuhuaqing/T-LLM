# 研究生毕业设计开题报告

## 课题名称：基于大模型语义融合的时间序列预测模型设计与实现

---

## 一、研究背景

### 1.1 研究意义

时间序列预测是数据科学领域的核心任务之一，广泛应用于能源负荷预测、金融市场分析、交通流量预测、气象预报等关键场景。准确的时间序列预测能够为决策者提供科学依据，降低运营成本，提升资源配置效率，具有重要的社会经济效益。例如，在电力系统中，精准的负荷预测可以优化发电调度、降低能源浪费；在交通领域，准确的流量预测有助于缓解拥堵、提升出行效率。然而，传统时序预测方法在面对复杂的非线性模式、长程依赖关系以及多变量交互时往往表现不足，难以捕捉数据中的深层模式和长期趋势。随着数据规模和复杂性的增长，研究更加准确且具备可解释性的时间序列预测模型，具有重要的学术价值和实际应用意义。

### 1.2 国内外研究现状

近年来，深度学习技术在时间序列预测领域取得了显著进展，相关研究可归纳为以下几个主要类别：

**基于Transformer架构的时序预测模型**是当前研究的热点方向之一。PatchTST借鉴视觉Transformer中Patch的思想，将时间序列分割为固定长度的子序列作为输入Token，通过通道独立策略有效降低了计算复杂度，在长期预测任务上取得了优异性能。iTransformer颠覆了传统Transformer在时间维度做注意力的范式，创新性地提出在变量维度进行注意力计算，每个变量被视为一个Token，通过变量间注意力机制捕获多变量相关性，在多变量预测任务上显著优于同期模型。TimesNet发现时间序列的变化可以分解为周期内变化与周期间变化，通过FFT检测主要周期，将一维时序重塑为二维张量，利用卷积网络捕获复杂模式。然而，这类模型存在明显局限：从零开始训练的模型难以在数据有限时学习到足够丰富的模式表示，且模型的泛化能力和可解释性仍有不足。

**多尺度与频域分解方法**为时序建模提供了另一种思路。TimeMixer提出多尺度分解混合架构，将时序数据在不同采样尺度下分别处理，细粒度尺度捕获局部波动，粗粒度尺度捕获长期趋势，通过Past-Decomposable-Mixing和Future-Multipredictor-Mixing模块实现尺度间信息融合。N-BEATS提出可解释的神经基扩展分析，通过约束网络输出为多项式基（趋势）和傅里叶基（季节性）的线性组合，实现预测结果的可解释分解。这类方法虽然在特定场景下表现优异，但计算开销较大，且缺乏预训练知识的支撑。

**大语言模型用于时序预测**是近期兴起的新范式。Time-LLM首次系统性地提出将预训练大语言模型重编程用于时序预测，核心创新包括：Patch Embedding将时序映射至LLM输入空间，Reprogramming Layer通过交叉注意力实现时序-文本跨模态对齐，Prompt-as-Prefix将统计信息编码为文本提示。AutoTimes将时序预测表述为上下文预测，通过自回归方式动态生成Prompt，增强模型对非平稳时序的适应能力。Time-MoE首次将稀疏专家混合引入时序基础模型，在大参数规模下通过稀疏激活机制实现高容量与低计算的平衡。这类方法利用了LLM的预训练知识，但可解释性仍然不足，预测过程仍是"黑盒"。

**混合模型与可解释性增强方法**是解决上述问题的重要方向。ES-RNN作为M4预测竞赛冠军方法，将指数平滑与RNN结合，ES负责分解时序的水平、趋势和季节性成分，RNN学习跨序列的共享局部趋势，证明了混合方法的有效性。Temporal Fusion Transformer提出变量选择网络量化每个输入特征的重要性，通过可解释的多头注意力机制衡量过去时间步的贡献，实现了预测过程的透明化。然而，现有研究尚缺乏系统性地将传统统计模型优势与LLM重编程范式深度融合的工作。

综上所述，现有研究在各自方向取得了突破，但存在以下问题亟待解决：（1）专用深度时序模型缺乏预训练知识，泛化能力有限；（2）LLM时序预测方法的可解释性不足；（3）传统统计模型的优势未能与深度学习方法有效融合。本课题将针对这些问题展开研究。

### 1.3 研究内容

本课题以Time-LLM为基础框架，深入研究大语言模型适配时序预测任务的关键技术，在完整复现论文的基础上设计创新性改进模块。主要研究内容包括：（1）深入理解Time-LLM的技术原理与数据流机制，完成论文复现与验证；（2）设计残差学习混合架构，融合传统统计模型与LLM深度预测；（3）实现分段自适应融合策略，动态调整不同模型的融合权重；（4）在长序列、多变量数据集上验证改进效果，分析优化后模型的优势与瓶颈；（5）完成模型性能分析、可解释性展示与毕业论文撰写。

---

## 二、技术路线

### 2.1 网络架构

Time-LLM的核心思想是通过"重编程"（Reprogramming）技术，在保持大语言模型权重冻结的前提下，将时序数据映射至LLM的语义空间，从而利用LLM预训练获得的模式识别能力进行时序预测。整体架构包含以下核心模块：


Time-LLM 数据流架构

原始时序数据 [Batch, SeqLen, N_vars]
    │
    ▼
[1. 实例归一化] ─── 计算均值/方差，Z-score标准化
    │
    ▼
[2. 统计特征提取] ─── min, max, median, trend, top-5 lags (FFT)
    │
    ▼
[3. Prompt构建] ─── 领域描述 + 任务描述 + 统计信息
    │                    │
    │                    ▼
    │              Tokenizer → Prompt Embeddings [B*N, PromptLen, llm_dim]
    │
    ▼
[4. Patching分块] ─── Unfold滑动窗口切分
    │
    ▼
[5. Patch Embedding] ─── 1D Conv投影到d_model维度
    │                      [B*N, num_patches, d_model]
    │
    ▼
[6. Reprogramming Layer] ─── Cross-Attention跨模态对齐
    │     Query: Patch Embeddings (时序域)
    │     Key/Value: LLM词嵌入 → Mapping Layer (文本域)
    │
    ▼
[7. 拼接输入] ─── Concat(Prompt Embeddings, Reprogrammed Patches)
    │
    ▼
[8. 冻结LLM前向传播] ─── GPT-2/Qwen/LLAMA (参数不更新)
    │
    ▼
[9. FlattenHead输出投影] ─── Linear映射到pred_len
    │
    ▼
[10. 反归一化] ─── 恢复原始尺度
    │
    ▼
预测结果 [Batch, pred_len, N_vars]


### 2.2 网络模块详解

**（1）实例归一化层（Instance Normalization）**

对每个输入样本独立进行归一化处理，计算并保存均值和标准差用于输出时的反归一化。该模块确保不同尺度的时序数据能够统一到相近的数值范围，有利于模型训练的稳定性。

**（2）Patching分块模块**

借鉴视觉Transformer的设计思想，将长时间序列通过滑动窗口切分为固定长度的Patch。以seq_len=96、patch_len=16、stride=8为例，共生成12个重叠的Patch。该设计既降低了Transformer的计算复杂度（从O(L²)降为O(num_patches²)），又保留了局部时序依赖关系。

**（3）Patch Embedding层**

使用一维卷积将每个Patch从patch_len维投影到d_model维，实现时序特征的初步提取。该模块参数量约800，是可训练参数的一部分。

**（4）Reprogramming Layer（重编程层）**

本模块是Time-LLM的核心创新。通过Cross-Attention机制实现时序域到文本域的跨模态对齐：Query来自Patch Embeddings（时序域），Key和Value来自经过Mapping Layer压缩的LLM词嵌入（文本域）。Mapping Layer将原始词表（如GPT-2的50257个词）压缩映射为1000个可学习的虚拟Token，既降低计算复杂度，又增强表达能力。该层参数量约6M。

**（5）Prompt-as-Prefix模块**

在输入前添加包含时序统计特征的自然语言提示，包括：数据集领域描述、预测任务描述、输入序列的统计信息（最小值、最大值、中位数、趋势方向、FFT分析得到的主要滞后周期）。这些信息激活LLM内部的趋势识别与模式匹配能力，增强预测的可解释性。

**（6）冻结的LLM骨干网络**

LLM参数完全冻结，不参与训练更新。本课题使用Qwen 2.5 3B模型（4-bit量化后约1.5GB显存），通过device_map="auto"实现自动设备分配。冻结策略既保留了预训练知识，又大幅降低了计算开销。

**（7）FlattenHead输出投影**

将LLM输出的后num_patches个位置的特征展平后，通过线性层映射到预测长度pred_len。该模块参数量约37K。

**数据形状变化示例（ETTh1数据集）**

| 阶段 | 张量形状 | 说明 |
|------|----------|------|
| 输入 | [32, 96, 7] | Batch=32, SeqLen=96, N_vars=7 |
| Patching后 | [224, 12, 16] | B*N=224, num_patches=12, d_model=16 |
| Reprogramming后 | [224, 12, 768] | 映射到llm_dim=768 |
| 拼接Prompt后 | [224, 140, 768] | Prompt(128) + Patches(12) |
| LLM输出 | [224, 140, 768] | 冻结LLM前向传播 |
| 最终输出 | [32, 96, 7] | 反归一化后的预测结果 |

**可训练参数分析**

| 组件 | 参数量 | 说明 |
|------|--------|------|
| PatchEmbedding | ~800 | 1D卷积投影 |
| Mapping Layer | ~50M | 词表压缩映射（最大参数块） |
| Reprogramming Layer | ~6M | Cross-Attention权重 |
| FlattenHead | ~37K | 输出投影 |
| **总计** | **~56M** | LLM参数完全冻结 |

### 2.3 核心创新设计

基于文献调研和对Time-LLM的深入理解，本课题拟设计以下创新模块：

**创新点一：残差学习混合架构**

Time-LLM虽然利用了LLM的能力，但预测过程仍是"黑盒"，缺乏可解释性。而传统统计模型（ARIMA、指数平滑）具有透明的数学结构和明确的物理含义。本课题采用残差学习架构：传统模型先捕获线性成分（趋势、季节性），Time-LLM学习残差中的非线性模式。

```
原始序列 y_t
    │
    ▼
[传统模型 (ARIMA/Holt-Winters)]
    │
    ├── 线性预测 ŷ_linear （趋势+季节性）
    │
    └── 残差 r_t = y_t - ŷ_linear
              │
              ▼
         [Time-LLM]
              │
              ▼
         非线性预测 ŷ_nonlinear
              │
              ▼
最终预测 = ŷ_linear + ŷ_nonlinear
```

理论支撑：Hybrid ARIMA-LSTM研究证明，残差修正框架将预测任务分解为线性趋势分析和非线性残差学习，可有效发挥两类模型的互补优势。预期效果：MSE降低10-15%，同时提供趋势/季节性分解的可解释输出。

**创新点二：分段自适应融合**

输入序列的不同区间可能呈现不同模式（如前半段平稳、后半段剧烈波动），固定权重的融合无法适应这种变化。本课题设计分段模式检测器，根据每段的特征动态分配传统模型与Time-LLM的权重。

| 检测指标 | 判断标准 | 融合策略 |
|---------|---------|---------|
| ADF平稳性检验 | p-value < 0.05 → 平稳 | ARIMA权重提高 |
| FFT能量集中度 | > 0.7 → 强周期性 | 传统模型权重提高 |
| 线性回归斜率 | 趋势强度 > 0.5 | Holt-Winters适用 |
| 噪声水平 | > 0.6 → 高噪声 | Time-LLM权重提高 |

预期效果：额外降低MSE 3-5%，输出包含各段的模式判断与权重分配报告，增强可解释性。

**创新点三：变量间注意力增强**

Time-LLM将多变量展平为独立样本处理，完全忽略了变量间的相关性。而实际数据中，变量间往往存在强关联（如电力负荷的有功/无功功率）。本课题在Reprogramming Layer后添加Inter-Variate Attention模块，在变量维度进行注意力计算，捕获多变量相关性。

预期效果：多变量预测MSE降低8-15%，可输出变量重要性权重作为可解释性输出。

---

## 三、实验评估

### 3.1 数据集

| 数据集 | 类型 | 变量数 | 序列长度 | 采样频率 | 特点 |
|-------|------|-------|---------|---------|------|
| ETTh1/h2 | 电力负荷 | 7 | 17420 | 小时 | 24h/168h周期明显 |
| ETTm1/m2 | 电力负荷 | 7 | 69680 | 15分钟 | 高频采样，长序列 |
| Weather | 气象 | 21 | 52696 | 10分钟 | 多变量，季节性强 |

本课题主要在ETT数据集（Electricity Transformer Temperature）上进行实验验证。ETT数据集记录了电力变压器的温度及相关负荷数据，包含油温（OT）和6个负荷特征（HUFL、HULL、MUFL、MULL、LUFL、LULL），具有明显的日周期（24小时）和周周期（168小时）特征，是时序预测领域的标准基准数据集。

### 3.2 评估指标

| 指标 | 公式 | 用途 |
|------|------|------|
| MSE | $\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$ | 主要性能指标，对大误差敏感 |
| MAE | $\frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$ | 辅助性能指标，单位与原数据一致 |
| RMSE | $\sqrt{MSE}$ | 均方根误差，便于直观理解 |

本课题以MSE为主要评价指标，同时报告MAE作为辅助参考。

### 3.3 实验环境与硬件适配

| 配置项 | 规格 |
|-------|------|
| 操作系统 | Windows 11 + WSL2 (Ubuntu) |
| GPU | NVIDIA GTX 1660 Ti (6GB VRAM) |
| 深度学习框架 | PyTorch 2.0+ |
| LLM后端 | Qwen 2.5 3B (4-bit量化) |
| 加速库 | Hugging Face Accelerate, BitsAndBytes |

由于本机GPU显存仅有6GB，无法直接运行原论文使用的LLAMA-7B模型（约14GB显存）。本课题采用以下优化策略实现硬件适配：

- 使用Qwen 2.5 3B替代LLAMA-7B，并通过4-bit量化（NF4）将显存占用压缩至约1.5GB
- 配置llm_layers=6（原始32层裁剪至6层），降低计算开销
- 调整batch_size为4-8，配合梯度累积策略
- 使用混合精度训练（FP16）进一步优化显存使用

### 3.4 实验结果

**复现验证实验**

本课题成功在ETTh1数据集上完成了Time-LLM的复现验证。由于本机算力有限，仅运行1个epoch以验证项目可以正确执行。训练日志显示模型正常收敛：

训练环境配置：
- 模型：Qwen 2.5 3B（4-bit量化）
- 数据集：ETTh1/ETTm1
- 序列配置：seq_len=512, pred_len=96

训练过程记录（部分）：

| 迭代次数 | 损失值 (Loss) | 训练速度 |
|---------|--------------|---------|
| 100 | 1.2920 | 1.09 s/iter |
| 200 | 0.3284 | 1.01 s/iter |
| 300 | 0.8609 | 1.03 s/iter |
| 400 | 0.3328 | 1.05 s/iter |
| 500 | 0.4864 | 1.01 s/iter |
| 2900 | 0.6280 | 1.13 s/iter |
| 3000 | 0.4780 | 1.06 s/iter |
| 3100 | 1.0924 | 1.09 s/iter |
| 3200 | 0.3007 | 1.11 s/iter |
| 3300 | 0.2923 | 1.20 s/iter |
| 3400 | 0.2098 | 1.06 s/iter |
| 3500 | 0.6787 | 1.05 s/iter |
| 3600 | 0.3568 | 1.07 s/iter |
| 3700 | 0.4803 | 1.99 s/iter |
| 3800 | 0.3164 | 1.19 s/iter |
| 3900 | 0.2834 | 1.05 s/iter |

**实验结果分析**

从训练日志可以观察到：
1. **模型成功加载与运行**：Qwen 2.5 3B模型通过4-bit量化成功加载，项目在受限硬件条件下正常运行
2. **损失函数收敛**：训练Loss从初始的1.29逐步下降至0.2-0.3区间，表明模型正在有效学习时序模式
3. **训练效率稳定**：每迭代约1.0-1.2秒，训练过程稳定无显存溢出
4. **损失波动特征**：Loss呈现一定波动（如iter 3100的1.0924），这与时序数据的非平稳性和批次采样的随机性相关

**代码修复记录**

在复现过程中，本课题解决了以下技术问题：

| 问题 | 原因 | 解决方案 |
|------|------|---------|
| KeyError: 'qwen2' | transformers版本过低 | 升级至≥4.40.0 |
| 数据类型不匹配 | 4-bit量化导致bfloat16与float32冲突 | 修改TimeLLM.py第297-298行，统一数据类型 |
| AttributeError: 'content' | load_content()调用顺序错误 | 将其移至模型初始化之前 |
| 显存溢出OOM | llm_layers默认32层过多 | 减少至6层 |

上述修复确保了项目在资源受限环境下的稳定运行，为后续创新模块的实现奠定了基础。

**预期实验结果**

在完整训练条件下（多epoch、更大算力），基于创新模块的预期效果如下：

| 模型配置 | ETTh1 MSE | ETTm1 MSE | 可解释性 |
|---------|-----------|-----------|---------|
| Time-LLM (基线) | 0.375 | 0.302 | 低 |
| + 残差学习架构 | 0.330 | 0.265 | 高 |
| + 分段自适应融合 | 0.315 | 0.250 | 高 |
| + 变量间注意力 | 0.305 | 0.240 | 高 |

---

## 四、未来计划

### 4.1 总体进度安排

```
2026年1月 ──────────────────────────────────────────────────────────────►
    │                    │                         │
    ▼                    ▼                         ▼
 [开题报告]          [论文撰写]               [创新实现]
                         │
                         ▼
                    [实验验证]
                         │
                         ▼
2026年2月 ──────────────────────────────────────────────────────────────►
    │                    │                         │
    ▼                    ▼                         ▼
 [代码改进]          [消融实验]               [算力申请]
                         │
                         ▼
                    [对比实验]
                         │
                         ▼
2026年3月 ──────────────────────────────────────────────────────────────►
    │                    │                         │
    ▼                    ▼                         ▼
 [论文完善]          [答辩准备]               [成果整理]
```

### 4.2 详细执行计划

| 时间节点 | 主要任务 | 具体内容 | 预期产出 |
|---------|---------|---------|---------|
| **1月末前** | 论文内容撰写 | 详细阐述课题意义、技术原理、代码框架、复现过程 | 论文初稿（方法与实验章节） |
| | 创新模块设计 | 完成残差学习架构的详细设计文档 | 模块设计文档 |
| **2月中旬前** | 代码实现 | 实现残差学习混合架构（创新点一） | 可运行的创新代码 |
| | 代码实现 | 实现分段自适应融合（创新点二） | 可运行的创新代码 |
| | 算力申请 | 协调课题组算力资源，申请更大参数模型的运行环境 | 算力使用权限 |
| **2月末前** | 实验验证 | 在ETT数据集上验证创新模块效果 | 完整实验结果 |
| | 消融实验 | 逐一验证各创新点的有效性 | 消融实验表格 |
| | 对比实验 | 与PatchTST、iTransformer等基线模型对比 | 对比实验报告 |
| | 可解释性展示 | 可视化趋势/季节性分解、权重分配 | 可视化图表 |
| **3月初** | 论文完善 | 撰写引言、相关工作、结论章节 | 论文终稿 |
| | 结果分析 | 深入分析实验结果，总结创新贡献 | 结果分析报告 |
| **3月中旬** | 答辩准备 | 制作答辩PPT，准备答辩材料 | 答辩材料 |
| | 代码整理 | 整理代码文档，确保可复现性 | 完整代码库 |

### 4.3 风险控制

| 风险类型 | 风险描述 | 应对措施 |
|---------|---------|---------|
| 算力不足 | 本机显存有限，无法完成大规模实验 | 申请课题组服务器资源，利用寒假空闲时段运行实验 |
| 创新效果不显著 | 设计的创新模块未能带来预期的性能提升 | 准备备选方案（变量间注意力、频域增强），灵活调整研究重点 |
| 实验周期过长 | 完整训练需要较长时间 | 优先在小数据集验证，确认有效后扩展至完整数据集 |
| 代码调试困难 | 新模块与原框架的集成可能存在兼容性问题 | 采用模块化设计，通过配置开关控制各创新模块的启用 |

### 4.4 预期成果

**技术成果**
- 可解释性增强的混合时序预测模型
- 在ETT数据集上MSE降低15-20%的实验验证
- 完整的代码实现与文档

**学术成果**
- 完成毕业论文撰写
- 具备投稿国内会议/期刊的研究基础

**应用价值**
- 在电力负荷预测、交通流量预测等场景提供可解释的预测依据
- 为大语言模型在时序预测领域的应用提供技术参考

---

## 五、参考文献

[1] Nie Y, Nguyen N H, Sinthong P, et al. A Time Series is Worth 64 Words: Long-term Forecasting with Transformers[C]. ICLR, 2023.

[2] Wu H, Hu T, Liu Y, et al. TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis[C]. ICLR, 2023.

[3] Liu Y, Hu T, Zhang H, et al. iTransformer: Inverted Transformers Are Effective for Time Series Forecasting[C]. ICLR, 2024.

[4] Jin M, Wang S, Ma L, et al. Time-LLM: Time Series Forecasting by Reprogramming Large Language Models[C]. ICLR, 2024.

[5] Wang S, Wu H, Shi X, et al. TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting[C]. ICLR, 2024.

[6] Oreshkin B N, Carpov D, Chapados N, et al. N-BEATS: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting[C]. ICLR, 2020.

[7] Liu Y, Zhang H, Li C, et al. AutoTimes: Autoregressive Time Series Forecasters via Large Language Models[C]. NeurIPS, 2024.

[8] Shi X, Wu H, Wang S, et al. Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts[C]. ICLR, 2025.

[9] Smyl S. A Hybrid Method of Exponential Smoothing and Recurrent Neural Networks for Time Series Forecasting[J]. International Journal of Forecasting, 2020.

[10] Lim B, Arık S Ö, Loeff N, et al. Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting[J]. International Journal of Forecasting, 2021.

[11] Zhang G P. Time Series Forecasting Using a Hybrid ARIMA and Neural Network Model[J]. Neurocomputing, 2003.

---

**报告撰写日期**：2026年1月

**字数统计**：约 4500 字
