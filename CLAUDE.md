# CLAUDE.md - Project Guide / é¡¹ç›®æŒ‡å—

> **Purpose / ç›®çš„**: This file provides comprehensive guidance to Claude Code for understanding and working with this Time-LLM research project.
> æœ¬æ–‡ä»¶ä¸º Claude Code æä¾›å…¨é¢çš„é¡¹ç›®ç†è§£å’Œå·¥ä½œæŒ‡å—ã€‚

---

## Table of Contents / ç›®å½•

1. [Project Overview / é¡¹ç›®æ¦‚è¿°](#1-project-overview--é¡¹ç›®æ¦‚è¿°)
2. [Project Status / é¡¹ç›®çŠ¶æ€](#2-project-status--é¡¹ç›®çŠ¶æ€)
3. [Documentation Index / æ–‡æ¡£ç´¢å¼•](#3-documentation-index--æ–‡æ¡£ç´¢å¼•)
4. [Innovation Points / åˆ›æ–°ç‚¹](#4-innovation-points--åˆ›æ–°ç‚¹)
5. [Architecture & Trainable Parameters / æ¶æ„ä¸å¯è®­ç»ƒå‚æ•°](#5-architecture--trainable-parameters--æ¶æ„ä¸å¯è®­ç»ƒå‚æ•°)
6. [Hardware Constraints / ç¡¬ä»¶é™åˆ¶](#6-hardware-constraints--ç¡¬ä»¶é™åˆ¶)
7. [Common Commands / å¸¸ç”¨å‘½ä»¤](#7-common-commands--å¸¸ç”¨å‘½ä»¤)
8. [Code Structure / ä»£ç ç»“æ„](#8-code-structure--ä»£ç ç»“æ„)
9. [Troubleshooting / æ•…éšœæ’é™¤](#9-troubleshooting--æ•…éšœæ’é™¤)

---

## 1. Project Overview / é¡¹ç›®æ¦‚è¿°

### What is Time-LLM? / Time-LLM æ˜¯ä»€ä¹ˆï¼Ÿ

Time-LLM is a framework that reprograms frozen Large Language Models (LLMs) for time series forecasting. Instead of training from scratch, it leverages the pattern recognition capabilities of pre-trained LLMs.

Time-LLM æ˜¯ä¸€ä¸ªå°†å†»ç»“çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡ç¼–ç¨‹ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„æ¡†æ¶ã€‚å®ƒä¸ä»é›¶å¼€å§‹è®­ç»ƒï¼Œè€Œæ˜¯åˆ©ç”¨é¢„è®­ç»ƒ LLM çš„æ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚

### Core Mechanisms / æ ¸å¿ƒæœºåˆ¶

```
Time Series Data / æ—¶åºæ•°æ®
    â”‚
    â–¼
[1. Patching] - Split into patches / åˆ‡åˆ†ä¸ºå—
    â”‚
    â–¼
[2. Patch Embedding] - Project to d_model dimension / æŠ•å½±åˆ° d_model ç»´åº¦
    â”‚
    â–¼
[3. Reprogramming Layer] - Cross-attention alignment / äº¤å‰æ³¨æ„åŠ›å¯¹é½
    â”‚         Query: Patch embeddings (time series domain / æ—¶åºåŸŸ)
    â”‚         Key/Value: LLM word embeddings (text domain / æ–‡æœ¬åŸŸ)
    â”‚
    â–¼
[4. Prompt-as-Prefix] - Prepend statistical prompts / å‰ç¼€ç»Ÿè®¡æç¤º
    â”‚         min, max, median, trend, top-5 lags (FFT)
    â”‚
    â–¼
[5. Frozen LLM Forward] - GPT-2/Qwen/LLAMA processes embeddings / å†»ç»“LLMå¤„ç†åµŒå…¥
    â”‚
    â–¼
[6. Output Projection] - FlattenHead maps to pred_len / è¾“å‡ºæŠ•å½±åˆ°é¢„æµ‹é•¿åº¦
    â”‚
    â–¼
Prediction / é¢„æµ‹ç»“æœ
```

### Key Insight / æ ¸å¿ƒæ´å¯Ÿ

**Input Reprogramming / è¾“å…¥é‡ç¼–ç¨‹**: Maps time series to LLM's word embedding space via learned projection, making LLM treat time series as "special text features".

é€šè¿‡å¯å­¦ä¹ æŠ•å½±å°†æ—¶åºæ˜ å°„åˆ° LLM è¯å‘é‡ç©ºé—´ï¼Œä½¿ LLM å°†æ—¶åºè§†ä¸º"ç‰¹æ®Šæ–‡æœ¬ç‰¹å¾"ã€‚

**Prompt Reprogramming / æç¤ºé‡ç¼–ç¨‹**: Encodes statistical features (mean, variance, trend) as natural language prompts, activating LLM's inherent trend recognition capabilities.

å°†ç»Ÿè®¡ç‰¹å¾ç¼–ç ä¸ºè‡ªç„¶è¯­è¨€æç¤ºï¼Œæ¿€æ´» LLM å†…åœ¨çš„è¶‹åŠ¿è¯†åˆ«èƒ½åŠ›ã€‚

---

## 2. Project Status / é¡¹ç›®çŠ¶æ€

### Completed Work / å·²å®Œæˆå·¥ä½œ

| Task / ä»»åŠ¡ | Status / çŠ¶æ€ | Details / è¯¦æƒ… |
|-------------|---------------|----------------|
| Paper reproduction / è®ºæ–‡å¤ç° | âœ… Done / å®Œæˆ | ETTh1, ETTm1 datasets verified / å·²éªŒè¯ |
| Environment setup / ç¯å¢ƒæ­å»º | âœ… Done / å®Œæˆ | 6GB VRAM adaptation / 6GBæ˜¾å­˜é€‚é… |
| Model replacement / æ¨¡å‹æ›¿æ¢ | âœ… Done / å®Œæˆ | GPT-2 â†’ Qwen 2.5 3B (4-bit) |
| Code fixes / ä»£ç ä¿®å¤ | âœ… Done / å®Œæˆ | 4 critical bugs fixed / 4ä¸ªå…³é”®bugå·²ä¿®å¤ |
| Innovation design / åˆ›æ–°è®¾è®¡ | âœ… Done / å®Œæˆ | 7 schemes documented / 7ä¸ªæ–¹æ¡ˆå·²æ–‡æ¡£åŒ– |
| Thesis proposal / å¼€é¢˜æŠ¥å‘Š | âœ… Done / å®Œæˆ | `report.md`, `baogao.md` |

### Pending Work / å¾…å®Œæˆå·¥ä½œ

| Task / ä»»åŠ¡ | Priority / ä¼˜å…ˆçº§ | Document / æ–‡æ¡£ |
|-------------|-------------------|-----------------|
| Implement Scheme 1 (Hybrid Model) / å®ç°æ–¹æ¡ˆä¸€ï¼ˆæ··åˆæ¨¡å‹ï¼‰ | ğŸ¥‡ High / é«˜ | `md/chuangxin-jieshi1.md` |
| Implement Inter-Variate Attention / å®ç°å˜é‡é—´æ³¨æ„åŠ› | ğŸ¥ˆ High / é«˜ | `md/chuangxin-shijian.md` |
| Ablation experiments / æ¶ˆèå®éªŒ | ğŸ¥‰ Medium / ä¸­ | - |
| Thesis writing / è®ºæ–‡æ’°å†™ | Medium / ä¸­ | - |

---

## 3. Documentation Index / æ–‡æ¡£ç´¢å¼•

### Root Directory Files / æ ¹ç›®å½•æ–‡ä»¶

| File / æ–‡ä»¶ | Purpose / ç”¨é€” |
|-------------|----------------|
| `CLAUDE.md` | **This file** - Project guide for AI assistants / æœ¬æ–‡ä»¶ - AIåŠ©æ‰‹é¡¹ç›®æŒ‡å— |
| `README.md` | Original Time-LLM documentation / åŸå§‹Time-LLMæ–‡æ¡£ |
| `report.md` | **Thesis proposal (Chinese)** / å¼€é¢˜æŠ¥å‘Šï¼ˆä¸­æ–‡ï¼‰ |
| `baogao.md` | **Thesis proposal (Formal)** / å¼€é¢˜æŠ¥å‘Šï¼ˆæ­£å¼ç‰ˆï¼‰ |

### `md/` Directory - Core Documentation / mdç›®å½• - æ ¸å¿ƒæ–‡æ¡£

#### Technical Analysis / æŠ€æœ¯åˆ†æ
| File / æ–‡ä»¶ | Content / å†…å®¹ |
|-------------|----------------|
| `md/work.md` | Initial project notes / åˆå§‹é¡¹ç›®ç¬”è®° |
| `md/work1.md` | Environment setup guide / ç¯å¢ƒæ­å»ºæŒ‡å— |
| `md/work2.md` | **Deep technical analysis** - Data flow, architecture, metrics / æ·±åº¦æŠ€æœ¯åˆ†æ - æ•°æ®æµã€æ¶æ„ã€æŒ‡æ ‡ |
| `md/work3.md` | Complete technical documentation / å®Œæ•´æŠ€æœ¯æ–‡æ¡£ |
| `md/Trainable-part.md` | **Trainable parameters analysis** (~56M params) / å¯è®­ç»ƒå‚æ•°åˆ†æ |
| `md/tuidao.md` | Mathematical derivations / æ•°å­¦æ¨å¯¼ |

#### Innovation Schemes / åˆ›æ–°æ–¹æ¡ˆ
| File / æ–‡ä»¶ | Content / å†…å®¹ |
|-------------|----------------|
| `md/chuangxin.md` | **Innovation overview** - 7 schemes summary / åˆ›æ–°æ¦‚è¿° - 7ä¸ªæ–¹æ¡ˆæ€»ç»“ |
| `md/chuangxin-lilun.md` | **Theoretical analysis** - Each scheme's theory / ç†è®ºåˆ†æ - æ¯ä¸ªæ–¹æ¡ˆçš„ç†è®ºåŸºç¡€ |
| `md/chuangxin-shijian.md` | **Implementation guide** - Code for all 7 schemes / å®è·µæŒ‡å— - 7ä¸ªæ–¹æ¡ˆçš„ä»£ç  |
| `md/chuangxin-jieshi1.md` | **Scheme 1 deep dive** - Hybrid model with traditional methods / æ–¹æ¡ˆä¸€æ·±åº¦è§£æ - ä¼ ç»Ÿæ¨¡å‹æ··åˆ |

#### Operational Guides / æ“ä½œæŒ‡å—
| File / æ–‡ä»¶ | Content / å†…å®¹ |
|-------------|----------------|
| `md/mingling.md` | Command parameters guide / å‘½ä»¤å‚æ•°æŒ‡å— |
| `md/mingling_2.md` | Extended command guide / æ‰©å±•å‘½ä»¤æŒ‡å— |
| `md/wenti.md` | **Troubleshooting** - All issues and solutions / æ•…éšœæ’é™¤ - æ‰€æœ‰é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ |
| `md/yunxing-fuxian.md` | Reproduction guide / å¤ç°æŒ‡å— |
| `md/yunxing-wenda.md` | Q&A during running / è¿è¡Œé—®ç­” |
| `md/yunfuwuqi.md` | Cloud server deployment / äº‘æœåŠ¡å™¨éƒ¨ç½² |

### `Task_Requirements/` Directory / ä»»åŠ¡è¦æ±‚ç›®å½•

| File / æ–‡ä»¶ | Content / å†…å®¹ |
|-------------|----------------|
| `task.md` | Thesis task requirements / æ¯•è®¾ä»»åŠ¡ä¹¦ |
| `Reference-writing-style.md` | User's writing style reference / ç”¨æˆ·å†™ä½œé£æ ¼å‚è€ƒ |

---

## 4. Innovation Points / åˆ›æ–°ç‚¹

### Overview of 7 Schemes / 7ä¸ªæ–¹æ¡ˆæ¦‚è¿°

| # | Scheme / æ–¹æ¡ˆ | Key Idea / æ ¸å¿ƒæ€æƒ³ | Expected Gain / é¢„æœŸæ”¶ç›Š | Priority / ä¼˜å…ˆçº§ |
|---|---------------|---------------------|--------------------------|-------------------|
| 1 | **Hybrid Model** / æ··åˆæ¨¡å‹ | Traditional + LLM residual learning / ä¼ ç»Ÿæ¨¡å‹+LLMæ®‹å·®å­¦ä¹  | MSE â†“15-20%, interpretability / å¯è§£é‡Šæ€§ | ğŸ¥‡ Highest / æœ€é«˜ |
| 2 | **Multi-Scale Decomposition** / å¤šå°ºåº¦åˆ†è§£ | Different scales for different patterns / ä¸åŒå°ºåº¦æ•è·ä¸åŒæ¨¡å¼ | MSE â†“5-10% | Medium / ä¸­ |
| 3 | **Frequency Enhancement** / é¢‘åŸŸå¢å¼º | FFT decompose trend/seasonal / FFTåˆ†è§£è¶‹åŠ¿/å­£èŠ‚æ€§ | MSE â†“10-15% | ğŸ¥ˆ High / é«˜ |
| 4 | **Inter-Variate Attention** / å˜é‡é—´æ³¨æ„åŠ› | Attention on variable dimension / åœ¨å˜é‡ç»´åº¦åšæ³¨æ„åŠ› | MSE â†“8-15% | ğŸ¥ˆ High / é«˜ |
| 5 | **Dynamic Prompt** / åŠ¨æ€æç¤º | Learnable prompt encoder / å¯å­¦ä¹ æç¤ºç¼–ç å™¨ | MSE â†“10-20% | Medium / ä¸­ |
| 6 | **MoE (Sparse Experts)** / ç¨€ç–ä¸“å®¶æ··åˆ | Multiple experts for different patterns / å¤šä¸“å®¶å¤„ç†ä¸åŒæ¨¡å¼ | Capacity â†‘4x | Low / ä½ |
| 7 | **Vocabulary Initialization** / è¯è¡¨åˆå§‹åŒ– | Task-specific initialization / ä»»åŠ¡ç›¸å…³åˆå§‹åŒ– | Convergence â†‘20-30% | Low / ä½ |

### Scheme 1 Deep Dive (Recommended) / æ–¹æ¡ˆä¸€è¯¦è§£ï¼ˆæ¨èï¼‰

**Document / æ–‡æ¡£**: `md/chuangxin-jieshi1.md`

#### Three Sub-approaches / ä¸‰ä¸ªå­æ–¹æ¡ˆ

**A. Residual Learning Architecture / æ®‹å·®å­¦ä¹ æ¶æ„** â­â­â­â­â­
```
Original Series â†’ [ARIMA/ES] â†’ Linear Prediction + Residual
                                        â†“
                               [Time-LLM learns residual]
                                        â†“
              Final = Linear Prediction + Nonlinear Prediction
```
- Theory: Hybrid ARIMA-LSTM paper proves effectiveness / ç†è®ºï¼šæ··åˆARIMA-LSTMè®ºæ–‡è¯æ˜æœ‰æ•ˆæ€§
- Expected: MSE â†“10-15% / é¢„æœŸï¼šMSEä¸‹é™10-15%

**B. Segment-wise Adaptive Fusion / åˆ†æ®µè‡ªé€‚åº”èåˆ** â­â­â­â­
```
Input Sequence â†’ [Segment Analyzer] â†’ Different weights per segment
                     â†“
    Segment 1 (periodic) â†’ Traditional weight: 0.8
    Segment 2 (complex)  â†’ Time-LLM weight: 0.9
    Segment 3 (trending) â†’ Traditional weight: 0.7
```
- Theory: AMD Framework (Mixture-of-Experts) / ç†è®ºï¼šAMDæ¡†æ¶
- Expected: Additional MSE â†“3-5% / é¢„æœŸï¼šé¢å¤–MSEä¸‹é™3-5%

**C. Knowledge Distillation / çŸ¥è¯†è’¸é¦** â­â­â­
- Soft label distillation: Learn teacher's distribution / è½¯æ ‡ç­¾è’¸é¦
- Decomposition distillation: Learn trend/seasonal decomposition / åˆ†è§£è’¸é¦
- Behavior distillation: Learn direction/magnitude consistency / è¡Œä¸ºè’¸é¦
- Theory: DE-TSMCL achieves MSE â†“24.2% on ETTm1 / ç†è®ºï¼šDE-TSMCLåœ¨ETTm1ä¸ŠMSEä¸‹é™24.2%

### Applicable Scenarios / é€‚ç”¨åœºæ™¯

| Scheme / æ–¹æ¡ˆ | Best For / æœ€é€‚åˆ | Dataset Examples / æ•°æ®é›†ç¤ºä¾‹ |
|---------------|-------------------|------------------------------|
| Hybrid Model / æ··åˆæ¨¡å‹ | Periodic + interpretability needed / å‘¨æœŸæ€§+éœ€è¦å¯è§£é‡Šæ€§ | ETT, Traffic |
| Frequency Enhancement / é¢‘åŸŸå¢å¼º | Strong periodicity / å¼ºå‘¨æœŸæ€§ | ETTh (24h/168h cycles) |
| Inter-Variate Attention / å˜é‡é—´æ³¨æ„åŠ› | Multi-variate with correlations / å¤šå˜é‡ä¸”æœ‰ç›¸å…³æ€§ | Electricity, Weather |
| Multi-Scale / å¤šå°ºåº¦ | Long sequences (720+) / é•¿åºåˆ— | ETTm (15-min data) |

---

## 5. Architecture & Trainable Parameters / æ¶æ„ä¸å¯è®­ç»ƒå‚æ•°

### Frozen vs Trainable / å†»ç»“ä¸å¯è®­ç»ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Time-LLM Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FROZEN (LLM Backbone) / å†»ç»“éƒ¨åˆ†                            â”‚
â”‚  â”œâ”€â”€ GPT-2: 124M params / å‚æ•°                               â”‚
â”‚  â”œâ”€â”€ Qwen 2.5 3B: ~1.5GB (4-bit) / 4-bité‡åŒ–å               â”‚
â”‚  â””â”€â”€ Word Embeddings: Used as Key/Value / ç”¨ä½œK/V            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TRAINABLE (~56M params for GPT-2) / å¯è®­ç»ƒéƒ¨åˆ†              â”‚
â”‚  â”œâ”€â”€ PatchEmbedding: ~800 params                             â”‚
â”‚  â”‚   â””â”€â”€ Conv1d + Positional Embedding                       â”‚
â”‚  â”œâ”€â”€ Mapping Layer: ~50M params                              â”‚
â”‚  â”‚   â””â”€â”€ Linear(vocab_size=50257, num_tokens=1000)           â”‚
â”‚  â”œâ”€â”€ Reprogramming Layer: ~6M params                         â”‚
â”‚  â”‚   â””â”€â”€ Cross-Attention (Q: patches, K/V: mapped words)     â”‚
â”‚  â””â”€â”€ FlattenHead: ~37K params                                â”‚
â”‚      â””â”€â”€ Linear(head_nf, pred_len)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Shape Flow (ETTh1 Example) / æ•°æ®å½¢çŠ¶æµï¼ˆETTh1ç¤ºä¾‹ï¼‰

| Stage / é˜¶æ®µ | Shape / å½¢çŠ¶ | Description / æè¿° |
|--------------|--------------|---------------------|
| Input / è¾“å…¥ | `[32, 96, 7]` | Batch=32, SeqLen=96, N_vars=7 |
| After Patching / åˆ†å—å | `[224, 12, 16]` | B*N=224, num_patches=12, d_model=16 |
| After Reprogramming / é‡ç¼–ç¨‹å | `[224, 12, 768]` | Mapped to llm_dim=768 |
| With Prompt / åŠ æç¤ºå | `[224, 140, 768]` | Prompt(128) + Patches(12) |
| LLM Output / LLMè¾“å‡º | `[224, 140, 768]` | GPT-2 forward |
| Final Output / æœ€ç»ˆè¾“å‡º | `[32, 96, 7]` | After FlattenHead + denormalize |

### Key Code Locations / å…³é”®ä»£ç ä½ç½®

| Component / ç»„ä»¶ | File / æ–‡ä»¶ | Lines / è¡Œå· |
|------------------|-------------|--------------|
| Main Model / ä¸»æ¨¡å‹ | `models/TimeLLM.py` | Full file / å…¨æ–‡ä»¶ |
| LLM Loading / LLMåŠ è½½ | `models/TimeLLM.py` | 43-96 (Qwen), 83-117 (GPT-2) |
| LLM Freezing / LLMå†»ç»“ | `models/TimeLLM.py` | 163-164 |
| Prompt Construction / æç¤ºæ„å»º | `models/TimeLLM.py` | 207-230 |
| PatchEmbedding | `layers/Embed.py` | 160-186 |
| ReprogrammingLayer | `models/TimeLLM.py` | 267-305 |
| Instance Normalization / å®ä¾‹å½’ä¸€åŒ– | `layers/StandardNorm.py` | Full file / å…¨æ–‡ä»¶ |

---

## 6. Hardware Constraints / ç¡¬ä»¶é™åˆ¶

### Current Setup / å½“å‰é…ç½®

- **GPU**: NVIDIA GTX 1660 Ti (6GB VRAM)
- **OS**: Windows 11 + WSL2 (Ubuntu)
- **LLM**: Qwen 2.5 3B with 4-bit quantization / 4-bité‡åŒ–

### VRAM Budget / æ˜¾å­˜é¢„ç®—

| Component / ç»„ä»¶ | VRAM / æ˜¾å­˜ |
|------------------|-------------|
| Qwen 2.5 3B (4-bit) | ~1.5 GB |
| Trainable params / å¯è®­ç»ƒå‚æ•° | ~0.5 GB |
| Intermediate tensors / ä¸­é—´å˜é‡ | ~2.0 GB |
| System overhead / ç³»ç»Ÿå ç”¨ | ~1.0 GB |
| **Total / æ€»è®¡** | **~5.0 GB** âœ… |

### Recommended Parameters / æ¨èå‚æ•°

| Parameter / å‚æ•° | Value / å€¼ | Reason / åŸå›  |
|------------------|------------|---------------|
| `--batch_size` | 4-8 | VRAM limit / æ˜¾å­˜é™åˆ¶ |
| `--llm_layers` | 6 | **Critical** - Prevents OOM / é˜²æ­¢æº¢å‡º |
| `--seq_len` | 96-512 | Balance accuracy/memory / å¹³è¡¡ç²¾åº¦å’Œå†…å­˜ |
| `--d_model` | 16-32 | Patch embedding dimension / åˆ†å—åµŒå…¥ç»´åº¦ |
| `--d_ff` | 32-128 | FFN dimension / FFNç»´åº¦ |
| `--load_in_4bit` | True | Enable quantization / å¯ç”¨é‡åŒ– |

---

## 7. Common Commands / å¸¸ç”¨å‘½ä»¤

### Training with Qwen 2.5 3B (Recommended) / ä½¿ç”¨Qwen 2.5 3Bè®­ç»ƒï¼ˆæ¨èï¼‰

```bash
# WSL/Linux
bash ./scripts/TimeLLM_ETTm1_2.sh

# Or direct command / æˆ–ç›´æ¥å‘½ä»¤
python run_main.py \
  --task_name long_term_forecast \
  --is_training 1 \
  --root_path ./dataset/ETT-small/ \
  --data_path ETTm1.csv \
  --model TimeLLM \
  --data ETTm1 \
  --features M \
  --seq_len 512 \
  --pred_len 96 \
  --batch_size 4 \
  --llm_model QWEN \
  --llm_model_path "./base_models/Qwen2.5-3B" \
  --llm_dim 2048 \
  --llm_layers 6 \
  --load_in_4bit \
  --prompt_domain 1 \
  --train_epochs 10
```

### Training with GPT-2 (Fallback) / ä½¿ç”¨GPT-2è®­ç»ƒï¼ˆå¤‡é€‰ï¼‰

```bash
accelerate launch --num_processes 1 --mixed_precision fp16 run_main.py \
  --task_name long_term_forecast \
  --is_training 1 \
  --root_path ./dataset/ETT-small/ \
  --data_path ETTh1.csv \
  --model TimeLLM \
  --data ETTh1 \
  --features M \
  --seq_len 96 \
  --pred_len 96 \
  --batch_size 4 \
  --llm_model GPT2 \
  --llm_dim 768 \
  --llm_layers 6 \
  --train_epochs 10
```

### Environment Verification / ç¯å¢ƒéªŒè¯

```bash
# Check CUDA / æ£€æŸ¥CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')"

# Monitor GPU / ç›‘æ§GPU
watch -n 1 nvidia-smi
```

---

## 8. Code Structure / ä»£ç ç»“æ„

```
Time-LLM/
â”œâ”€â”€ CLAUDE.md                 # This guide / æœ¬æŒ‡å—
â”œâ”€â”€ README.md                 # Original docs / åŸå§‹æ–‡æ¡£
â”œâ”€â”€ report.md                 # Thesis proposal / å¼€é¢˜æŠ¥å‘Š
â”œâ”€â”€ baogao.md                 # Thesis proposal (formal) / å¼€é¢˜æŠ¥å‘Šï¼ˆæ­£å¼ï¼‰
â”‚
â”œâ”€â”€ md/                       # Documentation / æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ chuangxin*.md        # Innovation schemes / åˆ›æ–°æ–¹æ¡ˆ
â”‚   â”œâ”€â”€ work*.md             # Technical analysis / æŠ€æœ¯åˆ†æ
â”‚   â”œâ”€â”€ Trainable-part.md    # Trainable params / å¯è®­ç»ƒå‚æ•°
â”‚   â”œâ”€â”€ mingling*.md         # Command guides / å‘½ä»¤æŒ‡å—
â”‚   â””â”€â”€ wenti.md             # Troubleshooting / æ•…éšœæ’é™¤
â”‚
â”œâ”€â”€ Task_Requirements/        # Thesis requirements / æ¯•è®¾è¦æ±‚
â”‚   â”œâ”€â”€ task.md              # Task description / ä»»åŠ¡æè¿°
â”‚   â””â”€â”€ Reference-writing-style.md
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ TimeLLM.py           # Core model / æ ¸å¿ƒæ¨¡å‹ â­
â”‚
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ Embed.py             # PatchEmbedding / åˆ†å—åµŒå…¥
â”‚   â””â”€â”€ StandardNorm.py      # Instance normalization / å®ä¾‹å½’ä¸€åŒ–
â”‚
â”œâ”€â”€ data_provider/
â”‚   â”œâ”€â”€ data_factory.py      # Dataset router / æ•°æ®é›†è·¯ç”±
â”‚   â””â”€â”€ data_loader.py       # Dataset classes / æ•°æ®é›†ç±»
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ ETT-small/           # ETTh1, ETTh2, ETTm1, ETTm2
â”‚   â””â”€â”€ prompt_bank/         # Domain prompts / é¢†åŸŸæç¤º
â”‚
â”œâ”€â”€ base_models/
â”‚   â”œâ”€â”€ gpt2/                # GPT-2 weights / GPT-2æƒé‡
â”‚   â””â”€â”€ Qwen2.5-3B/          # Qwen 2.5 3B weights / Qwenæƒé‡
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ TimeLLM_ETTh1.sh     # ETTh1 training script / è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ TimeLLM_ETTm1_2.sh   # WSL optimized script / WSLä¼˜åŒ–è„šæœ¬
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ run_main.py              # Main entry point / ä¸»å…¥å£ â­
â”œâ”€â”€ run_m4.py                # M4 benchmark entry / M4åŸºå‡†å…¥å£
â””â”€â”€ utils/
    â”œâ”€â”€ tools.py             # Utilities / å·¥å…·å‡½æ•°
    â””â”€â”€ metrics.py           # Evaluation metrics / è¯„ä¼°æŒ‡æ ‡
```

---

## 9. Troubleshooting / æ•…éšœæ’é™¤

### Common Issues / å¸¸è§é—®é¢˜

**Full troubleshooting guide / å®Œæ•´æ•…éšœæ’é™¤æŒ‡å—**: `md/wenti.md`

| Issue / é—®é¢˜ | Solution / è§£å†³æ–¹æ¡ˆ |
|--------------|---------------------|
| `KeyError: 'qwen2'` | `pip install "transformers>=4.40.0" --upgrade` |
| `CUDA_HOME does not exist` | `pip uninstall deepspeed -y` |
| `Input type mismatch (bfloat16 vs float32)` | Fixed in `models/TimeLLM.py:297-298` |
| `AttributeError: 'content'` | Fixed in `run_main.py:133-134` |
| OOM (Out of Memory) / æ˜¾å­˜æº¢å‡º | Reduce `--batch_size` to 2, `--llm_layers` to 4 |
| Slow convergence / æ”¶æ•›æ…¢ | Increase `--learning_rate` to 0.01 |

### Critical Code Fixes Applied / å·²åº”ç”¨çš„å…³é”®ä¿®å¤

1. **`run_main.py` Line 107**: Removed `deepspeed_plugin` for single GPU / ç§»é™¤deepspeed_pluginæ”¯æŒå•GPU
2. **`run_main.py` Lines 133-134**: Moved `load_content()` before model creation / å°†load_contentç§»è‡³æ¨¡å‹åˆ›å»ºå‰
3. **`models/TimeLLM.py` Lines 297-298**: Fixed dtype mismatch for 4-bit quantization / ä¿®å¤4-bité‡åŒ–æ•°æ®ç±»å‹ä¸åŒ¹é…

---

## Quick Reference Card / å¿«é€Ÿå‚è€ƒå¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Time-LLM Quick Reference                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Innovation Docs / åˆ›æ–°æ–‡æ¡£:                                  â”‚
â”‚   md/chuangxin.md          - Overview / æ¦‚è¿°                 â”‚
â”‚   md/chuangxin-lilun.md    - Theory / ç†è®º                   â”‚
â”‚   md/chuangxin-shijian.md  - Implementation / å®ç°           â”‚
â”‚   md/chuangxin-jieshi1.md  - Scheme 1 Deep Dive / æ–¹æ¡ˆä¸€è¯¦è§£ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Technical Docs / æŠ€æœ¯æ–‡æ¡£:                                   â”‚
â”‚   md/work2.md              - Deep analysis / æ·±åº¦åˆ†æ        â”‚
â”‚   md/Trainable-part.md     - Trainable params / å¯è®­ç»ƒå‚æ•°   â”‚
â”‚   md/wenti.md              - Troubleshooting / æ•…éšœæ’é™¤      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Key Parameters / å…³é”®å‚æ•°:                                   â”‚
â”‚   --llm_layers 6           â­ CRITICAL for 6GB VRAM          â”‚
â”‚   --batch_size 4           â­ CRITICAL for 6GB VRAM          â”‚
â”‚   --load_in_4bit           â­ Enable 4-bit quantization      â”‚
â”‚   --prompt_domain 1        Load domain prompts               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Priority Innovation / ä¼˜å…ˆåˆ›æ–°:                              â”‚
â”‚   1. Hybrid Model (Scheme 1) - md/chuangxin-jieshi1.md      â”‚
â”‚   2. Inter-Variate Attention - md/chuangxin-shijian.md Â§4   â”‚
â”‚   3. Frequency Enhancement - md/chuangxin-shijian.md Â§3     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Last Updated / æœ€åæ›´æ–°**: 2026-01-11
**Project Status / é¡¹ç›®çŠ¶æ€**: Innovation implementation phase / åˆ›æ–°å®ç°é˜¶æ®µ
**Hardware / ç¡¬ä»¶**: NVIDIA GTX 1660 Ti (6GB) + Qwen 2.5 3B (4-bit)
- 3. è®­ç»ƒå®Œæˆåæ¨ç†
# åŠ è½½ checkpoint (ä¸æ˜¯ checkpoint_step_N)
ckpt = torch.load('checkpoints/.../checkpoint')
model.load_state_dict(ckpt['model'])
model.eval()è¿™ä¸ªè®­ç»ƒå®Œæˆåæ¨ç†,ckptç›¸å…³å‚æ•°,åœ¨å“ªé‡Œä½“ç°çš„?