# Time-LLM è®­ç»ƒå‘½ä»¤è¯¦è§£ (Command Parameters Guide)

> **é€‚ç”¨ç‰ˆæœ¬**: Qwen 2.5 3B + 4-bit é‡åŒ–
> **ç¡¬ä»¶è¦æ±‚**: 6GB æ˜¾å­˜
> **æ•°æ®é›†**: ETTm1

---

## ğŸ“‹ å®Œæ•´è®­ç»ƒå‘½ä»¤

```powershell
python run_main.py ^
  --task_name long_term_forecast ^
  --is_training 1 ^
  --root_path ./dataset/ETT-small/ ^
  --data_path ETTm1.csv ^
  --model_id ETTm1_512_96 ^
  --model_comment Qwen3B ^
  --model TimeLLM ^
  --data ETTm1 ^
  --features M ^
  --seq_len 512 ^
  --label_len 48 ^
  --pred_len 96 ^
  --e_layers 2 ^
  --d_layers 1 ^
  --factor 3 ^
  --enc_in 7 ^
  --dec_in 7 ^
  --c_out 7 ^
  --batch_size 8 ^
  --d_model 32 ^
  --d_ff 32 ^
  --llm_dim 2048 ^
  --llm_layers 6 ^
  --num_workers 2 ^
  --prompt_domain 1 ^
  --train_epochs 10 ^
  --itr 1 ^
  --dropout 0.1 ^
  --llm_model QWEN ^
  --llm_model_path "e:\timellm\Time-LLM\base_models\Qwen2.5-3B" ^
  --load_in_4bit
```

---

## ğŸ“– å‚æ•°è¯¦è§£

### 1ï¸âƒ£ åŸºç¡€é…ç½® (Basic Config)

#### `--task_name long_term_forecast`
- **å«ä¹‰**: ä»»åŠ¡ç±»å‹
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 30 è¡Œ
- **å¯é€‰å€¼**:
  - `long_term_forecast` (é•¿æœŸé¢„æµ‹ï¼Œæ¨è)
  - `short_term_forecast` (çŸ­æœŸé¢„æµ‹)
  - `imputation` (ç¼ºå¤±å€¼å¡«å……)
  - `classification` (åˆ†ç±»)
  - `anomaly_detection` (å¼‚å¸¸æ£€æµ‹)
- **ä½œç”¨**: å†³å®šæ¨¡å‹çš„è¾“å‡ºæ ¼å¼å’ŒæŸå¤±å‡½æ•°ç±»å‹

#### `--is_training 1`
- **å«ä¹‰**: è®­ç»ƒæ¨¡å¼å¼€å…³
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 32 è¡Œ
- **å¯é€‰å€¼**: `1` (è®­ç»ƒ), `0` (æµ‹è¯•)
- **ä½œç”¨**: æ§åˆ¶æ˜¯è®­ç»ƒè¿˜æ˜¯ä»…æ¨ç†è¯„ä¼°

#### `--model_id ETTm1_512_96`
- **å«ä¹‰**: æ¨¡å‹æ ‡è¯†ç¬¦
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 33 è¡Œ
- **æ ¼å¼**: `{æ•°æ®é›†}_{è¾“å…¥é•¿åº¦}_{é¢„æµ‹é•¿åº¦}`
- **ä½œç”¨**: ç”Ÿæˆå”¯ä¸€çš„ checkpoint æ–‡ä»¶å¤¹åç§°

#### `--model_comment Qwen3B`
- **å«ä¹‰**: æ¨¡å‹å¤‡æ³¨
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 34 è¡Œ
- **ä½œç”¨**: é™„åŠ åˆ° checkpoint åç§°æœ«å°¾ï¼Œä¾¿äºåŒºåˆ†ä¸åŒå®éªŒ

#### `--model TimeLLM`
- **å«ä¹‰**: é€‰æ‹©æ¨¡å‹æ¶æ„
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 35 è¡Œ
- **å¯é€‰å€¼**: `TimeLLM`, `Autoformer`, `DLinear`
- **ä½œç”¨**: å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹ç±»
- **å¯¹åº”ä»£ç **: `run_main.py` ç¬¬ 138 è¡Œ â†’ `model = TimeLLM.Model(args).float()`

---

### 2ï¸âƒ£ æ•°æ®é…ç½® (Data Config)

#### `--data ETTm1`
- **å«ä¹‰**: æ•°æ®é›†åç§°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 40 è¡Œ
- **å¯é€‰å€¼**: `ETTh1`, `ETTh2`, `ETTm1`, `ETTm2`, `Weather`, `ECL`, `Traffic`
- **ä½œç”¨**: é€šè¿‡ `data_provider/data_factory.py` è·¯ç”±åˆ°å¯¹åº”æ•°æ®é›†ç±»
  - ETTm1/ETTm2 â†’ `Dataset_ETT_minute`
  - ETTh1/ETTh2 â†’ `Dataset_ETT_hour`

#### `--root_path ./dataset/ETT-small/`
- **å«ä¹‰**: æ•°æ®é›†æ ¹ç›®å½•
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 41 è¡Œ
- **ä½œç”¨**: æŒ‡å®šæ•°æ®æ–‡ä»¶å­˜æ”¾ä½ç½®
- **å®Œæ•´è·¯å¾„**: `{root_path}/{data_path}` = `./dataset/ETT-small/ETTm1.csv`

#### `--data_path ETTm1.csv`
- **å«ä¹‰**: æ•°æ®æ–‡ä»¶å
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 42 è¡Œ
- **ä½œç”¨**: ä¸ `root_path` æ‹¼æ¥æˆå®Œæ•´è·¯å¾„

#### `--features M`
- **å«ä¹‰**: é¢„æµ‹ä»»åŠ¡ç±»å‹
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 43 è¡Œ
- **å¯é€‰å€¼**:
  - `M` (Multivariate): å¤šå˜é‡é¢„æµ‹å¤šå˜é‡ (7â†’7)
  - `S` (Single): å•å˜é‡é¢„æµ‹å•å˜é‡ (1â†’1)
  - `MS` (Multivariate-to-Single): å¤šå˜é‡é¢„æµ‹å•å˜é‡ (7â†’1)
- **ä½œç”¨**: å†³å®šè¾“å…¥è¾“å‡ºçš„å˜é‡æ•°é‡

---

### 3ï¸âƒ£ æ—¶åºé•¿åº¦é…ç½® (Sequence Length)

#### `--seq_len 512`
- **å«ä¹‰**: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆå†å²è§‚æµ‹çª—å£ï¼‰
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 56 è¡Œ
- **ä½œç”¨**: æ¨¡å‹è§‚å¯Ÿè¿‡å» 512 ä¸ªæ—¶é—´æ­¥
- **æ˜¾å­˜å½±å“**: seq_len è¶Šå¤§ï¼Œæ˜¾å­˜å ç”¨è¶Šé«˜
- **æ¨èå€¼**: 6GB æ˜¾å­˜ä¸‹æ¨è 96-512

#### `--label_len 48`
- **å«ä¹‰**: Decoder èµ·å§‹ token é•¿åº¦
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 57 è¡Œ
- **ä½œç”¨**: Decoder è¾“å…¥çš„å‰ 48 æ­¥ä½¿ç”¨çœŸå€¼ï¼Œåé¢å¡«é›¶
- **æ¨èå€¼**: é€šå¸¸è®¾ä¸º `pred_len / 2`

#### `--pred_len 96`
- **å«ä¹‰**: é¢„æµ‹é•¿åº¦ï¼ˆæœªæ¥é¢„æµ‹çª—å£ï¼‰
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 58 è¡Œ
- **ä½œç”¨**: æ¨¡å‹é¢„æµ‹æœªæ¥ 96 ä¸ªæ—¶é—´æ­¥
- **å¸¸ç”¨å€¼**: 96, 192, 336, 720

**æ•°æ®æµç¤ºä¾‹**:
```
è¾“å…¥: [è¿‡å» 512 æ­¥] â†’ è¾“å‡º: [æœªæ¥ 96 æ­¥]
```

---

### 4ï¸âƒ£ æ¨¡å‹ç»“æ„é…ç½® (Model Architecture)

#### `--enc_in 7`
- **å«ä¹‰**: Encoder è¾“å…¥å˜é‡æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 62 è¡Œ
- **ä½œç”¨**: ETT æ•°æ®é›†æœ‰ 7 ä¸ªç‰¹å¾ (HUFL, HULL, MUFL, MULL, LUFL, LULL, OT)

#### `--dec_in 7`
- **å«ä¹‰**: Decoder è¾“å…¥å˜é‡æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 63 è¡Œ
- **ä½œç”¨**: ä¸ `enc_in` ä¿æŒä¸€è‡´

#### `--c_out 7`
- **å«ä¹‰**: è¾“å‡ºå˜é‡æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 64 è¡Œ
- **ä½œç”¨**: features=M æ—¶ï¼Œc_out=7ï¼ˆé¢„æµ‹å…¨éƒ¨ 7 ä¸ªå˜é‡ï¼‰

#### `--d_model 32`
- **å«ä¹‰**: æ¨¡å‹éšè—å±‚ç»´åº¦ï¼ˆPatch Embedding ç»´åº¦ï¼‰
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 65 è¡Œ
- **ä½œç”¨**: æ§åˆ¶ PatchEmbedding çš„è¾“å‡ºç»´åº¦
- **æ˜¾å­˜å½±å“**: d_model è¶Šå¤§ï¼Œå¯è®­ç»ƒå‚æ•°è¶Šå¤š
- **æ¨èå€¼**: 6GB æ˜¾å­˜ä¸‹æ¨è 16-32

#### `--n_heads 8`
- **å«ä¹‰**: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 66 è¡Œ
- **ä½œç”¨**: Reprogramming Layer çš„æ³¨æ„åŠ›å¤´æ•°
- **é»˜è®¤å€¼**: 8

#### `--e_layers 2`
- **å«ä¹‰**: Encoder å±‚æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 67 è¡Œ
- **ä½œç”¨**: Time-LLM ä¸­ä¿ç•™å…¼å®¹æ€§å‚æ•°

#### `--d_layers 1`
- **å«ä¹‰**: Decoder å±‚æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 68 è¡Œ
- **ä½œç”¨**: Time-LLM ä¸­ä¿ç•™å…¼å®¹æ€§å‚æ•°

#### `--factor 3`
- **å«ä¹‰**: æ³¨æ„åŠ›å› å­
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 71 è¡Œ
- **ä½œç”¨**: Time-LLM ä¸­ä¿ç•™å…¼å®¹æ€§å‚æ•°

#### `--d_ff 32`
- **å«ä¹‰**: Feed-Forward ç½‘ç»œç»´åº¦
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 69 è¡Œ
- **ä½œç”¨**:
  - æ§åˆ¶ LLM è¾“å‡ºæˆªå–çš„ç»´åº¦æ•°
  - `dec_out = dec_out[:, :, :d_ff]` (TimeLLM.py ç¬¬ 301 è¡Œ)
  - å½±å“ FlattenHead è¾“å…¥ç»´åº¦: `head_nf = d_ff * num_patches`
- **æ¨èå€¼**: 6GB æ˜¾å­˜ä¸‹æ¨è 32

#### `--dropout 0.1`
- **å«ä¹‰**: Dropout æ¦‚ç‡
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 72 è¡Œ
- **ä½œç”¨**: é˜²æ­¢è¿‡æ‹Ÿåˆ

---

### 5ï¸âƒ£ LLM é…ç½® (LLM Config) â­ æœ€å…³é”®

#### `--llm_model QWEN`
- **å«ä¹‰**: LLM åŸºåº§æ¨¡å‹ç±»å‹
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 80 è¡Œ
- **å¯é€‰å€¼**: `LLAMA`, `GPT2`, `BERT`, `QWEN`
- **ä½œç”¨**: å½“è®¾ç½® `llm_model_path` æ—¶ï¼Œæ­¤å‚æ•°ä»…ç”¨äºæ ‡è¯†
- **è¯´æ˜**: ä½¿ç”¨ `llm_model_path` æ—¶ï¼Œå®é™…åŠ è½½ç”± `AutoModel` å®Œæˆ

#### `--llm_dim 2048` â­
- **å«ä¹‰**: LLM éšè—å±‚ç»´åº¦
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 81 è¡Œ
- **ä¸åŒ LLM çš„ç»´åº¦**:
  - **Qwen 2.5 3B: 2048** âœ…
  - Llama-7B: 4096
  - GPT-2: 768
  - BERT: 768
- **ä½œç”¨**:
  - å†³å®š Reprogramming Layer çš„è¾“å‡ºç»´åº¦
  - å†³å®š Prompt Embeddings çš„ç»´åº¦
  - **å¿…é¡»ä¸æ¨¡å‹çš„ `config.hidden_size` åŒ¹é…ï¼**
- **å¯¹åº”ä»£ç **: `models/TimeLLM.py` ç¬¬ 39 è¡Œ `self.d_llm = configs.llm_dim`

#### `--llm_model_path "e:\timellm\Time-LLM\base_models\Qwen2.5-3B"` â­
- **å«ä¹‰**: LLM æ¨¡å‹æœ¬åœ°è·¯å¾„æˆ– HuggingFace ID
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 83 è¡Œï¼ˆæ–°å¢å‚æ•°ï¼‰
- **ä½œç”¨**:
  - æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨é€šç”¨åŠ è½½é€»è¾‘ (`AutoModel`)
  - æ”¯æŒä»»æ„ HuggingFace æ¨¡å‹
  - è‹¥ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ `llm_model` å‚æ•°æŒ‡å®šçš„ç¡¬ç¼–ç è·¯å¾„
- **å¯¹åº”ä»£ç **: `models/TimeLLM.py` ç¬¬ 43-96 è¡Œ

#### `--load_in_4bit` â­â­â­
- **å«ä¹‰**: å¯ç”¨ 4-bit é‡åŒ–
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 84 è¡Œï¼ˆæ–°å¢å‚æ•°ï¼‰
- **ä½œç”¨**:
  - ä½¿ç”¨ BitsAndBytesConfig è¿›è¡Œ NF4 é‡åŒ–
  - **æ˜¾å­˜ä» ~6GB é™è‡³ ~1.5GBï¼**
  - é€‚é… 6GB æ˜¾å¡çš„æ ¸å¿ƒé…ç½®
- **é‡åŒ–é…ç½®**ï¼ˆ`models/TimeLLM.py` ç¬¬ 52-57 è¡Œï¼‰:
  ```python
  BitsAndBytesConfig(
      load_in_4bit=True,
      bnb_4bit_compute_dtype=torch.float16,
      bnb_4bit_use_double_quant=True,
      bnb_4bit_quant_type="nf4"
  )
  ```

#### `--llm_layers 6` â­â­â­ æœ€é‡è¦ï¼
- **å«ä¹‰**: ä½¿ç”¨çš„ LLM å±‚æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 101 è¡Œ
- **ä½œç”¨**:
  - è®¾ç½® LLM config çš„ `num_hidden_layers`
  - æ§åˆ¶ LLM çš„æ·±åº¦ï¼Œç›´æ¥å½±å“æ˜¾å­˜å ç”¨
- **ä¸ºä»€ä¹ˆå¿…é¡»æŒ‡å®šï¼Ÿ**
  - Qwen 2.5 3B åŸå§‹æœ‰ **32 å±‚**
  - ä¸æŒ‡å®šä¼šä½¿ç”¨å…¨éƒ¨ 32 å±‚ï¼Œå¯¼è‡´ **æ˜¾å­˜çˆ†ç‚¸ï¼ˆ~8GBï¼‰**
  - è®¾ä¸º 6 å±‚å¯å°†æ˜¾å­˜é™è‡³ **~1.5GB**
- **å¯¹åº”ä»£ç **: `models/TimeLLM.py` ç¬¬ 60 è¡Œ
  ```python
  self.llm_config.num_hidden_layers = configs.llm_layers
  ```
- **æ¨èå€¼**: 6GB æ˜¾å­˜ä¸‹æ¨è **6**

---

### 6ï¸âƒ£ è®­ç»ƒä¼˜åŒ–é…ç½® (Training Config)

#### `--batch_size 8`
- **å«ä¹‰**: è®­ç»ƒæ‰¹å¤§å°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 92 è¡Œ
- **ä½œç”¨**: ä¸€æ¬¡è®­ç»ƒå¤„ç†çš„æ ·æœ¬æ•°
- **æ˜¾å­˜å½±å“**: batch_size è¶Šå¤§ï¼Œæ˜¾å­˜å ç”¨è¶Šé«˜
- **æ¨èå€¼**:
  - Qwen 2.5 3B + 4-bit: **8-16**
  - è‹¥ OOMï¼Œé™è‡³ **4 æˆ– 2**

#### `--num_workers 2` â­
- **å«ä¹‰**: æ•°æ®åŠ è½½å™¨çš„è¿›ç¨‹æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 88 è¡Œ
- **é»˜è®¤å€¼**: 10ï¼ˆå¤ªé«˜ï¼ï¼‰
- **ä½œç”¨**: æ§åˆ¶æ•°æ®é¢„å¤„ç†çš„å¹¶è¡Œåº¦
- **ä¸ºä»€ä¹ˆè¦è®¾ä¸º 2ï¼Ÿ**
  - é»˜è®¤å€¼ 10 ä¼šå¯åŠ¨ 10 ä¸ªæ•°æ®åŠ è½½è¿›ç¨‹
  - å ç”¨è¿‡å¤š CPU å†…å­˜ï¼Œå¯èƒ½å¯¼è‡´ç³»ç»Ÿå¡é¡¿
  - 6GB æ˜¾å­˜ç¯å¢ƒæ¨è **2**

#### `--train_epochs 10`
- **å«ä¹‰**: è®­ç»ƒè½®æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 90 è¡Œ
- **ä½œç”¨**: å®Œæ•´éå†è®­ç»ƒé›†çš„æ¬¡æ•°
- **æ¨èå€¼**: 10-100ï¼ˆå–å†³äºæ•°æ®é›†å¤§å°ï¼‰

#### `--itr 1`
- **å«ä¹‰**: å®éªŒé‡å¤æ¬¡æ•°
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 89 è¡Œ
- **ä½œç”¨**: é‡å¤è®­ç»ƒå¤šæ¬¡ä»¥è®¡ç®—ç»Ÿè®¡é‡
- **å¯¹åº”ä»£ç **: `run_main.py` ç¬¬ 109 è¡Œï¼ˆå¤–å±‚å¾ªç¯ `for ii in range(args.itr)`ï¼‰

#### `--prompt_domain 1` â­â­â­
- **å«ä¹‰**: æ˜¯å¦ä½¿ç”¨é¢†åŸŸç‰¹å®šæç¤ºè¯
- **ä»£ç ä½ç½®**: `run_main.py` ç¬¬ 79 è¡Œ
- **ä½œç”¨**:
  - `1`: ä» `dataset/prompt_bank/ETT.txt` åŠ è½½æè¿° âœ…
  - `0`: ä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤æè¿°
- **ä¸ºä»€ä¹ˆå¿…é¡»è®¾ä¸º 1ï¼Ÿ**
  - ä¸è®¾ç½®ä¼šä½¿ç”¨ä¸å®Œæ•´çš„é»˜è®¤æè¿°
  - æ— æ³•åŠ è½½ä½ ä¿®å¤åçš„ ETT.txtï¼ˆåŒ…å« ETTm2ï¼‰
- **å¯¹åº”ä»£ç **: `models/TimeLLM.py` ç¬¬ 223-226 è¡Œ
  ```python
  if configs.prompt_domain:
      self.description = configs.content  # ä» ETT.txt åŠ è½½
  else:
      self.description = 'The Electricity Transformer Temperature...'  # ç¡¬ç¼–ç 
  ```
- **åŠ è½½æµç¨‹**: `run_main.py` ç¬¬ 142 è¡Œ â†’ `utils/tools.py` ç¬¬ 226-233 è¡Œ

---

## ğŸ”— å‚æ•°ä¸ä»£ç å¯¹åº”å…³ç³»

### å…³é”®æ•°æ®æµè·¯å¾„

```
1ï¸âƒ£ å‚æ•°è§£æ
   run_main.py ç¬¬ 104 è¡Œ: args = parser.parse_args()

2ï¸âƒ£ åŠ è½½é¢†åŸŸæç¤ºè¯
   run_main.py ç¬¬ 142 è¡Œ: args.content = load_content(args)
   â†’ utils/tools.py ç¬¬ 226-233 è¡Œ
   â†’ è¯»å– dataset/prompt_bank/ETT.txt

3ï¸âƒ£ åŠ è½½æ•°æ®
   run_main.py ç¬¬ 129-131 è¡Œ:
   train_data, train_loader = data_provider(args, 'train')
   â†’ data_provider/data_factory.py

4ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹
   run_main.py ç¬¬ 138 è¡Œ: model = TimeLLM.Model(args).float()
   â†’ models/TimeLLM.py ç¬¬ 30-250 è¡Œ

5ï¸âƒ£ æ¨¡å‹åŠ è½½ï¼ˆQwen 2.5 3Bï¼‰
   models/TimeLLM.py ç¬¬ 43-96 è¡Œ:
   - æ£€æµ‹åˆ° llm_model_path ä¸ä¸ºç©º
   - ä½¿ç”¨ AutoModel.from_pretrained()
   - åº”ç”¨ 4-bit é‡åŒ–é…ç½®ï¼ˆç¬¬ 50-57 è¡Œï¼‰
   - è®¾ç½®å±‚æ•°ï¼ˆç¬¬ 60 è¡Œï¼‰

6ï¸âƒ£ è®­ç»ƒå¾ªç¯
   run_main.py ç¬¬ 150-ç»“æŸ
```

---

## ğŸ“Š æ˜¾å­˜å ç”¨ä¼°ç®—

| ç»„ä»¶ | æ˜¾å­˜å ç”¨ | å‚æ•°æ§åˆ¶ |
|------|---------|---------|
| Qwen 2.5 3B (4-bit, 6å±‚) | ~1.5 GB | `--load_in_4bit`, `--llm_layers` |
| Mapping Layer | ~0.2 GB | `--llm_dim` |
| Reprogramming Layer | ~0.1 GB | `--d_model`, `--llm_dim` |
| Patch Embeddings | ~0.05 GB | `--d_model`, `--seq_len` |
| ä¸­é—´å˜é‡ (batch=8) | ~2.0 GB | `--batch_size`, `--seq_len` |
| æ¢¯åº¦ç¼“å­˜ | ~0.5 GB | å¯è®­ç»ƒå‚æ•°é‡ |
| ç³»ç»Ÿå ç”¨ | ~0.65 GB | - |
| **æ€»è®¡** | **~5.0 GB** | âœ… 6GB æ˜¾å­˜è¶³å¤Ÿï¼ |

---

## âš ï¸ æ–°å¢å‚æ•°è¯´æ˜

ä»¥ä¸‹ **6 ä¸ªå‚æ•°** æ˜¯åœ¨ä½ åŸå§‹å‘½ä»¤åŸºç¡€ä¸Šæ–°å¢çš„ï¼Œå‡ä¸º**å¿…éœ€æˆ–å¼ºçƒˆæ¨è**ï¼š

| å‚æ•° | é‡è¦æ€§ | åŸå›  |
|------|--------|------|
| `--llm_layers 6` | â­â­â­ å¿…éœ€ | ä¸è®¾ç½®ä¼šä½¿ç”¨ 32 å±‚ï¼Œæ˜¾å­˜çˆ†ç‚¸ |
| `--prompt_domain 1` | â­â­â­ å¿…éœ€ | ä¸è®¾ç½®æ— æ³•åŠ è½½ ETT.txt æè¿° |
| `--num_workers 2` | â­â­ å¼ºçƒˆæ¨è | é»˜è®¤ 10 ä¼šå ç”¨è¿‡å¤š CPU å†…å­˜ |
| `--train_epochs 10` | â­ æ¨è | æ˜ç¡®æŒ‡å®šè®­ç»ƒè½®æ•° |
| `--itr 1` | â­ æ¨è | æ˜ç¡®æŒ‡å®šå®éªŒæ¬¡æ•° |

---

## ğŸš¨ å¸¸è§é—®é¢˜ä¸è§£å†³

### é—®é¢˜ 1ï¼šOOM (æ˜¾å­˜ä¸è¶³)
**ç—‡çŠ¶**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰:
1. é™ä½ `--batch_size` ä¸º 4 æˆ– 2
2. é™ä½ `--seq_len` ä¸º 256
3. é™ä½ `--llm_layers` ä¸º 4

### é—®é¢˜ 2ï¼šæç¤ºè¯æœªåŠ è½½
**ç—‡çŠ¶**: è®­ç»ƒæ—¶æœªä½¿ç”¨ ETT.txt æè¿°

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®ä¿è®¾ç½® `--prompt_domain 1`
- æ£€æŸ¥ `dataset/prompt_bank/ETT.txt` æ˜¯å¦å­˜åœ¨

### é—®é¢˜ 3ï¼šæ¨¡å‹åŠ è½½å¤±è´¥
**ç—‡çŠ¶**: `Model files not found`

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®è®¤ `base_models/Qwen2.5-3B/` ä¸‹æœ‰å®Œæ•´çš„ `.safetensors` æ–‡ä»¶
- ç¡®è®¤ `config.json` å’Œ `tokenizer.json` å­˜åœ¨

---

## ğŸ“ ç”Ÿæˆçš„ Checkpoint è·¯å¾„

æ ¹æ®ä¸Šè¿°å‚æ•°ï¼Œcheckpoint å°†ä¿å­˜åˆ°ï¼š

```
checkpoints/long_term_forecast_ETTm1_512_96_TimeLLM_ETTm1_ftM_sl512_ll48_pl96_dm32_nh8_el2_dl1_df32_fc3_ebtimeF_test_0-Qwen3B/checkpoint
```

**è·¯å¾„è§£æ**:
- `long_term_forecast`: task_name
- `ETTm1_512_96`: model_id
- `TimeLLM`: model
- `ftM`: features=M
- `sl512`: seq_len=512
- `pl96`: pred_len=96
- `dm32`: d_model=32
- `test_0`: des=test, itr=0
- `Qwen3B`: model_comment

---

**æ–‡æ¡£ç”Ÿæˆæ—¶é—´**: 2024-12-08
**é€‚é…ç¡¬ä»¶**: 6GB VRAM GPU
**é€‚é…æ¨¡å‹**: Qwen 2.5 3B + 4-bit NF4 é‡åŒ–
