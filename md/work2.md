# Time-LLM æ·±åº¦æŠ€æœ¯è§£æ (Deep Technical Analysis)

> **ä»"æ€ä¹ˆè·‘"åˆ°"æ€ä¹ˆæ‡‚"** â€”â€” Time-LLM æ ¸å¿ƒæœºåˆ¶å®Œå…¨å‰–æ

---

## ä¸€ã€ä»£ç åº“æ‹“æ‰‘å›¾ (Project Topology)

### 1.1 é¡¹ç›®ç»“æ„æ ‘

```
Time-LLM/
â”œâ”€â”€ models/                          # æ ¸å¿ƒæ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ TimeLLM.py                  # â˜…â˜…â˜… Time-LLM ä¸»æ¨¡å‹ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”œâ”€â”€ Autoformer.py               # åŸºçº¿æ¨¡å‹ï¼šAutoformer
â”‚   â””â”€â”€ DLinear.py                  # åŸºçº¿æ¨¡å‹ï¼šDLinear
â”‚
â”œâ”€â”€ layers/                          # ç¥ç»ç½‘ç»œå±‚ç»„ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ Embed.py                    # â˜…â˜…â˜… PatchEmbeddingï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”œâ”€â”€ StandardNorm.py             # â˜…â˜…â˜… å®ä¾‹å½’ä¸€åŒ–å±‚ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”œâ”€â”€ Transformer_EncDec.py       # Transformer ç¼–ç å™¨/è§£ç å™¨
â”‚   â”œâ”€â”€ Autoformer_EncDec.py        # Autoformer ç¼–ç å™¨/è§£ç å™¨
â”‚   â”œâ”€â”€ SelfAttention_Family.py     # æ³¨æ„åŠ›æœºåˆ¶æ—
â”‚   â”œâ”€â”€ AutoCorrelation.py          # è‡ªç›¸å…³æœºåˆ¶
â”‚   â””â”€â”€ Conv_Blocks.py              # å·ç§¯å—
â”‚
â”œâ”€â”€ data_provider/                   # æ•°æ®åŠ è½½ä¸å¤„ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_factory.py             # â˜… æ•°æ®é›†å·¥å‚ï¼ˆè·¯ç”±å™¨ï¼‰
â”‚   â”œâ”€â”€ data_loader.py              # â˜…â˜… æ•°æ®åŠ è½½å™¨ï¼ˆETT/Weather/ECL/Trafficï¼‰
â”‚   â””â”€â”€ m4.py                       # M4 ç«èµ›æ•°æ®é›†åŠ è½½å™¨
â”‚
â”œâ”€â”€ data_provider_pretrain/          # é¢„è®­ç»ƒæ•°æ®åŠ è½½ï¼ˆè¿ç§»å­¦ä¹ ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_factory.py
â”‚   â””â”€â”€ data_loader.py
â”‚
â”œâ”€â”€ utils/                           # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tools.py                    # â˜… è®­ç»ƒå·¥å…·ï¼ˆEarlyStopping, vali, load_contentï¼‰
â”‚   â”œâ”€â”€ metrics.py                  # â˜… è¯„ä¼°æŒ‡æ ‡ï¼ˆMAE, MSE, RMSE, MAPE, MSPEï¼‰
â”‚   â”œâ”€â”€ losses.py                   # æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ timefeatures.py             # æ—¶é—´ç‰¹å¾ç¼–ç 
â”‚   â””â”€â”€ m4_summary.py               # M4 è¯„ä¼°æ±‡æ€»
â”‚
â”œâ”€â”€ dataset/                         # æ•°æ®é›†å­˜æ”¾ç›®å½•
â”‚   â”œâ”€â”€ prompt_bank/                # â˜…â˜… é¢†åŸŸæè¿°æç¤ºè¯åº“
â”‚   â”‚   â”œâ”€â”€ ETT.txt                 # ETT æ•°æ®é›†æè¿°
â”‚   â”‚   â”œâ”€â”€ Weather.txt             # Weather æ•°æ®é›†æè¿°
â”‚   â”‚   â”œâ”€â”€ ECL.txt                 # Electricity æ•°æ®é›†æè¿°
â”‚   â”‚   â””â”€â”€ m4.txt                  # M4 æ•°æ®é›†æè¿°
â”‚   â”œâ”€â”€ ETT-small/                  # ETT æ•°æ®é›†ï¼ˆéœ€ä¸‹è½½ï¼‰
â”‚   â”œâ”€â”€ weather/                    # Weather æ•°æ®é›†ï¼ˆéœ€ä¸‹è½½ï¼‰
â”‚   â”œâ”€â”€ electricity/                # Electricity æ•°æ®é›†ï¼ˆéœ€ä¸‹è½½ï¼‰
â”‚   â””â”€â”€ traffic/                    # Traffic æ•°æ®é›†ï¼ˆéœ€ä¸‹è½½ï¼‰
â”‚
â”œâ”€â”€ scripts/                         # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ TimeLLM_ETTh1.sh
â”‚   â”œâ”€â”€ TimeLLM_ETTh2.sh
â”‚   â”œâ”€â”€ TimeLLM_ETTm1.sh
â”‚   â”œâ”€â”€ TimeLLM_ETTm2.sh
â”‚   â”œâ”€â”€ TimeLLM_Weather.sh
â”‚   â”œâ”€â”€ TimeLLM_ECL.sh
â”‚   â”œâ”€â”€ TimeLLM_Traffic.sh
â”‚   â””â”€â”€ TimeLLM_M4.sh
â”‚
â”œâ”€â”€ checkpoints/                     # æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•ï¼ˆè®­ç»ƒæ—¶è‡ªåŠ¨åˆ›å»ºï¼‰
â”‚
â”œâ”€â”€ run_main.py                      # â˜…â˜…â˜… ä¸»è®­ç»ƒå…¥å£ï¼ˆé•¿æœŸé¢„æµ‹ï¼‰
â”œâ”€â”€ run_m4.py                        # M4 çŸ­æœŸé¢„æµ‹å…¥å£
â”œâ”€â”€ run_pretrain.py                  # é¢„è®­ç»ƒ/è¿ç§»å­¦ä¹ å…¥å£
â”‚
â”œâ”€â”€ ds_config_zero2.json             # DeepSpeed ZeRO-2 é…ç½®
â”œâ”€â”€ requirements.txt                 # Python ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md                        # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ CLAUDE.md                        # Claude Code é¡¹ç›®æŒ‡å—
â””â”€â”€ LICENSE                          # è®¸å¯è¯
```

---

### 1.2 æ ¸å¿ƒæ¨¡å—èŒè´£è¯¦è§£

#### ğŸ“¦ `models/` - æ¨¡å‹å®šä¹‰å±‚

| æ–‡ä»¶ | èŒè´£ | æ ¸å¿ƒç±»/å‡½æ•° |
|------|------|-----------|
| **TimeLLM.py** | Time-LLM ä¸»æ¨¡å‹å®ç° | `Model`ï¼ˆä¸»æ¨¡å‹ï¼‰ã€`ReprogrammingLayer`ï¼ˆé‡ç¼–ç¨‹å±‚ï¼‰ã€`FlattenHead`ï¼ˆè¾“å‡ºæŠ•å½±ï¼‰ |
| Autoformer.py | Autoformer åŸºçº¿æ¨¡å‹ | `Model` |
| DLinear.py | DLinear åŸºçº¿æ¨¡å‹ | `Model` |

**TimeLLM.py æ ¸å¿ƒç»„ä»¶ï¼š**
- **LLM åŠ è½½æ¨¡å—**ï¼ˆç¬¬ 43-154 è¡Œï¼‰ï¼šæ”¯æŒ LLAMAã€GPT-2ã€BERT ä¸‰ç§åŸºåº§æ¨¡å‹
- **PatchEmbedding**ï¼ˆç¬¬ 173-174 è¡Œï¼‰ï¼šå°†æ—¶é—´åºåˆ—åˆ‡åˆ†ä¸º Patch å¹¶åµŒå…¥
- **Mapping Layer**ï¼ˆç¬¬ 179 è¡Œï¼‰ï¼šå°† LLM è¯åµŒå…¥ç©ºé—´æ˜ å°„ä¸ºå¯å­¦ä¹ çš„ Token ç©ºé—´
- **ReprogrammingLayer**ï¼ˆç¬¬ 181 è¡Œ + ç¬¬ 267-305 è¡Œï¼‰ï¼šè·¨æ¨¡æ€å¯¹é½å±‚ï¼Œå°†æ—¶åº Patch æ˜ å°„åˆ° LLM åµŒå…¥ç©ºé—´
- **Normalize å±‚**ï¼ˆç¬¬ 192 è¡Œï¼‰ï¼šå®ä¾‹å½’ä¸€åŒ–ï¼Œä¿å­˜ç»Ÿè®¡é‡ç”¨äºåå½’ä¸€åŒ–
- **FlattenHead**ï¼ˆç¬¬ 15-27 è¡Œï¼‰ï¼šè¾“å‡ºæŠ•å½±å±‚ï¼Œå°† LLM è¾“å‡ºæ˜ å°„ä¸ºé¢„æµ‹åºåˆ—

---

#### ğŸ§± `layers/` - ç¥ç»ç½‘ç»œå±‚ç»„ä»¶

| æ–‡ä»¶ | èŒè´£ | æ ¸å¿ƒç±» |
|------|------|--------|
| **Embed.py** | åµŒå…¥å±‚ï¼ˆToken/Positional/Temporal/Patchï¼‰ | `PatchEmbedding`ï¼ˆæ ¸å¿ƒï¼‰ã€`TokenEmbedding`ã€`PositionalEmbedding` |
| **StandardNorm.py** | å®ä¾‹å½’ä¸€åŒ–ï¼ˆRevINï¼‰ | `Normalize`ï¼ˆæ”¯æŒ norm/denorm åŒå‘æ“ä½œï¼‰ |
| Transformer_EncDec.py | Transformer ç¼–ç å™¨/è§£ç å™¨ | `Encoder`, `Decoder`, `EncoderLayer`, `DecoderLayer` |
| SelfAttention_Family.py | æ³¨æ„åŠ›æœºåˆ¶ | `FullAttention`, `ProbAttention`, `AttentionLayer` |

**PatchEmbedding æ ¸å¿ƒæœºåˆ¶ï¼ˆEmbed.py ç¬¬ 160-186 è¡Œï¼‰ï¼š**
```python
class PatchEmbedding(nn.Module):
    def forward(self, x):
        # 1. Paddingï¼šå¤åˆ¶æœ€åä¸€ä¸ªå€¼è¿›è¡Œå¡«å……
        x = self.padding_patch_layer(x)

        # 2. Unfoldï¼šæ»‘åŠ¨çª—å£åˆ‡åˆ† Patch
        # x.shape: [B, N, T] -> [B, N, num_patches, patch_len]
        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)

        # 3. Reshapeï¼šå±•å¹³ Batch å’Œå˜é‡ç»´åº¦
        # [B, N, num_patches, patch_len] -> [B*N, num_patches, patch_len]
        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))

        # 4. TokenEmbeddingï¼šä½¿ç”¨ 1D å·ç§¯å°† Patch æ˜ å°„åˆ° d_model ç»´åº¦
        # [B*N, num_patches, patch_len] -> [B*N, num_patches, d_model]
        x = self.value_embedding(x)

        return self.dropout(x), n_vars
```

---

#### ğŸ“Š `data_provider/` - æ•°æ®ç®¡é“å±‚

| æ–‡ä»¶ | èŒè´£ | æ ¸å¿ƒåŠŸèƒ½ |
|------|------|---------|
| **data_factory.py** | æ•°æ®é›†è·¯ç”±å™¨ | `data_provider()` å‡½æ•°ï¼šæ ¹æ® `args.data` é€‰æ‹©å¯¹åº”çš„ Dataset ç±» |
| **data_loader.py** | æ•°æ®é›†åŠ è½½å™¨ | `Dataset_ETT_hour`, `Dataset_ETT_minute`, `Dataset_Custom` |
| m4.py | M4 ç«èµ›æ•°æ®åŠ è½½ | `M4Dataset`, `M4Meta` |

**æ•°æ®é›†åˆ‡åˆ†ç­–ç•¥ï¼ˆä»¥ ETTh1 ä¸ºä¾‹ï¼‰ï¼š**
- **Train**ï¼šå‰ 12 ä¸ªæœˆï¼ˆ8640 å°æ—¶ï¼‰
- **Validation**ï¼šä¸­é—´ 4 ä¸ªæœˆï¼ˆ2880 å°æ—¶ï¼‰
- **Test**ï¼šæœ€å 4 ä¸ªæœˆï¼ˆ2880 å°æ—¶ï¼‰

**æ•°æ®åŠ è½½æµç¨‹ï¼š**
1. è¯»å– CSV æ–‡ä»¶
2. é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆMï¼šå¤šå˜é‡ï¼ŒSï¼šå•å˜é‡ï¼ŒMSï¼šå¤šå˜é‡é¢„æµ‹å•å˜é‡ï¼‰
3. StandardScaler æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡ï¼‰
4. æ—¶é—´ç‰¹å¾ç¼–ç ï¼ˆæœˆ/æ—¥/æ˜ŸæœŸ/å°æ—¶ï¼‰
5. è¿”å›æ»‘åŠ¨çª—å£æ ·æœ¬ï¼š`(seq_x, seq_y, seq_x_mark, seq_y_mark)`

---

#### ğŸ› ï¸ `utils/` - å·¥å…·å‡½æ•°å±‚

| æ–‡ä»¶ | èŒè´£ | æ ¸å¿ƒå‡½æ•° |
|------|------|---------|
| **tools.py** | è®­ç»ƒå·¥å…· | `EarlyStopping`ï¼ˆæ—©åœï¼‰ã€`vali()`ï¼ˆéªŒè¯ï¼‰ã€`adjust_learning_rate()`ã€`load_content()`ï¼ˆåŠ è½½ Promptï¼‰ |
| **metrics.py** | è¯„ä¼°æŒ‡æ ‡ | `MAE()`, `MSE()`, `RMSE()`, `MAPE()`, `MSPE()`, `metric()`ï¼ˆä¸€æ¬¡æ€§è®¡ç®—å…¨éƒ¨ï¼‰ |
| losses.py | æŸå¤±å‡½æ•° | `mape_loss`, `mase_loss`, `smape_loss`ï¼ˆM4 ä½¿ç”¨ï¼‰ |
| timefeatures.py | æ—¶é—´ç‰¹å¾ | `time_features()` æå–å‘¨æœŸæ€§ç‰¹å¾ |

---

#### ğŸ“ `dataset/prompt_bank/` - æç¤ºè¯åº“

å­˜å‚¨æ¯ä¸ªæ•°æ®é›†çš„é¢†åŸŸæè¿°æ–‡æœ¬ï¼Œç”¨äºæ„å»ºåŠ¨æ€ Promptã€‚

**ç¤ºä¾‹ï¼ˆETT.txtï¼‰ï¼š**
```
The Electricity Transformer Temperature (ETT) is a crucial indicator in the
electric power long-term deployment.
```

åœ¨ `TimeLLM.py` ç¬¬ 220 è¡Œæ„å»ºå®Œæ•´ Promptï¼š
```python
prompt_ = (
    f"<|start_prompt|>Dataset description: {self.description}"
    f"Task description: forecast the next {self.pred_len} steps given the previous {self.seq_len} steps information; "
    "Input statistics: "
    f"min value {min_values_str}, "
    f"max value {max_values_str}, "
    f"median value {median_values_str}, "
    f"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, "
    f"top 5 lags are : {lags_values_str}<|<end_prompt>|>"
)
```

---

## äºŒã€æ ¸å¿ƒæ•°æ®æµæœºåˆ¶ (Data Flow & Pipeline)

### 2.1 ç«¯åˆ°ç«¯æ•°æ®æµæ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Time-LLM æ•°æ®æµå…¨æ™¯å›¾                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

åŸå§‹æ—¶åºæ•°æ® (CSV)
    â†“
[1] æ•°æ®åŠ è½½ & é¢„å¤„ç† (data_loader.py)
    â”œâ”€ StandardScaler æ ‡å‡†åŒ–
    â”œâ”€ æ—¶é—´ç‰¹å¾ç¼–ç  (æœˆ/æ—¥/æ˜ŸæœŸ/å°æ—¶)
    â””â”€ æ»‘åŠ¨çª—å£åˆ‡ç‰‡
    â†“
è¾“å…¥å¼ é‡: x_enc [Batch, SeqLen, N_vars]
    â†“
[2] å®ä¾‹å½’ä¸€åŒ– (Normalize Layer)
    â”œâ”€ è®¡ç®—å‡å€¼/æ–¹å·®
    â””â”€ Z-score æ ‡å‡†åŒ–
    â†“
å½’ä¸€åŒ–æ•°æ®: x_enc [Batch, SeqLen, N_vars]
    â†“
[3] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â”ƒ  Prompt æ„å»º (ç»Ÿè®¡ç‰¹å¾æå–)          â”ƒ
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â”œâ”€ è®¡ç®— min, max, median
    â”œâ”€ è®¡ç®—è¶‹åŠ¿ (diff().sum())
    â”œâ”€ FFT è‡ªç›¸å…³ -> Top-5 Lags
    â””â”€ æ‹¼æ¥é¢†åŸŸæè¿° + ç»Ÿè®¡ä¿¡æ¯
    â†“
æ–‡æœ¬ Prompt â†’ Tokenizer â†’ prompt_embeddings [Batch, PromptLen, llm_dim]
    â†“
[4] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â”ƒ  Patching (æ—¶åºåˆ†å—åµŒå…¥)             â”ƒ
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â”œâ”€ Unfold: æ»‘åŠ¨çª—å£åˆ‡åˆ† Patch
    â”‚   x_enc [B, N, SeqLen] â†’ [B, N, num_patches, patch_len]
    â”œâ”€ Reshape: å±•å¹³ Batch å’Œå˜é‡
    â”‚   [B, N, num_patches, patch_len] â†’ [B*N, num_patches, patch_len]
    â””â”€ TokenEmbedding (1D Conv): æŠ•å½±åˆ° d_model
        [B*N, num_patches, patch_len] â†’ [B*N, num_patches, d_model]
    â†“
Patch Embeddings: enc_out [B*N, num_patches, d_model]
    â†“
[5] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â”ƒ  Reprogramming (è·¨æ¨¡æ€å¯¹é½)          â”ƒ
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â”œâ”€ Source Embeddings: LLM è¯åµŒå…¥ â†’ Mapping Layer
    â”‚   word_embeddings [vocab_size, llm_dim] â†’ [num_tokens, llm_dim]
    â”œâ”€ Cross-Attention (Query: Patch, Key/Value: Source)
    â”‚   Q = Linear(enc_out) [B*N, num_patches, d_keys*n_heads]
    â”‚   K = Linear(source_embeddings) [num_tokens, d_keys*n_heads]
    â”‚   V = Linear(source_embeddings) [num_tokens, d_keys*n_heads]
    â”‚   Attention = softmax(Q @ K^T / sqrt(d_keys)) @ V
    â””â”€ Output Projection: æ˜ å°„åˆ° llm_dim
        reprogrammed [B*N, num_patches, llm_dim]
    â†“
é‡ç¼–ç¨‹åçš„ Embeddings: enc_out [B*N, num_patches, llm_dim]
    â†“
[6] æ‹¼æ¥ Prompt + Patch Embeddings
    llama_enc_out = Concat([prompt_embeddings, enc_out], dim=1)
    Shape: [B*N, PromptLen + num_patches, llm_dim]
    â†“
[7] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â”ƒ  LLM Forward (å†»ç»“çš„ LLM Backbone)   â”ƒ
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â””â”€ GPT2/LLAMA/BERT å‰å‘ä¼ æ’­ï¼ˆå‚æ•°å†»ç»“ï¼‰
    â†“
LLM è¾“å‡º: dec_out [B*N, PromptLen + num_patches, llm_dim]
    â†“
[8] æˆªå–å‰ d_ff ç»´åº¦ + æå– Patch éƒ¨åˆ†
    dec_out = dec_out[:, :, :d_ff]  # å–å‰ d_ff ç»´
    dec_out = Reshape([B, N, total_len, d_ff])
    dec_out = dec_out[:, :, :, -num_patches:]  # æå– Patch å¯¹åº”çš„è¾“å‡º
    â†“
[9] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â”ƒ  FlattenHead (è¾“å‡ºæŠ•å½±)              â”ƒ
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â”œâ”€ Flatten: [B, N, d_ff, num_patches] â†’ [B, N, d_ff * num_patches]
    â””â”€ Linear: [B, N, d_ff * num_patches] â†’ [B, N, pred_len]
    â†“
é¢„æµ‹ç»“æœ (å½’ä¸€åŒ–ç©ºé—´): dec_out [Batch, pred_len, N_vars]
    â†“
[10] åå½’ä¸€åŒ– (Denormalize)
    dec_out = dec_out * stdev + mean
    â†“
æœ€ç»ˆé¢„æµ‹: dec_out [Batch, pred_len, N_vars]
```

---

### 2.2 å…³é”®æœºåˆ¶æ·±åº¦è§£æ

#### ğŸ”¹ æœºåˆ¶ 1: Patchingï¼ˆæ—¶åºåˆ†å—ï¼‰

**æ ¸å¿ƒæ€æƒ³ï¼š** å°†é•¿æ—¶é—´åºåˆ—åˆ‡åˆ†ä¸ºå›ºå®šé•¿åº¦çš„ Patchï¼Œç±»ä¼¼ Vision Transformer (ViT) çš„å›¾åƒåˆ†å—ã€‚

**ä»£ç ä½ç½®ï¼š** `layers/Embed.py` ç¬¬ 177-185 è¡Œ

**è¯¦ç»†æµç¨‹ï¼š**

1. **Paddingï¼ˆç¬¬ 180 è¡Œï¼‰**
   ```python
   x = self.padding_patch_layer(x)  # ReplicationPad1d
   # å¤åˆ¶æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„å€¼ï¼Œå¡«å…… stride é•¿åº¦
   # ç›®çš„ï¼šç¡®ä¿åºåˆ—é•¿åº¦èƒ½è¢« stride æ•´é™¤
   ```

2. **Unfoldï¼ˆç¬¬ 181 è¡Œï¼‰**
   ```python
   x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
   # æ»‘åŠ¨çª—å£åˆ‡åˆ†
   # è¾“å…¥: [B, N_vars, SeqLen + stride]
   # è¾“å‡º: [B, N_vars, num_patches, patch_len]
   # num_patches = (SeqLen - patch_len) / stride + 2
   ```

   **ç¤ºä¾‹ï¼š**
   ```
   SeqLen = 96, patch_len = 16, stride = 8
   num_patches = (96 - 16) / 8 + 2 = 12

   åŸå§‹åºåˆ—: [x0, x1, x2, ..., x95]
   Patch 0: [x0, x1, ..., x15]
   Patch 1: [x8, x9, ..., x23]  (stride=8, é‡å  50%)
   Patch 2: [x16, x17, ..., x31]
   ...
   Patch 11: [x88, x89, ..., x103] (padding éƒ¨åˆ†)
   ```

3. **Reshapeï¼ˆç¬¬ 182 è¡Œï¼‰**
   ```python
   x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))
   # å±•å¹³ Batch å’Œå˜é‡ç»´åº¦
   # [B, N_vars, num_patches, patch_len] â†’ [B*N_vars, num_patches, patch_len]
   # ç›®çš„ï¼šå°†æ¯ä¸ªå˜é‡çš„æ¯ä¸ª Patch å½“ä½œç‹¬ç«‹æ ·æœ¬å¤„ç†
   ```

4. **TokenEmbeddingï¼ˆç¬¬ 184 è¡Œï¼‰**
   ```python
   x = self.value_embedding(x)  # 1D Conv: in_channels=patch_len, out_channels=d_model
   # [B*N_vars, num_patches, patch_len] â†’ [B*N_vars, num_patches, d_model]
   ```

**ä¸ºä»€ä¹ˆä½¿ç”¨ Patchingï¼Ÿ**
- **é™ä½è®¡ç®—å¤æ‚åº¦ï¼š** Transformer çš„å¤æ‚åº¦æ˜¯ O(LÂ²)ï¼ŒPatching å°†åºåˆ—é•¿åº¦ä» L é™ä¸º num_patches
- **å±€éƒ¨æ¨¡å¼æ•æ‰ï¼š** æ¯ä¸ª Patch ä¿ç•™å±€éƒ¨æ—¶åºä¾èµ–
- **ä¸ LLM Token å¯¹é½ï¼š** Patch ç±»æ¯”æ–‡æœ¬ä¸­çš„ Tokenï¼Œæ›´ç¬¦åˆ LLM çš„è¾“å…¥å½¢å¼

---

#### ğŸ”¹ æœºåˆ¶ 2: Reprogrammingï¼ˆé‡ç¼–ç¨‹/è·¨æ¨¡æ€å¯¹é½ï¼‰

**æ ¸å¿ƒæ€æƒ³ï¼š** é€šè¿‡ **Cross-Attention** æœºåˆ¶ï¼Œå°†æ—¶åº Patch Embeddingsï¼ˆæ¥è‡ªæ—¶åºåŸŸï¼‰æ˜ å°„åˆ° LLM çš„è¯åµŒå…¥ç©ºé—´ï¼ˆæ–‡æœ¬åŸŸï¼‰ï¼Œå®ç°è·¨æ¨¡æ€å¯¹é½ã€‚

**ä»£ç ä½ç½®ï¼š** `models/TimeLLM.py` ç¬¬ 267-305 è¡Œ `ReprogrammingLayer`

**è¯¦ç»†æµç¨‹ï¼š**

1. **Source Embeddings æ„å»ºï¼ˆç¬¬ 237 è¡Œï¼‰**
   ```python
   # è·å– LLM çš„è¯åµŒå…¥çŸ©é˜µ
   self.word_embeddings = self.llm_model.get_input_embeddings().weight
   # Shape: [vocab_size, llm_dim]ï¼Œä¾‹å¦‚ GPT2: [50257, 768]

   # Mapping Layer: å°†è¯è¡¨ç©ºé—´å‹ç¼©ä¸ºå¯å­¦ä¹ çš„ Token ç©ºé—´
   source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)
   # [vocab_size, llm_dim] â†’ [llm_dim, vocab_size] â†’ Linear â†’ [llm_dim, num_tokens] â†’ [num_tokens, llm_dim]
   # num_tokens = 1000 (å¯å­¦ä¹ çš„è™šæ‹Ÿè¯è¡¨å¤§å°)
   ```

2. **Cross-Attention å¯¹é½ï¼ˆç¬¬ 280-305 è¡Œï¼‰**
   ```python
   # Query: æ¥è‡ª Patch Embeddingsï¼ˆæ—¶åºåŸŸï¼‰
   Q = self.query_projection(target_embedding)  # [B*N, num_patches, d_keys * n_heads]

   # Key & Value: æ¥è‡ª Source Embeddingsï¼ˆLLM è¯åµŒå…¥ç©ºé—´ï¼‰
   K = self.key_projection(source_embedding)    # [num_tokens, d_keys * n_heads]
   V = self.value_projection(value_embedding)   # [num_tokens, d_keys * n_heads]

   # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
   scores = torch.einsum("blhe,she->bhls", Q, K)  # [B*N, n_heads, num_patches, num_tokens]
   # l: num_patches, s: num_tokens, h: n_heads, e: d_keys

   # Softmax å½’ä¸€åŒ–
   A = softmax(scores / sqrt(d_keys), dim=-1)  # [B*N, n_heads, num_patches, num_tokens]

   # åŠ æƒèšåˆ Value
   out = torch.einsum("bhls,she->blhe", A, V)  # [B*N, num_patches, n_heads, d_keys]

   # è¾“å‡ºæŠ•å½±åˆ° llm_dim
   out = self.out_projection(out.reshape(B, L, -1))  # [B*N, num_patches, llm_dim]
   ```

**ç‰©ç†æ„ä¹‰ï¼š**
- **Queryï¼ˆæ—¶åºåŸŸï¼‰ï¼š** "æˆ‘æ˜¯ä¸€ä¸ªæ—¶åº Patchï¼Œæˆ‘æƒ³æ‰¾åˆ°æ–‡æœ¬è¯è¡¨ä¸­ä¸æˆ‘è¯­ä¹‰æœ€æ¥è¿‘çš„è¯"
- **Keyï¼ˆæ–‡æœ¬åŸŸï¼‰ï¼š** "æˆ‘æ˜¯ LLM è¯è¡¨ä¸­çš„ä¸€ä¸ªè¯ï¼Œæˆ‘ä»£è¡¨æŸç§è¯­ä¹‰"
- **Attention æƒé‡ï¼š** åº¦é‡æ—¶åº Patch ä¸æ–‡æœ¬è¯ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦
- **Outputï¼š** æ—¶åº Patch åœ¨ LLM åµŒå…¥ç©ºé—´ä¸­çš„è¡¨ç¤ºï¼ˆèåˆäº†æ–‡æœ¬è¯­ä¹‰ï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦ Reprogrammingï¼Ÿ**
- **æ¨¡æ€é¸¿æ²Ÿï¼š** æ—¶åºæ•°æ®ï¼ˆè¿ç»­æ•°å€¼ï¼‰ä¸æ–‡æœ¬æ•°æ®ï¼ˆç¦»æ•£ Tokenï¼‰åˆ†å¸ƒå·®å¼‚å·¨å¤§
- **å†»ç»“ LLMï¼š** LLM å‚æ•°å†»ç»“ï¼Œæ— æ³•ç›´æ¥é€‚é…æ—¶åºæ•°æ®ï¼Œéœ€è¦é€šè¿‡ Reprogramming å±‚"ç¿»è¯‘"
- **çŸ¥è¯†è¿ç§»ï¼š** åˆ©ç”¨ LLM çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼ˆè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼‰å¸®åŠ©æ—¶åºå»ºæ¨¡

---

### 2.3 æ•°æ®å½¢çŠ¶å˜åŒ–å…¨æµç¨‹

ä»¥ **ETTh1 æ•°æ®é›† + GPT-2 æ¨¡å‹** ä¸ºä¾‹ï¼š

| é˜¶æ®µ | å¼ é‡åç§° | Shape | è¯´æ˜ |
|------|---------|-------|------|
| **è¾“å…¥** | x_enc | `[32, 96, 7]` | Batch=32, SeqLen=96, N_vars=7 |
| **å½’ä¸€åŒ–å** | x_enc | `[32, 96, 7]` | å®ä¾‹å½’ä¸€åŒ–ï¼ˆå‡å‡å€¼é™¤æ–¹å·®ï¼‰ |
| **Prompt åµŒå…¥** | prompt_embeddings | `[32, 128, 768]` | Prompt é•¿åº¦çº¦ 128 Tokenï¼ŒGPT2 éšè—å±‚ 768 |
| **Patching å** | enc_out | `[224, 12, 16]` | B*N=32*7=224, num_patches=12, d_model=16 |
| **Reprogramming å** | enc_out | `[224, 12, 768]` | æ˜ å°„åˆ° llm_dim=768 |
| **æ‹¼æ¥è¾“å…¥** | llama_enc_out | `[224, 140, 768]` | Concat Prompt(128) + Patches(12) = 140 |
| **LLM è¾“å‡º** | dec_out | `[224, 140, 768]` | GPT2 å‰å‘ä¼ æ’­ |
| **æˆªå– + Reshape** | dec_out | `[32, 7, 32, 12]` | å–å‰ d_ff=32 ç»´ï¼ŒReshape å› [B, N, d_ff, num_patches] |
| **FlattenHead è¾“å…¥** | dec_out | `[32, 7, 384]` | Flatten åä¸¤ç»´ï¼š32*12=384 |
| **FlattenHead è¾“å‡º** | dec_out | `[32, 7, 96]` | Linear(384 â†’ pred_len=96) |
| **åå½’ä¸€åŒ–å** | dec_out | `[32, 96, 7]` | æœ€ç»ˆé¢„æµ‹ï¼šBatch=32, PredLen=96, N_vars=7 |

**å…³é”®å‚æ•°è®¡ç®—ï¼š**
```python
# Patch æ•°é‡
num_patches = (seq_len - patch_len) / stride + 2
            = (96 - 16) / 8 + 2 = 12

# FlattenHead è¾“å…¥ç»´åº¦
head_nf = d_ff * num_patches = 32 * 12 = 384

# Prompt Token é•¿åº¦ï¼ˆåŠ¨æ€ï¼Œå–å†³äºç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²é•¿åº¦ï¼‰
prompt_len â‰ˆ 128 (tokenizer è‡ªåŠ¨ padding/truncation)
```

---

## ä¸‰ã€æ¨¡å‹æ¶æ„è§£æ (Model Architecture)

### 3.1 å†»ç»“ (Frozen) vs å¯è®­ç»ƒ (Trainable) å‚æ•°

#### â„ï¸ å†»ç»“éƒ¨åˆ†ï¼ˆå‚æ•°ä¸æ›´æ–°ï¼‰

| ç»„ä»¶ | ä»£ç ä½ç½® | å‚æ•°é‡ï¼ˆä»¥ GPT-2 ä¸ºä¾‹ï¼‰ | è¯´æ˜ |
|------|---------|----------------------|------|
| **LLM Backbone** | `models/TimeLLM.py` ç¬¬ 163-164 è¡Œ | **117M** | GPT-2 å…¨éƒ¨å‚æ•°å†»ç»“ |

```python
# å†»ç»“ LLM å‚æ•°
for param in self.llm_model.parameters():
    param.requires_grad = False
```

**ä¸ºä»€ä¹ˆå†»ç»“ LLMï¼Ÿ**
1. **å‚æ•°æ•ˆç‡ï¼š** LLM å‚æ•°é‡å·¨å¤§ï¼ˆGPT-2: 117Mï¼ŒLLAMA-7B: 7Bï¼‰ï¼Œå…¨é‡å¾®è°ƒéœ€è¦å¤§é‡æ˜¾å­˜
2. **çŸ¥è¯†ä¿ç•™ï¼š** å†»ç»“å‚æ•°ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ï¼Œé¿å…åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆ
3. **è®¡ç®—æ•ˆç‡ï¼š** åå‘ä¼ æ’­æ—¶ä¸è®¡ç®— LLM æ¢¯åº¦ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å’Œè®¡ç®—å¼€é”€

---

#### ğŸ”¥ å¯è®­ç»ƒéƒ¨åˆ†ï¼ˆå‚æ•°æ›´æ–°ï¼‰

| ç»„ä»¶ | ä»£ç ä½ç½® | å½¢çŠ¶ | å‚æ•°é‡ï¼ˆç¤ºä¾‹ï¼‰ | è¯´æ˜ |
|------|---------|------|--------------|------|
| **PatchEmbedding** | `layers/Embed.py` ç¬¬ 169 è¡Œ | `Conv1d(16, 16, 3)` | ~800 | å°† Patch åµŒå…¥åˆ° d_model |
| **Mapping Layer** | `models/TimeLLM.py` ç¬¬ 179 è¡Œ | `Linear(50257, 1000)` | **50.3M** | è¯è¡¨æ˜ å°„ï¼ˆGPT-2 è¯è¡¨ 50257ï¼‰ |
| **Reprogramming Layer** | `models/TimeLLM.py` ç¬¬ 181 è¡Œ | 4 ä¸ª Linear å±‚ | ~6M | Cross-Attention æƒé‡ |
| **Output Projection (FlattenHead)** | `models/TimeLLM.py` ç¬¬ 187 è¡Œ | `Linear(384, 96)` | ~37K | è¾“å‡ºæŠ•å½±åˆ°é¢„æµ‹é•¿åº¦ |
| **Normalize Layer** | `models/TimeLLM.py` ç¬¬ 192 è¡Œ | æ— å‚æ•°ï¼ˆä»…ç»Ÿè®¡é‡ï¼‰ | 0 | å®ä¾‹å½’ä¸€åŒ–ï¼ˆéå¯å­¦ä¹ ï¼‰ |

**æ€»å¯è®­ç»ƒå‚æ•°é‡ï¼š** çº¦ **56-60M**ï¼ˆå–å†³äº d_model, d_ff, n_heads é…ç½®ï¼‰

**è®­ç»ƒæ—¶å‚æ•°æ›´æ–°ï¼š**
```python
# run_main.py ç¬¬ 148-150 è¡Œ
trained_parameters = []
for p in model.parameters():
    if p.requires_grad is True:
        trained_parameters.append(p)

# ä¼˜åŒ–å™¨åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
optimizer = optim.Adam(trained_parameters, lr=args.learning_rate)
```

---

### 3.2 Checkpoint å†…å®¹è¯¦è§£

**ä¿å­˜ä½ç½®ï¼š** `checkpoints/{setting}-{model_comment}/checkpoint`

**ä¿å­˜ä»£ç ï¼š** `utils/tools.py` ç¬¬ 79-83 è¡Œ
```python
def save_checkpoint(self, val_loss, model, path):
    if self.accelerator is not None:
        model = self.accelerator.unwrap_model(model)  # è§£åŒ… DDP/DeepSpeed åŒ…è£…
        torch.save(model.state_dict(), path + '/' + 'checkpoint')
```

**Checkpoint åŒ…å«çš„å‚æ•°ï¼š**
```python
checkpoint = {
    # 1. PatchEmbedding
    'patch_embedding.value_embedding.tokenConv.weight': [16, 16, 3],

    # 2. Mapping Layer
    'mapping_layer.weight': [1000, 50257],  # â˜… æœ€å¤§å‚æ•°å—
    'mapping_layer.bias': [1000],

    # 3. Reprogramming Layer
    'reprogramming_layer.query_projection.weight': [d_keys * n_heads, d_model],
    'reprogramming_layer.key_projection.weight': [d_keys * n_heads, llm_dim],
    'reprogramming_layer.value_projection.weight': [d_keys * n_heads, llm_dim],
    'reprogramming_layer.out_projection.weight': [llm_dim, d_keys * n_heads],

    # 4. FlattenHead (Output Projection)
    'output_projection.linear.weight': [pred_len, d_ff * num_patches],
    'output_projection.linear.bias': [pred_len],

    # æ³¨æ„ï¼šLLM å‚æ•°ä¸åœ¨ Checkpoint ä¸­ï¼ˆå› ä¸ºè¢«å†»ç»“ï¼‰
}
```

**Checkpoint æ–‡ä»¶å¤§å°ï¼š**
- **GPT-2 + d_model=16 + d_ff=32ï¼š** çº¦ **200-250 MB**
- **LLAMA-7B + d_model=32 + d_ff=128ï¼š** çº¦ **300-400 MB**

**åŠ è½½ Checkpoint æ¨ç†ï¼š**
```python
# 1. åˆå§‹åŒ–æ¨¡å‹
model = TimeLLM.Model(args).float()

# 2. åŠ è½½æƒé‡
checkpoint_path = './checkpoints/long_term_forecast_ETTh1_96_96_.../checkpoint'
model.load_state_dict(torch.load(checkpoint_path))

# 3. æ¨ç†æ¨¡å¼
model.eval()
with torch.no_grad():
    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
```

---

## å››ã€äº§å‡ºä¸è¯„ä¼° (Outputs & Inference)

### 4.1 è®­ç»ƒäº§å‡ºæ–‡ä»¶

#### ğŸ“ `checkpoints/` ç›®å½•ç»“æ„

```
checkpoints/
â””â”€â”€ long_term_forecast_ETTh1_96_96_TimeLLM_ETTh1_ftM_sl96_ll48_pl96_dm16_nh8_el2_dl1_df32_fc3_ebtimeF_Exp_0-TimeLLM-GPT2-LowMem/
    â”œâ”€â”€ checkpoint              # â˜… æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆæœ€ä½³éªŒè¯é›†æ¨¡å‹ï¼‰
    â””â”€â”€ [å·²åˆ é™¤] checkpoint.tmp # è®­ç»ƒç»“æŸåè‡ªåŠ¨åˆ é™¤
```

**å‘½åè§„åˆ™ï¼š**
```
{task_name}_{model_id}_{model}_{data}_ft{features}_sl{seq_len}_ll{label_len}_pl{pred_len}_
dm{d_model}_nh{n_heads}_el{e_layers}_dl{d_layers}_df{d_ff}_fc{factor}_eb{embed}_{des}_{itr}-{model_comment}
```

**ç¤ºä¾‹ï¼š**
```
long_term_forecast_ETTh1_96_96_TimeLLM_ETTh1_ftM_sl96_ll48_pl96_dm16_nh8_el2_dl1_df32_fc3_ebtimeF_Exp_0-TimeLLM-GPT2-LowMem
```

---

### 4.2 æ¨ç†æµç¨‹

#### ğŸ”¹ å•æ‰¹æ¬¡æ¨ç†ç¤ºä¾‹

```python
import torch
from models import TimeLLM
import argparse

# 1. é…ç½®å‚æ•°ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
args = argparse.Namespace(
    llm_model='GPT2',
    llm_dim=768,
    llm_layers=6,
    d_model=16,
    d_ff=32,
    n_heads=8,
    dropout=0.1,
    seq_len=96,
    pred_len=96,
    patch_len=16,
    stride=8,
    enc_in=7,
    task_name='long_term_forecast',
    prompt_domain=1,
    content='The Electricity Transformer Temperature (ETT) is a crucial indicator...'
)

# 2. åˆå§‹åŒ–æ¨¡å‹
model = TimeLLM.Model(args).float()

# 3. åŠ è½½ Checkpoint
model.load_state_dict(torch.load('checkpoints/.../checkpoint'))
model.eval()

# 4. å‡†å¤‡è¾“å…¥æ•°æ®
batch_x = torch.randn(1, 96, 7)        # [Batch, SeqLen, N_vars]
batch_x_mark = torch.randn(1, 96, 4)   # [Batch, SeqLen, TimeFeatures]
dec_inp = torch.zeros(1, 96, 7)        # [Batch, PredLen, N_vars] (å ä½ç¬¦)
batch_y_mark = torch.randn(1, 96, 4)   # [Batch, PredLen, TimeFeatures]

# 5. æ¨ç†
with torch.no_grad():
    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
    # outputs.shape: [1, 96, 7]  (Batch=1, PredLen=96, N_vars=7)

print(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {outputs.shape}")
print(f"é¢„æµ‹å€¼èŒƒå›´: [{outputs.min():.4f}, {outputs.max():.4f}]")
```

---

#### ğŸ”¹ æ‰¹é‡æ¨ç†ï¼ˆæµ‹è¯•é›†è¯„ä¼°ï¼‰

```python
from tqdm import tqdm
from utils.metrics import metric

# 1. åŠ è½½æµ‹è¯•é›†
test_data, test_loader = data_provider(args, 'test')

# 2. æ¨ç†å¾ªç¯
preds = []
trues = []

model.eval()
with torch.no_grad():
    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(tqdm(test_loader)):
        # è¾“å…¥æ•°æ®
        batch_x = batch_x.float().to(device)
        batch_y = batch_y.float().to(device)
        batch_x_mark = batch_x_mark.float().to(device)
        batch_y_mark = batch_y_mark.float().to(device)

        # Decoder è¾“å…¥ï¼ˆå‰ label_len å–çœŸå€¼ï¼Œå pred_len å¡«é›¶ï¼‰
        dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()
        dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)

        # æ¨ç†
        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)

        # æå–é¢„æµ‹çª—å£
        outputs = outputs[:, -args.pred_len:, :]
        batch_y = batch_y[:, -args.pred_len:, :]

        # ç´¯ç§¯ç»“æœ
        preds.append(outputs.cpu().numpy())
        trues.append(batch_y.cpu().numpy())

# 3. æ‹¼æ¥å¹¶è®¡ç®—æŒ‡æ ‡
preds = np.concatenate(preds, axis=0)  # [N_samples, pred_len, N_vars]
trues = np.concatenate(trues, axis=0)

mae, mse, rmse, mape, mspe = metric(preds, trues)
print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')
```

---

### 4.3 è¯„ä¼°æŒ‡æ ‡è¯¦è§£

#### ğŸ“Š æ”¯æŒçš„è¯„ä¼°æŒ‡æ ‡ï¼ˆ`utils/metrics.py`ï¼‰

| æŒ‡æ ‡ | å…¬å¼ | ç‰©ç†æ„ä¹‰ | ä»£ç ä½ç½® |
|------|------|---------|---------|
| **MAE** | $\frac{1}{N}\sum \|y_{pred} - y_{true}\|$ | å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆå•ä½ä¸åŸæ•°æ®ä¸€è‡´ï¼‰ | ç¬¬ 14-15 è¡Œ |
| **MSE** | $\frac{1}{N}\sum (y_{pred} - y_{true})^2$ | å‡æ–¹è¯¯å·®ï¼ˆå¯¹å¤§è¯¯å·®æ•æ„Ÿï¼‰ | ç¬¬ 18-19 è¡Œ |
| **RMSE** | $\sqrt{MSE}$ | å‡æ–¹æ ¹è¯¯å·®ï¼ˆå•ä½ä¸åŸæ•°æ®ä¸€è‡´ï¼‰ | ç¬¬ 22-23 è¡Œ |
| **MAPE** | $\frac{1}{N}\sum \|\frac{y_{pred} - y_{true}}{y_{true}}\|$ | å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼ˆ0-1 èŒƒå›´ï¼‰ | ç¬¬ 26-27 è¡Œ |
| **MSPE** | $\frac{1}{N}\sum (\frac{y_{pred} - y_{true}}{y_{true}})^2$ | å‡æ–¹ç™¾åˆ†æ¯”è¯¯å·® | ç¬¬ 30-31 è¡Œ |

#### ğŸ“Œ æŒ‡æ ‡é€‰æ‹©å»ºè®®

- **é•¿æœŸé¢„æµ‹ï¼ˆETT/Weather/Trafficï¼‰ï¼š** ä¸»è¦å…³æ³¨ **MSE** å’Œ **MAE**
- **M4 çŸ­æœŸé¢„æµ‹ï¼š** ä½¿ç”¨ **SMAPE** å’Œ **MASE**ï¼ˆM4 ç«èµ›æ ‡å‡†ï¼‰
- **å¯¹æ¯”ä¸åŒæ¨¡å‹ï¼š** åŒæ—¶æŠ¥å‘Š MAEã€MSEã€RMSE ä¸‰é¡¹

#### ğŸ” è®­ç»ƒæ—¥å¿—ç¤ºä¾‹

```
Epoch: 1 cost time: 123.45
Epoch: 1 | Train Loss: 0.4523 Vali Loss: 0.3912 Test Loss: 0.4012
          MAE: 0.4567, MSE: 0.3912, RMSE: 0.6254, MAPE: 0.1234, MSPE: 0.0234

Epoch: 2 cost time: 119.32
Epoch: 2 | Train Loss: 0.3821 Vali Loss: 0.3654 Test Loss: 0.3701
          MAE: 0.4321, MSE: 0.3654, RMSE: 0.6045, MAPE: 0.1198, MSPE: 0.0221

...

EarlyStopping counter: 1 out of 10
Validation loss decreased (0.365400 --> 0.351200). Saving model ...
```

---

## äº”ã€æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹æ€»ç»“

### 5.1 Time-LLM çš„ä¸‰å¤§åˆ›æ–°

#### ğŸ”¹ 1. Prompt-as-Prefixï¼ˆæç¤ºä½œä¸ºå‰ç¼€ï¼‰

- **åŠ¨æ€ç»Ÿè®¡æç¤ºï¼š** æå– min/max/median/trend/lags ä½œä¸ºä¸Šä¸‹æ–‡
- **é¢†åŸŸæè¿°ï¼š** ä» `prompt_bank/` åŠ è½½æ•°æ®é›†æè¿°æ–‡æœ¬
- **è”åˆåµŒå…¥ï¼š** Prompt Embeddings + Patch Embeddings æ‹¼æ¥åè¾“å…¥ LLM

#### ğŸ”¹ 2. Reprogramming Layerï¼ˆé‡ç¼–ç¨‹å±‚ï¼‰

- **è·¨æ¨¡æ€å¯¹é½ï¼š** é€šè¿‡ Cross-Attention å°†æ—¶åºåµŒå…¥æ˜ å°„åˆ° LLM ç©ºé—´
- **å‚æ•°é«˜æ•ˆï¼š** ä»…è®­ç»ƒå¯¹é½å±‚ï¼ŒLLM å®Œå…¨å†»ç»“
- **çŸ¥è¯†è¿ç§»ï¼š** åˆ©ç”¨ LLM çš„è¯­ä¹‰ç†è§£èƒ½åŠ›å¢å¼ºæ—¶åºå»ºæ¨¡

#### ğŸ”¹ 3. Patching Strategyï¼ˆåˆ†å—ç­–ç•¥ï¼‰

- **å±€éƒ¨æ€§ï¼š** ä¿ç•™æ—¶åºå±€éƒ¨ä¾èµ–
- **æ•ˆç‡ï¼š** é™ä½åºåˆ—é•¿åº¦ï¼Œå‡å°‘è®¡ç®—å¤æ‚åº¦
- **å¯¹é½ï¼š** Patch ç±»æ¯”æ–‡æœ¬ Tokenï¼Œç¬¦åˆ LLM è¾“å…¥èŒƒå¼

---

### 5.2 ä¸ä¼ ç»Ÿæ—¶åºæ¨¡å‹å¯¹æ¯”

| ç»´åº¦ | ä¼ ç»Ÿæ¨¡å‹ï¼ˆAutoformer/DLinearï¼‰ | Time-LLM |
|------|----------------------------|----------|
| **å‚æ•°é‡** | å…¨é‡è®­ç»ƒï¼ˆ100K-10Mï¼‰ | ä»…è®­ç»ƒå¯¹é½å±‚ï¼ˆ50-60Mï¼‰ï¼ŒLLM å†»ç»“ï¼ˆ117M-7Bï¼‰ |
| **æ•°æ®éœ€æ±‚** | éœ€è¦å¤§é‡æ—¶åºæ•°æ® | å¯åˆ©ç”¨ LLM é¢„è®­ç»ƒçŸ¥è¯†ï¼Œå°æ•°æ®é›†ä¹Ÿæœ‰æ•ˆ |
| **æ³›åŒ–èƒ½åŠ›** | ä¾èµ–æ•°æ®åˆ†å¸ƒ | è·¨æ•°æ®é›†è¿ç§»èƒ½åŠ›æ›´å¼º |
| **è®¡ç®—å¼€é”€** | è®­ç»ƒå¿« | æ¨ç†æ—¶éœ€åŠ è½½ LLMï¼ˆæ˜¾å­˜å ç”¨å¤§ï¼‰ |
| **å¯è§£é‡Šæ€§** | åŸºäºæ³¨æ„åŠ›æƒé‡ | Prompt æä¾›ç»Ÿè®¡è§£é‡Š + Attention å¯è§†åŒ– |

---

### 5.3 ä½æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥æ€»ç»“

| ä¼˜åŒ–é¡¹ | åŸå§‹é…ç½® | 6GB æ˜¾å­˜é…ç½® | ä¼˜åŒ–æ•ˆæœ |
|--------|---------|-------------|---------|
| LLM æ¨¡å‹ | LLAMA-7B (14GB) | GPT-2 (500MB) | â˜…â˜…â˜… æ˜¾å­˜é™ä½ 96% |
| Batch Size | 24 | 2-4 | â˜…â˜…â˜… æ˜¾å­˜é™ä½ 83% |
| Seq Len | 512 | 96 | â˜…â˜… æ˜¾å­˜é™ä½ 81% |
| LLM Layers | 32 | 6 | â˜… æ˜¾å­˜é™ä½ 81% |
| Mixed Precision | bf16 | fp16 | â˜… æ˜¾å­˜é™ä½ 50% |
| Num Workers | 10 | 2 | â˜… CPU å†…å­˜ä¼˜åŒ– |

---

## å…­ã€å¸¸è§é—®é¢˜ (FAQ)

### Q1: ä¸ºä»€ä¹ˆ LLM å‚æ•°å†»ç»“åè¿˜èƒ½æå‡æ—¶åºé¢„æµ‹æ€§èƒ½ï¼Ÿ

**A:** LLM é€šè¿‡é¢„è®­ç»ƒå­¦ä¹ äº†ä¸°å¯Œçš„æ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼ˆä¾‹å¦‚åºåˆ—ä¾èµ–ã€é•¿ç¨‹å…³è”ï¼‰ã€‚è™½ç„¶å‚æ•°å†»ç»“ï¼Œä½†é€šè¿‡ Reprogramming Layer å°†æ—¶åºæ•°æ®"ç¿»è¯‘"æˆ LLM èƒ½ç†è§£çš„å½¢å¼åï¼ŒLLM çš„è¡¨ç¤ºèƒ½åŠ›ä¾ç„¶å¯ä»¥è¢«åˆ©ç”¨ã€‚ç±»ä¼¼äº Prompt Tuningï¼Œåªè°ƒæ•´è¾“å…¥è€Œéæ¨¡å‹æƒé‡ã€‚

---

### Q2: Mapping Layer çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

**A:** LLM çš„è¯åµŒå…¥çŸ©é˜µï¼ˆå¦‚ GPT-2 çš„ 50257 ä¸ªè¯ï¼‰å¤ªå¤§ï¼Œç›´æ¥ä½œä¸º Reprogramming çš„ Source ä¼šå¯¼è‡´è®¡ç®—å¼€é”€è¿‡é«˜ã€‚Mapping Layer å°†è¯è¡¨ç©ºé—´å‹ç¼©ä¸º 1000 ä¸ªå¯å­¦ä¹ çš„"è™šæ‹Ÿè¯"ï¼Œä½œä¸º Reprogramming çš„ Key/Valueï¼Œæ—¢é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œåˆå¢å¼ºè¡¨è¾¾èƒ½åŠ›ã€‚

---

### Q3: å¦‚ä½•å¯è§†åŒ– Reprogramming Layer çš„å¯¹é½æ•ˆæœï¼Ÿ

**A:** å¯ä»¥æå– Attention æƒé‡çŸ©é˜µ `A` (shape: `[B*N, n_heads, num_patches, num_tokens]`)ï¼Œç»˜åˆ¶çƒ­åŠ›å›¾ï¼š
```python
# åœ¨ ReprogrammingLayer.reprogramming() ç¬¬ 302 è¡Œåæ·»åŠ 
self.attention_weights = A.detach().cpu()

# è®­ç»ƒåå¯è§†åŒ–
import matplotlib.pyplot as plt
plt.imshow(model.reprogramming_layer.attention_weights[0, 0, :, :], cmap='viridis')
plt.xlabel('Source Tokens (LLM Vocab)')
plt.ylabel('Time Series Patches')
plt.colorbar()
plt.show()
```

---

### Q4: è®­ç»ƒæ—¶æ˜¾å­˜ OOM æ€ä¹ˆåŠï¼Ÿ

**A:** æŒ‰ä»¥ä¸‹é¡ºåºè°ƒæ•´ï¼š
1. **é™ä½ Batch Size**ï¼ˆ2 â†’ 1ï¼‰
2. **å‡å°‘ LLM Layers**ï¼ˆ6 â†’ 4ï¼‰
3. **ç¼©çŸ­ Seq Len**ï¼ˆ96 â†’ 64ï¼‰
4. **é™ä½ d_ff**ï¼ˆ32 â†’ 16ï¼‰
5. **å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼ˆåœ¨ `TimeLLM.py` ä¸­æ·»åŠ  `self.llm_model.gradient_checkpointing_enable()`ï¼‰

---

### Q5: å¦‚ä½•è¿ç§»åˆ°æ–°æ•°æ®é›†ï¼Ÿ

**A:**
1. åœ¨ `dataset/prompt_bank/` åˆ›å»ºæ–°çš„æè¿°æ–‡æœ¬ï¼ˆå¦‚ `my_dataset.txt`ï¼‰
2. åœ¨ `data_provider/data_loader.py` æ·»åŠ æ–°çš„ Dataset ç±»
3. åœ¨ `data_provider/data_factory.py` æ³¨å†Œæ–°æ•°æ®é›†
4. åœ¨ `run_main.py` ä½¿ç”¨ `--data my_dataset --prompt_domain 1` å¯åŠ¨è®­ç»ƒ

---

## ä¸ƒã€Qwen 2.5 3B 4-bit é‡åŒ–æ”¯æŒ (2024-12-05 æ›´æ–°)

### 7.1 ä»£ç ä¿®æ”¹æ€»ç»“

#### ğŸ“ `run_main.py` (ç¬¬ 82-84 è¡Œ)
```python
# ========== æ–°å¢å‚æ•°ï¼šæ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„å’Œ4-bité‡åŒ– ==========
parser.add_argument('--llm_model_path', type=str, default='', help='LLM model path (local or HuggingFace ID)')
parser.add_argument('--load_in_4bit', action='store_true', help='Load model in 4-bit quantization to save VRAM')
# =========================================================
```

#### ğŸ“ `models/TimeLLM.py` (ç¬¬ 43-96 è¡Œ)
```python
if configs.llm_model_path:
    # é€šç”¨æ¨¡å‹åŠ è½½é€»è¾‘
    from transformers import AutoModel, AutoTokenizer, AutoConfig, BitsAndBytesConfig
    
    quantization_config = None
    if configs.load_in_4bit:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
    
    self.llm_model = AutoModel.from_pretrained(
        configs.llm_model_path,
        trust_remote_code=True,
        quantization_config=quantization_config,
        device_map="auto" if configs.load_in_4bit else None
    )
```

### 7.2 æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å¹´ä»½ | å‚æ•°é‡ | 16-bit æ˜¾å­˜ | 4-bit æ˜¾å­˜ | æ€§èƒ½ |
|------|------|--------|-------------|------------|------|
| GPT-2 | 2019 | 124M | ~500 MB | N/A | åŸºå‡† |
| **Qwen 2.5 3B** | 2024 | 3B | ~6 GB | **~1.5 GB** | **å¼ºå¾—å¤š** |
| Llama 3.1 8B | 2024 | 8B | ~16 GB | ~4.5 GB | æ›´å¼º |

### 7.3 4-bit é‡åŒ–åŸç†

**ä¸ºä»€ä¹ˆ 3B æ¨¡å‹èƒ½åœ¨ 6GB æ˜¾å­˜è¿è¡Œï¼Ÿ**

| ç²¾åº¦ | æ¯å‚æ•°å­—èŠ‚ | 3B æ¨¡å‹æ˜¾å­˜ |
|------|-----------|-------------|
| FP32 | 4 å­—èŠ‚ | 12 GB |
| FP16/BF16 | 2 å­—èŠ‚ | 6 GB |
| **INT4 (NF4)** | **0.5 å­—èŠ‚** | **1.5 GB** |

**NF4 é‡åŒ–é…ç½®ï¼š**
```python
BitsAndBytesConfig(
    load_in_4bit=True,              # å¯ç”¨ 4-bit é‡åŒ–
    bnb_4bit_compute_dtype=torch.float16,  # è®¡ç®—æ—¶ä½¿ç”¨ FP16
    bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–è¿›ä¸€æ­¥å‹ç¼©
    bnb_4bit_quant_type="nf4"        # ä½¿ç”¨ NF4 (Normal Float 4-bit)
)
```

### 7.4 è¿è¡Œå‘½ä»¤

```powershell
python run_main.py ^
  --task_name long_term_forecast ^
  --is_training 1 ^
  --root_path ./dataset/ETT-small/ ^
  --data_path ETTm1.csv ^
  --model_id ETTm1_512_96 ^
  --model_comment Qwen3B ^
  --model TimeLLM ^
  --data ETTm1 ^
  --features M ^
  --seq_len 512 ^
  --label_len 48 ^
  --pred_len 96 ^
  --batch_size 8 ^
  --d_model 32 ^
  --d_ff 32 ^
  --llm_dim 2048 ^
  --llm_model QWEN ^
  --llm_model_path "e:\timellm\Time-LLM\base_models\Qwen2.5-3B" ^
  --load_in_4bit
```

---

## å…«ã€å‚è€ƒæ–‡çŒ®ä¸èµ„æº

### ğŸ“š è®ºæ–‡åŸæ–‡
- **Time-LLM: Time Series Forecasting by Reprogramming Large Language Models**
  ICLR 2024 | [arXiv:2310.01728](https://arxiv.org/abs/2310.01728)

### ğŸ”— ç›¸å…³é“¾æ¥
- **GitHub ä»“åº“ï¼š** [https://github.com/KimMeen/Time-LLM](https://github.com/KimMeen/Time-LLM)
- **æ•°æ®é›†ä¸‹è½½ï¼š** [Google Drive](https://drive.google.com/drive/folders/1ZOYpTUa82_jCcxIdTmyr0LXQfvaM9vIy)
- **HuggingFace æ¨¡å‹ï¼š**
  - GPT-2: `openai-community/gpt2`
  - **Qwen 2.5 3B:** `Qwen/Qwen2.5-3B-Instruct` â˜… æ¨è
  - LLAMA-7B: `huggyllama/llama-7b`
  - BERT: `google-bert/bert-base-uncased`

### ğŸ› ï¸ æ¨èå·¥å…·
- **æ˜¾å­˜ç›‘æ§ï¼š** `nvidia-smi -l 1` (Windows) / `watch -n 1 nvidia-smi` (Linux)
- **å¯è§†åŒ–ï¼š** `tensorboard --logdir=./logs`
- **è°ƒè¯•ï¼š** `python -m pdb run_main.py ...`

---

**æ–‡æ¡£ç”Ÿæˆæ—¶é—´ï¼š** 2024-12-05
**æœ€åæ›´æ–°ï¼š** æ–°å¢ Qwen 2.5 3B 4-bit é‡åŒ–æ”¯æŒ
**é€‚ç”¨ç‰ˆæœ¬ï¼š** Time-LLM v1.0 (åŸºäº ICLR'24 è®ºæ–‡å®ç°)
**ä½œè€…ï¼š** Claude Code Technical Analysis
