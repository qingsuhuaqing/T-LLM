# æ–¹æ¡ˆä¸€æ·±åº¦è§£æï¼šå¯è§£é‡Šæ€§å¢å¼ºä¸ä¼ ç»Ÿæ¨¡å‹é›†æˆ
â— æ–‡æ¡£ chuangxin-jieshi1.md å·²æˆåŠŸåˆ›å»ºã€‚ä»¥ä¸‹æ˜¯å…³äºæ–¹æ¡ˆä¸€ï¼šå¯è§£é‡Šæ€§å¢å¼ºä¸ä¼ ç»Ÿæ¨¡å‹é›†æˆçš„æ ¸å¿ƒå†…å®¹æ€»ç»“ï¼š

  ---
  ğŸ“‹ æ–‡æ¡£æ‘˜è¦

  ä¸€ã€é—®é¢˜ 1 å›ç­”ï¼šè¾“å…¥æ•°æ®åˆ¤æ–­æœºåˆ¶

  æ˜¯çš„ï¼Œå½“å‰æ–¹æ¡ˆæ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹æ€§é€‰æ‹©/åŠ æƒä¼ ç»Ÿæ¨¡å‹ã€‚ åˆ¤æ–­ä¾æ®åŒ…æ‹¬ï¼š

  | æ£€æµ‹æ–¹æ³•      | åˆ¤æ–­æ ‡å‡†                | å¯¹åº”æ¨¡å‹ç­–ç•¥          |
  |-----------|---------------------|-----------------|
  | ADF å¹³ç¨³æ€§æ£€éªŒ | p-value < 0.05 â†’ å¹³ç¨³ | ARIMA æƒé‡é«˜       |
  | FFT èƒ½é‡é›†ä¸­åº¦ | > 0.7 â†’ å¼ºå‘¨æœŸæ€§        | ä¼ ç»Ÿæ¨¡å‹æƒé‡é«˜         |
  | çº¿æ€§å›å½’æ–œç‡    | è¶‹åŠ¿å¼ºåº¦ > 0.5          | Holt-Winters é€‚ç”¨ |
  | å™ªå£°æ°´å¹³      | > 0.6 â†’ é«˜å™ªå£°         | ç§»åŠ¨å¹³å‡é¢„å¤„ç†         |

  ---
  äºŒã€é—®é¢˜ 2 å›ç­”ï¼šæ·±åº¦èåˆæ–¹æ¡ˆ

  æå‡ºäº†ä¸‰ç§å¯è¡Œæ–¹æ¡ˆï¼š

  æ–¹æ¡ˆ Aï¼šæ®‹å·®å­¦ä¹ æ¶æ„ï¼ˆæ¨è â­â­â­â­â­ï¼‰

  åŸå§‹åºåˆ— â†’ [ä¼ ç»Ÿæ¨¡å‹] â†’ çº¿æ€§é¢„æµ‹ + æ®‹å·®
                                â†“
                         [Time-LLM å­¦ä¹ æ®‹å·®]
                                â†“
                æœ€ç»ˆé¢„æµ‹ = çº¿æ€§é¢„æµ‹ + éçº¿æ€§é¢„æµ‹
  - ç†è®ºæ”¯æ’‘ï¼šHybrid ARIMA-LSTM è®ºæ–‡è¯æ˜æ®‹å·®ä¿®æ­£æ¡†æ¶æœ‰æ•ˆ
  - é¢„æœŸæ”¶ç›Šï¼šMSE é™ä½ 10-15%

  æ–¹æ¡ˆ Bï¼šåˆ†æ®µè‡ªé€‚åº”èåˆï¼ˆæ¨è â­â­â­â­ï¼‰

  - å°†è¾“å…¥åºåˆ—åˆ†æ®µåˆ†æï¼ˆå‘¨æœŸæ€§å¼º/è¶‹åŠ¿å¼º/å¤æ‚ï¼‰
  - æ¯æ®µåŠ¨æ€åˆ†é…ä¼ ç»Ÿæ¨¡å‹ä¸ Time-LLM çš„æƒé‡
  - ç†è®ºæ”¯æ’‘ï¼šAMD Framework (Mixture-of-Experts)

  æ–¹æ¡ˆ Cï¼šçŸ¥è¯†è’¸é¦ï¼ˆæ¨è â­â­â­ï¼‰

  - è½¯æ ‡ç­¾è’¸é¦ï¼šTime-LLM å­¦ä¹ ä¼ ç»Ÿæ¨¡å‹çš„é¢„æµ‹åˆ†å¸ƒ
  - åˆ†è§£è’¸é¦ï¼šå­¦ä¹ ä¼ ç»Ÿæ¨¡å‹çš„è¶‹åŠ¿/å­£èŠ‚æ€§åˆ†è§£èƒ½åŠ›
  - è¡Œä¸ºæ¨¡ä»¿ï¼šå­¦ä¹ é¢„æµ‹æ–¹å‘ä¸€è‡´æ€§ã€å¹…åº¦ä¸€è‡´æ€§
  - ç†è®ºæ”¯æ’‘ï¼šDE-TSMCL åœ¨ ETTm1 ä¸Š MSE æå‡ 24.2%

  ---
  ä¸‰ã€æ¨èå®æ–½æ–¹æ¡ˆï¼šA+B ç»„åˆ

  åŸå§‹åºåˆ— â†’ [ä¼ ç»Ÿæ¨¡å‹] â†’ çº¿æ€§é¢„æµ‹ + æ®‹å·®
                                â†“
                          [åˆ†æ®µåˆ†æå™¨]
                      â†“         â†“         â†“
                    Seg1      Seg2      Seg3
                  (å‘¨æœŸå¼º)   (å¤æ‚)   (è¶‹åŠ¿å¼º)
                    â†“         â†“         â†“
                  æƒé‡0.3   æƒé‡0.9   æƒé‡0.4
                                â†“
                          [Time-LLM]
                                â†“
         æœ€ç»ˆé¢„æµ‹ = çº¿æ€§é¢„æµ‹ + Î£(w_i Ã— éçº¿æ€§é¢„æµ‹_i)

  é¢„æœŸæ€»æ”¶ç›Šï¼šMSE é™ä½ 15-20%ï¼Œå¯è§£é‡Šæ€§å¤§å¹…æå‡

  ---
  å››ã€å‰æ²¿è®ºæ–‡æ”¯æ’‘

  | è®ºæ–‡             | è´¡çŒ®                         | å¯¹ Time-LLM çš„å¯ç¤º  |
  |----------------|----------------------------|-----------------|
  | ES-RNN (M4 å† å†›) | ES + RNN æ··åˆæ¶æ„              | éªŒè¯æ··åˆæ–¹æ³•æœ‰æ•ˆæ€§       |
  | N-BEATS        | å¤šé¡¹å¼åŸº + å‚…é‡Œå¶åŸº                | å¯è§£é‡Šçš„è¾“å‡ºå±‚è®¾è®¡       |
  | TFT            | Variable Selection Network | å˜é‡é‡è¦æ€§é‡åŒ–         |
  | ERF            | ä¸‰é˜¶æ®µæ®‹å·®é›†æˆ                    | çº¿æ€§+éçº¿æ€§åˆ†ç¦»        |
  | DE-TSMCL       | æ—¶åºçŸ¥è¯†è’¸é¦                     | ETTm1 MSEâ†“24.2% |

  ---
  å®Œæ•´æ–‡æ¡£ä½äº /mnt/e/timellm-chuangxin/Time-LLM/md/chuangxin-jieshi1.mdï¼ŒåŒ…å«çº¦ 1080 è¡Œï¼Œæ¶µç›–ï¼š
  - å½“å‰æ–¹æ¡ˆåŸç†ä¸å…­å¤§ä¼˜åŠ¿
  - ä¸¤ä¸ªé—®é¢˜çš„è¯¦ç»†å›ç­”ä¸ä»£ç å®ç°
  - 8 ç¯‡å‰æ²¿è®ºæ–‡çš„ç†è®ºåˆ†æ
  - å®Œæ•´çš„ AdvancedHybridForecaster ä»£ç å®ç°
  - ä¸‰é˜¶æ®µå®æ–½è·¯å¾„å»ºè®®
> **ä»åŸç†åˆ°å®è·µ** â€”â€” æ·±å…¥åˆ†æä¼ ç»Ÿæ¨¡å‹ä¸ Time-LLM çš„èåˆç­–ç•¥

---

## ç›®å½•

1. [å½“å‰æ–¹æ¡ˆåŸç†ä¸ä¼˜åŠ¿](#ä¸€-å½“å‰æ–¹æ¡ˆåŸç†ä¸ä¼˜åŠ¿)
2. [é—®é¢˜ä¸€ï¼šè¾“å…¥æ•°æ®åˆ¤æ–­æœºåˆ¶](#äºŒ-é—®é¢˜ä¸€è¾“å…¥æ•°æ®åˆ¤æ–­æœºåˆ¶)
3. [é—®é¢˜äºŒï¼šä¼ ç»Ÿæ¨¡å‹ä¸ Time-LLM æ·±åº¦èåˆ](#ä¸‰-é—®é¢˜äºŒä¼ ç»Ÿæ¨¡å‹ä¸-time-llm-æ·±åº¦èåˆ)
4. [å‰æ²¿è®ºæ–‡æ”¯æ’‘ä¸ç†è®ºåˆ†æ](#å››-å‰æ²¿è®ºæ–‡æ”¯æ’‘ä¸ç†è®ºåˆ†æ)
5. [å¯è¡Œæ–¹æ¡ˆæ•´ç†ä¸æ¨è](#äº”-å¯è¡Œæ–¹æ¡ˆæ•´ç†ä¸æ¨è)
6. [å®æ–½è·¯å¾„å»ºè®®](#å…­-å®æ–½è·¯å¾„å»ºè®®)

---

## ä¸€ã€å½“å‰æ–¹æ¡ˆåŸç†ä¸ä¼˜åŠ¿

### 1.1 æ ¸å¿ƒåŸç†

å½“å‰æ–¹æ¡ˆä¸€çš„æ ¸å¿ƒæ€æƒ³æ˜¯**"åŒåˆ†æ”¯é¢„æµ‹ + è‡ªé€‚åº”èåˆ"**ï¼š

```
åŸå§‹æ—¶åºæ•°æ®
    â”‚
    â”œâ”€â”€â†’ [ä¼ ç»Ÿæ¨¡å‹åˆ†æ”¯] â”€â”€â”€â”€â†’ çº¿æ€§æˆåˆ†é¢„æµ‹
    â”‚         â”‚                    â”‚
    â”‚    (ARIMA/æŒ‡æ•°å¹³æ»‘/         â”‚
    â”‚     Holt-Winters)           â”‚
    â”‚         â”‚                    â”‚
    â”‚         â–¼                    â”‚
    â”‚    å¯è§£é‡Šæ€§è¾“å‡º              â”‚
    â”‚    (è¶‹åŠ¿/å­£èŠ‚æ€§åˆ†è§£)         â”‚
    â”‚                              â”‚
    â””â”€â”€â†’ [Time-LLM åˆ†æ”¯] â”€â”€â”€â”€â†’ éçº¿æ€§æˆåˆ†é¢„æµ‹
              â”‚                    â”‚
              â–¼                    â”‚
         å¤æ‚æ¨¡å¼æ•è·              â”‚
              â”‚                    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
              [è‡ªé€‚åº”èåˆæ¨¡å—]
                       â”‚
                       â–¼
              æœ€ç»ˆé¢„æµ‹ + å¯è§£é‡Šæ€§æŠ¥å‘Š
```

### 1.2 ç†è®ºåŸºç¡€

#### 1.2.1 çº¿æ€§ä¸éçº¿æ€§åˆ†è§£å‡è®¾

æ—¶é—´åºåˆ—å¯ä»¥è¢«åˆ†è§£ä¸ºï¼š

$$y_t = L_t + N_t + \epsilon_t$$

å…¶ä¸­ï¼š
- $L_t$ï¼šçº¿æ€§æˆåˆ†ï¼ˆè¶‹åŠ¿ã€å­£èŠ‚æ€§ï¼‰â€”â€” ä¼ ç»Ÿæ¨¡å‹æ“…é•¿
- $N_t$ï¼šéçº¿æ€§æˆåˆ†ï¼ˆå¤æ‚æ¨¡å¼ã€çªå˜ï¼‰â€”â€” æ·±åº¦å­¦ä¹ æ“…é•¿
- $\epsilon_t$ï¼šéšæœºå™ªå£°

**å…³é”®æ´å¯Ÿ**ï¼šä¼ ç»Ÿç»Ÿè®¡æ¨¡å‹ï¼ˆARIMAã€æŒ‡æ•°å¹³æ»‘ï¼‰åœ¨æ•è·çº¿æ€§è¶‹åŠ¿å’Œå‘¨æœŸæ€§æ–¹é¢å…·æœ‰ç†è®ºæœ€ä¼˜æ€§ï¼Œè€Œç¥ç»ç½‘ç»œåœ¨æ•è·å¤æ‚éçº¿æ€§æ¨¡å¼æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚

#### 1.2.2 M4 ç«èµ›çš„å¯ç¤º

[M4 ç«èµ›å† å†› ES-RNN](https://www.uber.com/blog/m4-forecasting-competition/) è¯æ˜äº†ï¼š

> "æ··åˆæ–¹æ³•å°†ä¼ ç»ŸæŒ‡æ•°å¹³æ»‘ï¼ˆESï¼‰ä¸å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç»“åˆï¼Œå–å¾—äº†è¶…è¶Šå•ä¸€æ¨¡å‹çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚ES è´Ÿè´£åˆ†è§£æ—¶åºçš„æ°´å¹³ã€è¶‹åŠ¿å’Œå­£èŠ‚æ€§æˆåˆ†ï¼ŒRNN åˆ™å­¦ä¹ è·¨åºåˆ—çš„å…±äº«å±€éƒ¨è¶‹åŠ¿ã€‚"

è¿™ä¸€æˆåŠŸæ¡ˆä¾‹ä¸º Time-LLM ä¸ä¼ ç»Ÿæ¨¡å‹çš„èåˆæä¾›äº†å¼ºæœ‰åŠ›çš„å®è¯æ”¯æŒã€‚

### 1.3 å½“å‰æ–¹æ¡ˆçš„å…­å¤§ä¼˜åŠ¿

| ä¼˜åŠ¿ | è¯¦ç»†è¯´æ˜ | ç†è®ºæ”¯æ’‘ |
|------|----------|----------|
| **å¯è§£é‡Šæ€§** | ä¼ ç»Ÿæ¨¡å‹æä¾›é€æ˜çš„è¶‹åŠ¿/å­£èŠ‚æ€§åˆ†è§£ | ç”¨æˆ·å¯ç†è§£é¢„æµ‹ä¾æ® |
| **é²æ£’æ€§** | ä¼ ç»Ÿæ¨¡å‹å¯¹å°æ ·æœ¬ã€ç®€å•æ¨¡å¼æ›´ç¨³å®š | é¿å…æ·±åº¦æ¨¡å‹çš„è¿‡æ‹Ÿåˆ |
| **è®¡ç®—æ•ˆç‡** | ä¼ ç»Ÿæ¨¡å‹ CPU è¿è¡Œï¼Œä¸å ç”¨ GPU | å¹¶è¡ŒåŠ é€Ÿï¼Œé™ä½æ¨ç†å»¶è¿Ÿ |
| **äº’è¡¥æ€§** | çº¿æ€§+éçº¿æ€§æˆåˆ†åˆ†åˆ«å»ºæ¨¡ | å‘æŒ¥å„è‡ªä¼˜åŠ¿ |
| **ç†è®ºä¿è¯** | ARIMA ç­‰æœ‰æ˜ç¡®çš„ç»Ÿè®¡ç†è®ºåŸºç¡€ | ç½®ä¿¡åŒºé—´å¯è®¡ç®— |
| **é™çº§èƒ½åŠ›** | å½“æ·±åº¦æ¨¡å‹ä¸ç¡®å®šæ—¶å¯ä¾èµ–ä¼ ç»Ÿæ¨¡å‹ | æé«˜ç³»ç»Ÿå¯é æ€§ |

---

## äºŒã€é—®é¢˜ä¸€ï¼šè¾“å…¥æ•°æ®åˆ¤æ–­æœºåˆ¶

### 2.1 å½“å‰è®¾è®¡çš„åˆ¤æ–­é€»è¾‘

**å›ç­”ï¼šæ˜¯çš„ï¼Œå½“å‰æ–¹æ¡ˆç¡®å®æ˜¯æ ¹æ®è¾“å…¥æ•°æ®çš„ç‰¹æ€§æ¥é€‰æ‹©/åŠ æƒç›¸åº”çš„ä¼ ç»Ÿæ¨¡å‹ã€‚**

åˆ¤æ–­æµç¨‹å¦‚ä¸‹ï¼š

```python
def analyze_and_select_model(x_enc):
    """
    åˆ†æè¾“å…¥æ•°æ®ç‰¹å¾ï¼Œé€‰æ‹©åˆé€‚çš„ä¼ ç»Ÿæ¨¡å‹ç­–ç•¥
    """
    # 1. å¹³ç¨³æ€§æ£€éªŒ (ADF Test)
    is_stationary = adf_test(x_enc)

    # 2. å‘¨æœŸæ€§æ£€æµ‹ (FFT/è‡ªç›¸å…³åˆ†æ)
    periodicity, main_periods = detect_periodicity(x_enc)

    # 3. è¶‹åŠ¿æ€§åˆ¤æ–­ (çº¿æ€§å›å½’æ–œç‡)
    trend_strength = compute_trend(x_enc)

    # 4. å™ªå£°æ°´å¹³è¯„ä¼°
    noise_level = compute_noise_ratio(x_enc)

    # å†³ç­–é€»è¾‘
    if is_stationary and periodicity > 0.7:
        return "ARIMA", high_weight  # å¹³ç¨³+å¼ºå‘¨æœŸ â†’ ARIMA æƒé‡é«˜
    elif trend_strength > 0.5 and periodicity > 0.5:
        return "Holt-Winters", high_weight  # è¶‹åŠ¿+å‘¨æœŸ â†’ HW æƒé‡é«˜
    elif noise_level > 0.6:
        return "MovingAverage", medium_weight  # é«˜å™ªå£° â†’ ç§»åŠ¨å¹³å‡é¢„å¤„ç†
    else:
        return "TimeLLM_Only", low_weight  # å¤æ‚æ¨¡å¼ â†’ ä¾èµ– Time-LLM
```

### 2.2 æ•°æ®ç‰¹å¾æ£€æµ‹æ–¹æ³•

#### 2.2.1 å¹³ç¨³æ€§æ£€éªŒ (ADF Test)

```python
from statsmodels.tsa.stattools import adfuller

def adf_test(series, significance=0.05):
    """ADF å¹³ç¨³æ€§æ£€éªŒ"""
    result = adfuller(series.flatten(), autolag='AIC')
    p_value = result[1]
    return p_value < significance  # True = å¹³ç¨³
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- p-value < 0.05 â†’ å¹³ç¨³åºåˆ— â†’ ARIMA é€‚ç”¨
- p-value â‰¥ 0.05 â†’ éå¹³ç¨³åºåˆ— â†’ éœ€å·®åˆ†æˆ–ä½¿ç”¨å…¶ä»–æ¨¡å‹

#### 2.2.2 å‘¨æœŸæ€§æ£€æµ‹ (FFT èƒ½é‡é›†ä¸­åº¦)

```python
def detect_periodicity(x, top_k=5):
    """åŸºäº FFT çš„å‘¨æœŸæ€§æ£€æµ‹"""
    x_fft = torch.fft.rfft(x, dim=1)
    amplitude = torch.abs(x_fft)

    # èƒ½é‡é›†ä¸­åº¦ï¼šTop-K é¢‘ç‡èƒ½é‡å æ¯”
    total_energy = amplitude.sum()
    sorted_amp, _ = torch.sort(amplitude, descending=True)
    top_k_energy = sorted_amp[:top_k].sum()

    periodicity_score = top_k_energy / (total_energy + 1e-8)

    # ä¸»è¦å‘¨æœŸæå–
    _, top_indices = torch.topk(amplitude, top_k)
    freqs = torch.fft.rfftfreq(x.shape[1])
    main_periods = 1.0 / (freqs[top_indices] + 1e-8)

    return periodicity_score.item(), main_periods.tolist()
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- èƒ½é‡é›†ä¸­åº¦ > 0.7 â†’ å¼ºå‘¨æœŸæ€§ â†’ ä¼ ç»Ÿæ¨¡å‹æƒé‡é«˜
- èƒ½é‡é›†ä¸­åº¦ < 0.3 â†’ å¼±å‘¨æœŸæ€§ â†’ Time-LLM æƒé‡é«˜

#### 2.2.3 è¶‹åŠ¿å¼ºåº¦æ£€æµ‹

```python
def compute_trend(x):
    """è®¡ç®—è¶‹åŠ¿å¼ºåº¦"""
    T = x.shape[1]
    t = torch.arange(T, dtype=x.dtype, device=x.device)

    # çº¿æ€§å›å½’æ–œç‡
    slope = torch.polyfit(t, x.mean(dim=-1), 1)[0]

    # å½’ä¸€åŒ–è¶‹åŠ¿å¼ºåº¦
    trend_strength = abs(slope) / (x.std() + 1e-8)
    return trend_strength.item()
```

### 2.3 æ¨¡å‹é€‰æ‹©çš„ä¼˜ç¼ºç‚¹åˆ†æ

| ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|
| é’ˆå¯¹æ€§å¼ºï¼šæ ¹æ®æ•°æ®ç‰¹æ€§é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹ | **é™æ€é€‰æ‹©**ï¼šä¸€æ—¦é€‰å®šï¼Œæ•´ä¸ªåºåˆ—ä½¿ç”¨åŒä¸€æ¨¡å‹ |
| è®¡ç®—é«˜æ•ˆï¼šé¿å…ä¸å¿…è¦çš„æ¨¡å‹è¿è¡Œ | **äºŒå…ƒå†³ç­–**ï¼šæ— æ³•å¤„ç†åºåˆ—å†…éƒ¨çš„æ¨¡å¼å˜åŒ– |
| å¯è§£é‡Šï¼šé€‰æ‹©ç†ç”±æ˜ç¡® | **è¾¹ç•Œé—®é¢˜**ï¼šé˜ˆå€¼è®¾å®šå¯èƒ½ä¸å¤Ÿçµæ´» |

---

## ä¸‰ã€é—®é¢˜äºŒï¼šä¼ ç»Ÿæ¨¡å‹ä¸ Time-LLM æ·±åº¦èåˆ

### 3.1 é—®é¢˜åˆ†æ

æ‚¨æå‡ºäº†ä¸‰ä¸ªå…³é”®å­é—®é¢˜ï¼š

1. **å±€éƒ¨åŒºé—´å¼•å…¥**ï¼šè¾“å…¥æ•°æ®çš„æŸäº›éƒ¨åˆ†ç¬¦åˆä¼ ç»Ÿæ¨¡å‹ï¼Œèƒ½å¦åœ¨è¯¥åŒºé—´å¼•å…¥ï¼Ÿ
2. **è¾“å‡ºåŒºé—´å¼•å…¥**ï¼šé¢„æµ‹è¾“å‡ºåœ¨æŸäº›åŒºé—´ç¬¦åˆä¼ ç»Ÿæ¨¡å‹èµ°åŠ¿ï¼Œèƒ½å¦åœ¨è¯¥åŒºé—´ä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹ï¼Ÿ
3. **çŸ¥è¯†è’¸é¦**ï¼šèƒ½å¦è®© Time-LLM å­¦ä¹ ä¼ ç»Ÿæ¨¡å‹çš„ä¼˜åŠ¿ï¼Ÿ

### 3.2 æ–¹æ¡ˆ Aï¼šæ®‹å·®å­¦ä¹ æ¶æ„ (Residual Learning)

#### 3.2.1 æ ¸å¿ƒæ€æƒ³

**ä¸æ˜¯"é€‰æ‹©"è€Œæ˜¯"åä½œ"**ï¼šä¼ ç»Ÿæ¨¡å‹å…ˆå¤„ç†çº¿æ€§æˆåˆ†ï¼ŒTime-LLM å­¦ä¹ æ®‹å·®ä¸­çš„éçº¿æ€§æˆåˆ†ã€‚

```
åŸå§‹åºåˆ— y_t
    â”‚
    â–¼
[ä¼ ç»Ÿæ¨¡å‹ (ARIMA/ES)]
    â”‚
    â”œâ”€â”€â†’ çº¿æ€§é¢„æµ‹ Å·_linear
    â”‚
    â””â”€â”€â†’ æ®‹å·® r_t = y_t - Å·_linear
              â”‚
              â–¼
         [Time-LLM]
              â”‚
              â–¼
         éçº¿æ€§é¢„æµ‹ Å·_nonlinear
              â”‚
              â–¼
æœ€ç»ˆé¢„æµ‹ = Å·_linear + Å·_nonlinear
```

#### 3.2.2 ç†è®ºæ”¯æ’‘

[Hybrid ARIMA-LSTM ç ”ç©¶](https://link.springer.com/article/10.1007/s44196-025-00930-4) è¡¨æ˜ï¼š

> "æ®‹å·®ä¿®æ­£æ¡†æ¶å°†é¢„æµ‹ä»»åŠ¡åˆ†è§£ä¸ºçº¿æ€§è¶‹åŠ¿åˆ†æå’Œéçº¿æ€§æ®‹å·®å­¦ä¹ ã€‚ARIMA æ“…é•¿å­¦ä¹ çº¿æ€§å…³ç³»ï¼Œè€Œ CNN/LSTM åœ¨æ•è·éçº¿æ€§å…³ç³»æ–¹é¢æ›´ä¼˜ã€‚"

[STL åˆ†è§£ + æ··åˆæ¨¡å‹](https://arxiv.org/abs/2510.23668) è¯æ˜ï¼š

> "STL å°†æ—¶åºåˆ†è§£ä¸ºè¶‹åŠ¿ã€å­£èŠ‚æ€§å’Œæ®‹å·®ã€‚LSTM å»ºæ¨¡é•¿æœŸè¶‹åŠ¿ï¼ŒARIMA æ•è·å­£èŠ‚å‘¨æœŸï¼ŒXGBoost é¢„æµ‹éçº¿æ€§æ®‹å·®æ³¢åŠ¨ã€‚"

#### 3.2.3 ä»£ç å®ç°

```python
class ResidualHybridForecaster(nn.Module):
    """æ®‹å·®å­¦ä¹ æ··åˆé¢„æµ‹å™¨"""

    def __init__(self, pred_len, traditional_model='arima'):
        super().__init__()
        self.pred_len = pred_len
        self.traditional_model = traditional_model

    def forward(self, x_enc, timellm_model):
        """
        Args:
            x_enc: [B, T, N] è¾“å…¥åºåˆ—
            timellm_model: Time-LLM æ¨¡å‹å®ä¾‹
        """
        B, T, N = x_enc.shape
        device = x_enc.device

        # Step 1: ä¼ ç»Ÿæ¨¡å‹é¢„æµ‹ + è®¡ç®—æ®‹å·®
        linear_pred = torch.zeros(B, self.pred_len, N, device=device)
        residuals = torch.zeros_like(x_enc)

        for b in range(B):
            for n in range(N):
                series = x_enc[b, :, n].cpu().numpy()

                # ARIMA æ‹Ÿåˆ
                arima = ARIMA(series, order=(2, 1, 1))
                fitted = arima.fit()

                # çº¿æ€§é¢„æµ‹
                linear_pred[b, :, n] = torch.tensor(
                    fitted.forecast(self.pred_len), device=device
                )

                # è®¡ç®—æ®‹å·® (åŸåºåˆ— - æ‹Ÿåˆå€¼)
                residuals[b, :, n] = x_enc[b, :, n] - torch.tensor(
                    fitted.fittedvalues, device=device
                )

        # Step 2: Time-LLM å­¦ä¹ æ®‹å·®æ¨¡å¼
        # å°†æ®‹å·®ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹æœªæ¥æ®‹å·®
        nonlinear_pred = timellm_model.forecast_from_residuals(residuals)

        # Step 3: æœ€ç»ˆé¢„æµ‹ = çº¿æ€§é¢„æµ‹ + éçº¿æ€§é¢„æµ‹
        final_pred = linear_pred + nonlinear_pred

        return final_pred, {
            'linear_component': linear_pred,
            'nonlinear_component': nonlinear_pred
        }
```

#### 3.2.4 ä¼˜åŠ¿ä¸é€‚ç”¨åœºæ™¯

| ä¼˜åŠ¿ | è¯´æ˜ |
|------|------|
| **å…¨åºåˆ—åä½œ** | ä¼ ç»Ÿæ¨¡å‹å’Œ Time-LLM åŒæ—¶ä½œç”¨äºæ•´ä¸ªåºåˆ— |
| **è‡ªåŠ¨åˆ†å·¥** | çº¿æ€§æˆåˆ†è‡ªåŠ¨è¢«ä¼ ç»Ÿæ¨¡å‹å¤„ç† |
| **å¯è§£é‡Šæ€§å¼º** | å¯ä»¥åˆ†åˆ«æŸ¥çœ‹çº¿æ€§å’Œéçº¿æ€§è´¡çŒ® |
| **ç†è®ºæˆç†Ÿ** | å¤§é‡ç ”ç©¶éªŒè¯äº†æ®‹å·®å­¦ä¹ çš„æœ‰æ•ˆæ€§ |

**é€‚ç”¨åœºæ™¯**ï¼š
- æ•°æ®åŒæ—¶åŒ…å«æ˜æ˜¾è¶‹åŠ¿/å­£èŠ‚æ€§å’Œå¤æ‚éçº¿æ€§æ¨¡å¼
- éœ€è¦å¯è§£é‡Šçš„é¢„æµ‹åˆ†è§£
- ETTã€Traffic ç­‰å‘¨æœŸæ€§+çªå˜æ··åˆçš„æ•°æ®é›†

---

### 3.3 æ–¹æ¡ˆ Bï¼šåˆ†æ®µè‡ªé€‚åº”èåˆ (Segment-wise Adaptive Fusion)

#### 3.3.1 æ ¸å¿ƒæ€æƒ³

**å›ç­”æ‚¨çš„é—®é¢˜**ï¼šæ˜¯çš„ï¼Œå¯ä»¥åœ¨ç¬¦åˆä¼ ç»Ÿæ¨¡å‹çš„åŒºé—´å¼•å…¥ä¼ ç»Ÿæ¨¡å‹ï¼

```
è¾“å…¥åºåˆ— [t_0, t_1, ..., t_T]
    â”‚
    â–¼
[åˆ†æ®µæ¨¡å¼æ£€æµ‹å™¨]
    â”‚
    â”œâ”€â”€â†’ Segment 1 [t_0:t_k]: å‘¨æœŸæ€§å¼º â†’ ä¼ ç»Ÿæ¨¡å‹æƒé‡ 0.8
    â”‚
    â”œâ”€â”€â†’ Segment 2 [t_k:t_m]: çªå˜/å¤æ‚ â†’ Time-LLM æƒé‡ 0.9
    â”‚
    â””â”€â”€â†’ Segment 3 [t_m:t_T]: è¶‹åŠ¿æ˜æ˜¾ â†’ ä¼ ç»Ÿæ¨¡å‹æƒé‡ 0.7
              â”‚
              â–¼
     [åˆ†æ®µåŠ æƒèåˆ] â†’ æœ€ç»ˆé¢„æµ‹
```

#### 3.3.2 ç†è®ºæ”¯æ’‘

[Explainable Adaptive Tree-based Model Selection (TSMS)](https://arxiv.org/pdf/2401.01124) æå‡ºï¼š

> "åŸºäº'èƒ½åŠ›åŒºåŸŸ'(Region of Competence, RoC) é€‰æ‹©é¢„æµ‹å™¨ï¼šé€‰æ‹©é¢„æµ‹å™¨ i æ˜¯å› ä¸ºå…¶ RoC åŒ…å«ä¸å½“å‰è¾“å…¥æ¨¡å¼æœ€ç›¸ä¼¼çš„å­åºåˆ—ã€‚"

[Adaptive Multi-Scale Decomposition (AMD) Framework](https://arxiv.org/html/2406.03751v1) è¯æ˜ï¼š

> "AMD ä½¿ç”¨ Mixture-of-Experts (MoE) çš„è‡ªé€‚åº”ç‰¹æ€§ï¼Œä¸ºä¸åŒçš„æ—¶é—´æ¨¡å¼è®¾è®¡ä¸åŒçš„é¢„æµ‹å™¨ã€‚æ—¶é—´æ¨¡å¼é€‰æ‹©å™¨ (TP-Selector) åŠ¨æ€åˆ†é…æƒé‡ã€‚"

[STaRNet](https://www.sciencedirect.com/science/article/abs/pii/S0952197625031355) å¼•å…¥ï¼š

> "è½»é‡çº§åˆ†æ®µé—¨æ§å‰é¦ˆç½‘ç»œ (LSG-FFN) è‡ªé€‚åº”åœ°æ”¾å¤§æˆ–æŠ‘åˆ¶ä¸åŒæ—¶é—´æ®µçš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œå®ç°åŒºåŸŸç‰¹å®šæ¨¡å¼çš„ç²¾ç¡®å»ºæ¨¡ã€‚"

#### 3.3.3 ä»£ç å®ç°

```python
class SegmentAdaptiveFusion(nn.Module):
    """åˆ†æ®µè‡ªé€‚åº”èåˆæ¨¡å—"""

    def __init__(self, d_model, num_segments=4, segment_len=None):
        super().__init__()
        self.num_segments = num_segments

        # åˆ†æ®µæ¨¡å¼æ£€æµ‹å™¨
        self.segment_analyzer = nn.Sequential(
            nn.Linear(segment_len or 24, 64),
            nn.GELU(),
            nn.Linear(64, 32),
            nn.GELU(),
            nn.Linear(32, 3)  # è¾“å‡º: [å‘¨æœŸæ€§åˆ†æ•°, è¶‹åŠ¿åˆ†æ•°, å¤æ‚æ€§åˆ†æ•°]
        )

        # æƒé‡ç”Ÿæˆå™¨
        self.weight_generator = nn.Sequential(
            nn.Linear(3, 16),
            nn.GELU(),
            nn.Linear(16, 2),  # [ä¼ ç»Ÿæ¨¡å‹æƒé‡, Time-LLM æƒé‡]
            nn.Softmax(dim=-1)
        )

    def forward(self, x_enc, traditional_pred, timellm_pred):
        """
        Args:
            x_enc: [B, T, N] è¾“å…¥åºåˆ—
            traditional_pred: [B, pred_len, N] ä¼ ç»Ÿæ¨¡å‹é¢„æµ‹
            timellm_pred: [B, pred_len, N] Time-LLM é¢„æµ‹
        """
        B, T, N = x_enc.shape
        segment_len = T // self.num_segments

        # åˆ†æ®µåˆ†æ
        segment_weights = []
        for i in range(self.num_segments):
            start = i * segment_len
            end = start + segment_len
            segment = x_enc[:, start:end, :]  # [B, seg_len, N]

            # åˆ†ææ¯ä¸ªæ®µçš„æ¨¡å¼ç‰¹å¾
            seg_flat = segment.mean(dim=-1)  # [B, seg_len]
            pattern_scores = self.segment_analyzer(seg_flat)  # [B, 3]

            # ç”Ÿæˆè¯¥æ®µçš„èåˆæƒé‡
            weights = self.weight_generator(pattern_scores)  # [B, 2]
            segment_weights.append(weights)

        # å°†åˆ†æ®µæƒé‡æ˜ å°„åˆ°é¢„æµ‹é•¿åº¦
        # ç®€åŒ–ï¼šä½¿ç”¨æœ€åä¸€ä¸ªæ®µçš„æƒé‡ï¼ˆæœ€æ¥è¿‘é¢„æµ‹åŒºé—´ï¼‰
        final_weights = segment_weights[-1]  # [B, 2]

        # åŠ æƒèåˆ
        w_trad = final_weights[:, 0:1].unsqueeze(-1)  # [B, 1, 1]
        w_llm = final_weights[:, 1:2].unsqueeze(-1)   # [B, 1, 1]

        fused_pred = w_trad * traditional_pred + w_llm * timellm_pred

        return fused_pred, {
            'segment_weights': segment_weights,
            'traditional_weight': w_trad.mean().item(),
            'timellm_weight': w_llm.mean().item()
        }
```

#### 3.3.4 é¢„æµ‹è¾“å‡ºçš„åˆ†æ®µèåˆ

**å›ç­”æ‚¨çš„é—®é¢˜**ï¼šæ˜¯çš„ï¼Œé¢„æµ‹è¾“å‡ºä¹Ÿå¯ä»¥åˆ†æ®µèåˆï¼

```python
class OutputSegmentFusion(nn.Module):
    """é¢„æµ‹è¾“å‡ºçš„åˆ†æ®µè‡ªé€‚åº”èåˆ"""

    def __init__(self, pred_len, num_output_segments=4):
        super().__init__()
        self.pred_len = pred_len
        self.num_segments = num_output_segments
        self.segment_len = pred_len // num_output_segments

        # åŸºäºé¢„æµ‹å€¼æœ¬èº«åˆ¤æ–­èåˆæƒé‡
        self.output_analyzer = nn.Sequential(
            nn.Linear(self.segment_len * 2, 32),  # è¾“å…¥: [trad_seg, llm_seg]
            nn.GELU(),
            nn.Linear(32, 2),
            nn.Softmax(dim=-1)
        )

    def forward(self, traditional_pred, timellm_pred):
        """
        æ ¹æ®é¢„æµ‹è¾“å‡ºçš„ç‰¹æ€§ï¼Œåˆ†æ®µå†³å®šèåˆæƒé‡
        """
        B, L, N = traditional_pred.shape
        fused_segments = []
        segment_info = []

        for i in range(self.num_segments):
            start = i * self.segment_len
            end = start + self.segment_len

            trad_seg = traditional_pred[:, start:end, :]  # [B, seg_len, N]
            llm_seg = timellm_pred[:, start:end, :]       # [B, seg_len, N]

            # åˆ†æä¸¤ä¸ªé¢„æµ‹çš„å·®å¼‚å’Œç‰¹æ€§
            for n in range(N):
                seg_pair = torch.cat([
                    trad_seg[:, :, n],  # [B, seg_len]
                    llm_seg[:, :, n]    # [B, seg_len]
                ], dim=-1)  # [B, seg_len * 2]

                # åˆ¤æ–­è¯¥æ®µåº”è¯¥æ›´ä¿¡ä»»å“ªä¸ªæ¨¡å‹
                weights = self.output_analyzer(seg_pair)  # [B, 2]

                # åŠ æƒèåˆè¯¥æ®µ
                w_trad = weights[:, 0:1]  # [B, 1]
                w_llm = weights[:, 1:2]   # [B, 1]

                fused_seg = w_trad * trad_seg[:, :, n] + w_llm * llm_seg[:, :, n]
                fused_segments.append(fused_seg)

                segment_info.append({
                    'segment': i,
                    'variable': n,
                    'trad_weight': w_trad.mean().item(),
                    'llm_weight': w_llm.mean().item()
                })

        # é‡ç»„è¾“å‡º
        fused_pred = torch.stack(fused_segments, dim=-1)  # éœ€è¦é€‚å½“é‡å¡‘

        return fused_pred, segment_info
```

---

### 3.4 æ–¹æ¡ˆ Cï¼šçŸ¥è¯†è’¸é¦ (Knowledge Distillation)

#### 3.4.1 æ ¸å¿ƒæ€æƒ³

**å›ç­”æ‚¨çš„é—®é¢˜**ï¼šæ˜¯çš„ï¼Œå¯ä»¥è®© Time-LLM å­¦ä¹ ä¼ ç»Ÿæ¨¡å‹çš„ä¼˜åŠ¿ï¼

```
                    [ä¼ ç»Ÿæ¨¡å‹ (Teacher)]
                           â”‚
                           â–¼
                    è¶‹åŠ¿/å­£èŠ‚æ€§é¢„æµ‹
                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                      â”‚                      â”‚
    â–¼                      â–¼                      â–¼
[è½¯æ ‡ç­¾è’¸é¦]         [ç‰¹å¾å¯¹é½è’¸é¦]         [è¡Œä¸ºæ¨¡ä»¿è’¸é¦]
    â”‚                      â”‚                      â”‚
    â”‚   ä¼ ç»Ÿæ¨¡å‹çš„         â”‚   ä¼ ç»Ÿæ¨¡å‹çš„         â”‚   ä¼ ç»Ÿæ¨¡å‹çš„
    â”‚   é¢„æµ‹åˆ†å¸ƒ           â”‚   åˆ†è§£ç‰¹å¾           â”‚   è¾“å‡ºè¶‹åŠ¿
    â”‚                      â”‚                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                      [Time-LLM]
                           â”‚
                           â–¼
                    å­¦ä¹ ä¼ ç»Ÿæ¨¡å‹çš„ä¼˜åŠ¿
```

#### 3.4.2 ç†è®ºæ”¯æ’‘

[DE-TSMCL (Distillation Enhanced Time Series Forecasting)](https://arxiv.org/html/2401.17802v1) è¯æ˜ï¼š

> "çŸ¥è¯†è’¸é¦æŠ€æœ¯å¯ä»¥æ˜¾è‘—æå‡æ—¶åºé¢„æµ‹æ€§èƒ½ã€‚ä¸ TS2Vec ç›¸æ¯”ï¼ŒDE-TSMCL åœ¨ ETTm1 ä¸Š MSE æå‡ 24.2%ï¼ŒMAE æå‡ 14.7%ã€‚"

[Clinical Time Series Knowledge Distillation](https://dspace.mit.edu/handle/1721.1/151355) è¡¨æ˜ï¼š

> "çŸ¥è¯†è’¸é¦å¯ä»¥å°†é«˜é¢„æµ‹èƒ½åŠ›çš„'æ•™å¸ˆæ¨¡å‹'çš„çŸ¥è¯†è¿ç§»åˆ°å…·æœ‰å…¶ä»–ä¼˜è‰¯ç‰¹æ€§ï¼ˆå¦‚å¯è§£é‡Šæ€§ï¼‰çš„'å­¦ç”Ÿæ¨¡å‹'ä¸­ã€‚"

#### 3.4.3 ä¸‰ç§è’¸é¦ç­–ç•¥

##### ç­–ç•¥ 1ï¼šè½¯æ ‡ç­¾è’¸é¦ (Soft Label Distillation)

```python
class SoftLabelDistillation(nn.Module):
    """è½¯æ ‡ç­¾è’¸é¦ï¼šè®© Time-LLM å­¦ä¹ ä¼ ç»Ÿæ¨¡å‹çš„é¢„æµ‹åˆ†å¸ƒ"""

    def __init__(self, temperature=2.0, alpha=0.3):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha  # è’¸é¦æŸå¤±æƒé‡

    def distillation_loss(self, student_pred, teacher_pred, ground_truth):
        """
        Args:
            student_pred: Time-LLM é¢„æµ‹
            teacher_pred: ä¼ ç»Ÿæ¨¡å‹é¢„æµ‹
            ground_truth: çœŸå®å€¼
        """
        # ä¸»æŸå¤±ï¼šå­¦ç”Ÿ vs çœŸå®å€¼
        main_loss = F.mse_loss(student_pred, ground_truth)

        # è’¸é¦æŸå¤±ï¼šå­¦ç”Ÿ vs æ•™å¸ˆ (è½¯åŒ–å)
        # å¯¹äºå›å½’ä»»åŠ¡ï¼Œä½¿ç”¨ Huber Loss æ›´ç¨³å®š
        distill_loss = F.smooth_l1_loss(
            student_pred / self.temperature,
            teacher_pred / self.temperature
        ) * (self.temperature ** 2)

        # ç»„åˆæŸå¤±
        total_loss = (1 - self.alpha) * main_loss + self.alpha * distill_loss

        return total_loss, {
            'main_loss': main_loss.item(),
            'distill_loss': distill_loss.item()
        }
```

##### ç­–ç•¥ 2ï¼šè¶‹åŠ¿-å­£èŠ‚æ€§åˆ†è§£è’¸é¦

```python
class DecompositionDistillation(nn.Module):
    """åˆ†è§£è’¸é¦ï¼šè®© Time-LLM å­¦ä¹ ä¼ ç»Ÿæ¨¡å‹çš„è¶‹åŠ¿/å­£èŠ‚æ€§åˆ†è§£èƒ½åŠ›"""

    def __init__(self, d_model, pred_len):
        super().__init__()

        # Time-LLM çš„å¯å­¦ä¹ åˆ†è§£å¤´
        self.trend_head = nn.Linear(d_model, pred_len)
        self.seasonal_head = nn.Linear(d_model, pred_len)

    def forward(self, llm_features, traditional_decomposition):
        """
        Args:
            llm_features: Time-LLM çš„éšè—ç‰¹å¾ [B, L, D]
            traditional_decomposition: ä¼ ç»Ÿæ¨¡å‹çš„åˆ†è§£ {'trend': ..., 'seasonal': ...}
        """
        # ä» LLM ç‰¹å¾é¢„æµ‹è¶‹åŠ¿å’Œå­£èŠ‚æ€§
        features_pooled = llm_features.mean(dim=1)  # [B, D]

        pred_trend = self.trend_head(features_pooled)
        pred_seasonal = self.seasonal_head(features_pooled)

        # è’¸é¦æŸå¤±ï¼šå¯¹é½åˆ°ä¼ ç»Ÿæ¨¡å‹çš„åˆ†è§£
        trend_loss = F.mse_loss(pred_trend, traditional_decomposition['trend'])
        seasonal_loss = F.mse_loss(pred_seasonal, traditional_decomposition['seasonal'])

        decomp_loss = trend_loss + seasonal_loss

        return decomp_loss, {
            'pred_trend': pred_trend,
            'pred_seasonal': pred_seasonal
        }
```

##### ç­–ç•¥ 3ï¼šè¡Œä¸ºæ¨¡ä»¿è’¸é¦ (Behavior Cloning)

```python
class BehaviorDistillation(nn.Module):
    """è¡Œä¸ºè’¸é¦ï¼šè®© Time-LLM æ¨¡ä»¿ä¼ ç»Ÿæ¨¡å‹åœ¨ç‰¹å®šåŒºé—´çš„é¢„æµ‹è¡Œä¸º"""

    def __init__(self):
        super().__init__()

    def compute_behavioral_similarity(self, student_pred, teacher_pred):
        """
        è®¡ç®—é¢„æµ‹è¡Œä¸ºçš„ç›¸ä¼¼åº¦ï¼ˆè€Œéå…·ä½“æ•°å€¼ï¼‰
        """
        # 1. è¶‹åŠ¿æ–¹å‘ä¸€è‡´æ€§
        student_diff = student_pred[:, 1:] - student_pred[:, :-1]
        teacher_diff = teacher_pred[:, 1:] - teacher_pred[:, :-1]

        direction_match = (torch.sign(student_diff) == torch.sign(teacher_diff)).float()
        direction_loss = 1 - direction_match.mean()

        # 2. å˜åŒ–å¹…åº¦ä¸€è‡´æ€§
        student_magnitude = torch.abs(student_diff)
        teacher_magnitude = torch.abs(teacher_diff)

        magnitude_loss = F.mse_loss(
            student_magnitude / (student_magnitude.max() + 1e-8),
            teacher_magnitude / (teacher_magnitude.max() + 1e-8)
        )

        # 3. å‘¨æœŸæ€§è¡Œä¸ºä¸€è‡´æ€§ (FFT é¢‘è°±ç›¸ä¼¼åº¦)
        student_fft = torch.fft.rfft(student_pred, dim=1)
        teacher_fft = torch.fft.rfft(teacher_pred, dim=1)

        spectrum_loss = F.mse_loss(
            torch.abs(student_fft),
            torch.abs(teacher_fft)
        )

        total_loss = direction_loss + magnitude_loss + 0.5 * spectrum_loss

        return total_loss, {
            'direction_loss': direction_loss.item(),
            'magnitude_loss': magnitude_loss.item(),
            'spectrum_loss': spectrum_loss.item()
        }
```

---

## å››ã€å‰æ²¿è®ºæ–‡æ”¯æ’‘ä¸ç†è®ºåˆ†æ

### 4.1 N-BEATSï¼šå¯è§£é‡Šçš„ç¥ç»åŸºæ‰©å±•åˆ†æ

[N-BEATS (ICLR 2020)](https://arxiv.org/abs/1905.10437) æä¾›äº†é‡è¦çš„ç†è®ºæ¡†æ¶ï¼š

**æ ¸å¿ƒæ€æƒ³**ï¼š
> "é€šè¿‡çº¦æŸç¥ç»ç½‘ç»œçš„åŸºæ‰©å±•å‡½æ•°ï¼Œå¼ºåˆ¶æ¨¡å‹ä»…å­¦ä¹ è¶‹åŠ¿å’Œå­£èŠ‚æ€§æˆåˆ†ï¼Œå®ç°å¯è§£é‡Šæ€§ã€‚"

**å®ç°æ–¹æ³•**ï¼š
- ä½¿ç”¨**å¤šé¡¹å¼åŸº**å»ºæ¨¡è¶‹åŠ¿
- ä½¿ç”¨**å‚…é‡Œå¶åŸº**å»ºæ¨¡å­£èŠ‚æ€§
- åŒé‡æ®‹å·®å †å ï¼šè¶‹åŠ¿ä»è¾“å…¥çª—å£ä¸­å»é™¤ï¼Œè¶‹åŠ¿å’Œå­£èŠ‚æ€§çš„éƒ¨åˆ†é¢„æµ‹å¯ä½œä¸ºç‹¬ç«‹çš„å¯è§£é‡Šè¾“å‡º

**å¯¹ Time-LLM çš„å¯ç¤º**ï¼š
```python
# å¯ä»¥åœ¨ Time-LLM çš„è¾“å‡ºå±‚æ·»åŠ ç±»ä¼¼çš„åŸºæ‰©å±•çº¦æŸ
class InterpretableOutputHead(nn.Module):
    def __init__(self, d_model, pred_len, polynomial_degree=3, num_harmonics=5):
        super().__init__()

        # è¶‹åŠ¿åŸºï¼šå¤šé¡¹å¼
        self.trend_coeffs = nn.Linear(d_model, polynomial_degree + 1)

        # å­£èŠ‚åŸºï¼šå‚…é‡Œå¶
        self.seasonal_coeffs = nn.Linear(d_model, num_harmonics * 2)

        self.pred_len = pred_len
        self.polynomial_degree = polynomial_degree
        self.num_harmonics = num_harmonics

    def forward(self, features):
        # å¤šé¡¹å¼è¶‹åŠ¿
        t = torch.linspace(0, 1, self.pred_len, device=features.device)
        trend_powers = torch.stack([t ** i for i in range(self.polynomial_degree + 1)], dim=-1)
        trend = (self.trend_coeffs(features).unsqueeze(1) * trend_powers.unsqueeze(0)).sum(-1)

        # å‚…é‡Œå¶å­£èŠ‚æ€§
        freqs = torch.arange(1, self.num_harmonics + 1, device=features.device)
        seasonal = torch.zeros(features.shape[0], self.pred_len, device=features.device)
        coeffs = self.seasonal_coeffs(features)
        for h in range(self.num_harmonics):
            a, b = coeffs[:, 2*h], coeffs[:, 2*h + 1]
            seasonal += a.unsqueeze(1) * torch.cos(2 * np.pi * freqs[h] * t.unsqueeze(0))
            seasonal += b.unsqueeze(1) * torch.sin(2 * np.pi * freqs[h] * t.unsqueeze(0))

        return trend, seasonal, trend + seasonal
```

### 4.2 Temporal Fusion Transformer (TFT)ï¼šå˜é‡é€‰æ‹©ä¸æ³¨æ„åŠ›å¯è§£é‡Šæ€§

[TFT (International Journal of Forecasting 2021)](https://arxiv.org/abs/1912.09363) æä¾›äº†å¯è§£é‡Šæ€§çš„èŒƒä¾‹ï¼š

**ä¸‰ç§å¯è§£é‡Šæ€§**ï¼š
1. **å˜é‡é‡è¦æ€§**ï¼šVariable Selection Network é‡åŒ–æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§
2. **æ—¶é—´é‡è¦æ€§**ï¼šå¯è§£é‡Šçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶è¡¡é‡è¿‡å»æ—¶é—´æ­¥çš„é‡è¦æ€§
3. **é™æ€åå˜é‡å½±å“**ï¼šé™æ€å˜é‡å¦‚ä½•å½±å“é¢„æµ‹

**å¯¹ Time-LLM çš„å¯ç¤º**ï¼š
```python
class VariableSelectionNetwork(nn.Module):
    """å˜é‡é€‰æ‹©ç½‘ç»œï¼šé‡åŒ–æ¯ä¸ªè¾“å…¥å˜é‡çš„é‡è¦æ€§"""

    def __init__(self, d_model, n_vars):
        super().__init__()
        self.n_vars = n_vars

        # æ¯ä¸ªå˜é‡çš„é‡è¦æ€§æ‰“åˆ†
        self.importance_scorer = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Linear(d_model, 1)
        )

        # å…¨å±€å˜é‡é€‰æ‹© (ç±»ä¼¼ TFT)
        self.global_selector = nn.Linear(n_vars, n_vars)

    def forward(self, x, return_weights=True):
        """
        Args:
            x: [B, T, N, D] å¤šå˜é‡ç‰¹å¾
        """
        B, T, N, D = x.shape

        # è®¡ç®—æ¯ä¸ªå˜é‡çš„é‡è¦æ€§åˆ†æ•°
        scores = self.importance_scorer(x)  # [B, T, N, 1]
        weights = F.softmax(scores.squeeze(-1), dim=-1)  # [B, T, N]

        # åŠ æƒèšåˆ
        weighted_x = (x * weights.unsqueeze(-1)).sum(dim=2)  # [B, T, D]

        if return_weights:
            # è¿”å›å¯è§£é‡Šçš„å˜é‡é‡è¦æ€§
            var_importance = weights.mean(dim=(0, 1))  # [N]
            return weighted_x, var_importance

        return weighted_x
```

### 4.3 ES-RNNï¼šM4 ç«èµ›å† å†›çš„æ··åˆæ¶æ„

[ES-RNN (Uber)](https://www.uber.com/blog/m4-forecasting-competition/) çš„å…³é”®åˆ›æ–°ï¼š

**æ¶æ„ç‰¹ç‚¹**ï¼š
- **å±‚çº§åŒ–æ··åˆ**ï¼šES å‚æ•°æ˜¯åºåˆ—ç‰¹å®šçš„ï¼ŒRNN å‚æ•°æ˜¯å…¨å±€å…±äº«çš„
- **åŠ¨æ€è®¡ç®—å›¾**ï¼šæ¯ä¸ªåºåˆ—çš„è®¡ç®—å›¾å› åŒ…å«åºåˆ—ç‰¹å®šå‚æ•°è€Œä¸åŒ
- **åŒæ­¥ä¼˜åŒ–**ï¼šES å¹³æ»‘ç³»æ•°å’Œ RNN æƒé‡é€šè¿‡åŒä¸€ä¸ª SGD è¿‡ç¨‹ä¼˜åŒ–

**æ ¸å¿ƒå…¬å¼**ï¼š
$$\hat{y}_{t+h} = l_t \cdot s_{t+h-m} \cdot \text{RNN}(x_t)$$

å…¶ä¸­ $l_t$ æ˜¯æ°´å¹³ï¼Œ$s_{t+h-m}$ æ˜¯å­£èŠ‚æ€§ç³»æ•°ã€‚

**å¯¹ Time-LLM çš„å¯ç¤º**ï¼šå¯ä»¥å°† ES çš„åˆ†è§£å…¬å¼é›†æˆåˆ° Time-LLM çš„å‰å¤„ç†ä¸­ã€‚

### 4.4 ERFï¼šæ®‹å·®é¢„æµ‹çš„é›†æˆæ–¹æ³•

[Ensemble Method for Residual Forecast (ERF)](https://www.sciencedirect.com/science/article/abs/pii/S0020025523011994) æå‡ºï¼š

**ä¸‰é˜¶æ®µæ–¹æ³•**ï¼š
1. çº¿æ€§ç»Ÿè®¡æ¨¡å‹å»ºæ¨¡ + è®¡ç®—æ®‹å·®
2. ML é›†æˆæ¨¡å‹é¢„æµ‹æ®‹å·®
3. ç®€å•æ±‚å’Œç»„åˆé¢„æµ‹

**å…³é”®å‘ç°**ï¼š
> "æ··åˆç³»ç»Ÿåˆ†åˆ«å»ºæ¨¡çº¿æ€§å’Œéçº¿æ€§æ¨¡å¼ï¼Œæ—¨åœ¨å…‹æœä»…ä½¿ç”¨å•ä¸€æ¨¡å‹çš„å±€é™æ€§ã€‚"

---

## äº”ã€å¯è¡Œæ–¹æ¡ˆæ•´ç†ä¸æ¨è

### 5.1 æ–¹æ¡ˆå¯è¡Œæ€§è¯„ä¼°

| æ–¹æ¡ˆ | åˆ›æ–°æ€§ | å®ç°éš¾åº¦ | é¢„æœŸæ”¶ç›Š | å¯è§£é‡Šæ€§ | æ¨èæŒ‡æ•° |
|------|--------|----------|----------|----------|----------|
| **A. æ®‹å·®å­¦ä¹ æ¶æ„** | â­â­â­ | â­â­ | é«˜ (10-15% MSEâ†“) | é«˜ | â­â­â­â­â­ |
| **B. åˆ†æ®µè‡ªé€‚åº”èåˆ** | â­â­â­â­ | â­â­â­ | ä¸­é«˜ (8-12% MSEâ†“) | ä¸­ | â­â­â­â­ |
| **C. çŸ¥è¯†è’¸é¦** | â­â­â­â­â­ | â­â­â­ | ä¸­ (5-10% MSEâ†“) | ä½ | â­â­â­ |
| **A+B ç»„åˆ** | â­â­â­â­â­ | â­â­â­â­ | æé«˜ (15-20% MSEâ†“) | é«˜ | â­â­â­â­â­ |

### 5.2 æ¨èæ–¹æ¡ˆï¼šæ®‹å·®å­¦ä¹  + åˆ†æ®µè‡ªé€‚åº” (A+B ç»„åˆ)

```
åŸå§‹åºåˆ— y_t
    â”‚
    â–¼
[ä¼ ç»Ÿæ¨¡å‹ (ARIMA/ES)]
    â”‚
    â”œâ”€â”€â†’ çº¿æ€§é¢„æµ‹ Å·_linear
    â”‚
    â””â”€â”€â†’ æ®‹å·® r_t = y_t - Å·_linear
              â”‚
              â–¼
         [åˆ†æ®µåˆ†æå™¨]
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼         â–¼
 Seg1      Seg2      Seg3
 (å‘¨æœŸå¼º)  (å¤æ‚)   (è¶‹åŠ¿å¼º)
    â”‚         â”‚         â”‚
    â–¼         â–¼         â–¼
 æƒé‡0.3   æƒé‡0.9   æƒé‡0.4  â† åŠ¨æ€å†³å®š Time-LLM è´¡çŒ®
    â”‚         â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
         [Time-LLM]
              â”‚
              â–¼
         éçº¿æ€§é¢„æµ‹ Å·_nonlinear (åŠ æƒ)
              â”‚
              â–¼
æœ€ç»ˆé¢„æµ‹ = Å·_linear + Î£(w_i Ã— Å·_nonlinear_i)
```

### 5.3 å®Œæ•´å®ç°ä»£ç 

```python
class AdvancedHybridForecaster(nn.Module):
    """
    é«˜çº§æ··åˆé¢„æµ‹å™¨ï¼šæ®‹å·®å­¦ä¹  + åˆ†æ®µè‡ªé€‚åº”èåˆ

    ç»“åˆäº†ï¼š
    1. ä¼ ç»Ÿæ¨¡å‹çš„çº¿æ€§æˆåˆ†æ•è·èƒ½åŠ›
    2. Time-LLM çš„éçº¿æ€§æ¨¡å¼å­¦ä¹ èƒ½åŠ›
    3. åˆ†æ®µè‡ªé€‚åº”çš„åŠ¨æ€æƒé‡åˆ†é…
    """

    def __init__(self, configs):
        super().__init__()
        self.pred_len = configs.pred_len
        self.seq_len = configs.seq_len
        self.n_vars = configs.enc_in
        self.num_segments = getattr(configs, 'num_segments', 4)

        # åˆ†æ®µæ¨¡å¼åˆ†æå™¨
        segment_len = self.seq_len // self.num_segments
        self.segment_analyzer = nn.Sequential(
            nn.Linear(segment_len, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Linear(64, 32),
            nn.LayerNorm(32),
            nn.GELU(),
            nn.Linear(32, 3)  # [å‘¨æœŸæ€§, è¶‹åŠ¿æ€§, å¤æ‚æ€§]
        )

        # æƒé‡ç”Ÿæˆå™¨
        self.weight_generator = nn.Sequential(
            nn.Linear(3, 16),
            nn.GELU(),
            nn.Linear(16, 1),
            nn.Sigmoid()  # Time-LLM æƒé‡ [0, 1]
        )

        # å¯å­¦ä¹ çš„ä¼ ç»Ÿæ¨¡å‹ç½®ä¿¡åº¦
        self.traditional_confidence = nn.Parameter(torch.tensor(0.5))

    def get_traditional_prediction(self, x_enc):
        """è·å–ä¼ ç»Ÿæ¨¡å‹çš„é¢„æµ‹å’Œæ®‹å·®"""
        B, T, N = x_enc.shape
        device = x_enc.device

        linear_preds = []
        residuals_list = []

        x_np = x_enc.detach().cpu().numpy()

        for b in range(B):
            batch_preds = []
            batch_residuals = []

            for n in range(N):
                series = x_np[b, :, n]

                try:
                    # å°è¯• ARIMA
                    from statsmodels.tsa.arima.model import ARIMA
                    model = ARIMA(series, order=(2, 1, 1))
                    fitted = model.fit()

                    pred = fitted.forecast(self.pred_len)
                    residual = series - fitted.fittedvalues

                except:
                    # å›é€€åˆ°æŒ‡æ•°å¹³æ»‘
                    from statsmodels.tsa.holtwinters import ExponentialSmoothing
                    model = ExponentialSmoothing(
                        series,
                        trend='add',
                        seasonal=None
                    )
                    fitted = model.fit()

                    pred = fitted.forecast(self.pred_len)
                    residual = series - fitted.fittedvalues

                batch_preds.append(pred)
                batch_residuals.append(residual)

            linear_preds.append(np.stack(batch_preds, axis=-1))
            residuals_list.append(np.stack(batch_residuals, axis=-1))

        linear_pred = torch.tensor(
            np.stack(linear_preds, axis=0),
            dtype=torch.float32,
            device=device
        )
        residuals = torch.tensor(
            np.stack(residuals_list, axis=0),
            dtype=torch.float32,
            device=device
        )

        return linear_pred, residuals

    def analyze_segments(self, x_enc):
        """åˆ†ææ¯ä¸ªæ®µçš„æ¨¡å¼ç‰¹å¾ï¼Œç”Ÿæˆ Time-LLM æƒé‡"""
        B, T, N = x_enc.shape
        segment_len = T // self.num_segments

        all_weights = []

        for i in range(self.num_segments):
            start = i * segment_len
            end = start + segment_len
            segment = x_enc[:, start:end, :]  # [B, seg_len, N]

            # å¯¹æ¯ä¸ªå˜é‡åˆ†æ
            seg_weights = []
            for n in range(N):
                seg_data = segment[:, :, n]  # [B, seg_len]
                pattern_scores = self.segment_analyzer(seg_data)  # [B, 3]
                weight = self.weight_generator(pattern_scores)  # [B, 1]
                seg_weights.append(weight)

            seg_weights = torch.stack(seg_weights, dim=-1)  # [B, 1, N]
            all_weights.append(seg_weights)

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ®µçš„æƒé‡ï¼ˆæœ€æ¥è¿‘é¢„æµ‹çª—å£ï¼‰
        # æˆ–è€…å¯ä»¥åŠ æƒå¹³å‡
        final_weights = all_weights[-1].squeeze(1)  # [B, N]

        return final_weights

    def forward(self, x_enc, timellm_model, x_mark_enc=None, x_dec=None, x_mark_dec=None):
        """
        æ··åˆé¢„æµ‹å‰å‘ä¼ æ’­

        Args:
            x_enc: [B, T, N] è¾“å…¥åºåˆ—
            timellm_model: Time-LLM æ¨¡å‹å®ä¾‹

        Returns:
            final_pred: [B, pred_len, N] æœ€ç»ˆé¢„æµ‹
            report: dict å¯è§£é‡Šæ€§æŠ¥å‘Š
        """
        B, T, N = x_enc.shape

        # Step 1: ä¼ ç»Ÿæ¨¡å‹é¢„æµ‹ + æ®‹å·®è®¡ç®—
        linear_pred, residuals = self.get_traditional_prediction(x_enc)

        # Step 2: åˆ†æ®µåˆ†æï¼Œç”Ÿæˆæƒé‡
        segment_weights = self.analyze_segments(x_enc)  # [B, N]

        # Step 3: Time-LLM é¢„æµ‹ï¼ˆå¯ä»¥åŸºäºåŸå§‹æ•°æ®æˆ–æ®‹å·®ï¼‰
        # æ–¹å¼ A: åŸºäºæ®‹å·®
        timellm_pred = timellm_model(residuals, x_mark_enc, x_dec, x_mark_dec)
        timellm_pred = timellm_pred[:, -self.pred_len:, :]

        # Step 4: è‡ªé€‚åº”èåˆ
        # Time-LLM æƒé‡
        w_llm = segment_weights.unsqueeze(1)  # [B, 1, N]
        # ä¼ ç»Ÿæ¨¡å‹æƒé‡
        w_trad = 1 - w_llm

        # è°ƒæ•´æƒé‡ï¼ˆä¼ ç»Ÿæ¨¡å‹ç½®ä¿¡åº¦ï¼‰
        conf = torch.sigmoid(self.traditional_confidence)
        w_trad = w_trad * conf
        w_llm = w_llm * (1 - conf) + conf

        # å½’ä¸€åŒ–
        w_sum = w_trad + w_llm
        w_trad = w_trad / w_sum
        w_llm = w_llm / w_sum

        # æœ€ç»ˆé¢„æµ‹ = çº¿æ€§é¢„æµ‹ + åŠ æƒéçº¿æ€§é¢„æµ‹
        final_pred = linear_pred + w_llm * timellm_pred

        # ç”Ÿæˆå¯è§£é‡Šæ€§æŠ¥å‘Š
        report = {
            'linear_component': linear_pred,
            'nonlinear_component': timellm_pred,
            'traditional_weight': w_trad.mean().item(),
            'timellm_weight': w_llm.mean().item(),
            'segment_weights': segment_weights.detach().cpu().numpy(),
            'traditional_confidence': conf.item()
        }

        return final_pred, report
```

---

## å…­ã€å®æ–½è·¯å¾„å»ºè®®

### 6.1 é˜¶æ®µæ€§å®æ–½è®¡åˆ’

#### é˜¶æ®µä¸€ï¼šåŸºç¡€æ®‹å·®å­¦ä¹  (1-2å‘¨)

1. å®ç° `ResidualHybridForecaster` åŸºç¡€ç‰ˆæœ¬
2. é›†æˆ ARIMA å’ŒæŒ‡æ•°å¹³æ»‘
3. åœ¨ ETTh1 ä¸ŠéªŒè¯åŸºæœ¬åŠŸèƒ½

**é¢„æœŸæ”¶ç›Š**ï¼šMSE é™ä½ 8-12%

#### é˜¶æ®µäºŒï¼šåˆ†æ®µè‡ªé€‚åº”èåˆ (2-3å‘¨)

1. å®ç° `SegmentAdaptiveFusion` æ¨¡å—
2. æ·»åŠ æ¨¡å¼æ£€æµ‹å™¨
3. ä¸é˜¶æ®µä¸€ç»“åˆ

**é¢„æœŸæ”¶ç›Š**ï¼šMSE é¢å¤–é™ä½ 3-5%

#### é˜¶æ®µä¸‰ï¼šçŸ¥è¯†è’¸é¦å¢å¼º (2-3å‘¨)

1. å®ç°è½¯æ ‡ç­¾è’¸é¦
2. æ·»åŠ åˆ†è§£ç‰¹å¾è’¸é¦
3. ä¼˜åŒ–æŸå¤±å‡½æ•°æƒé‡

**é¢„æœŸæ”¶ç›Š**ï¼šæ”¶æ•›é€Ÿåº¦æå‡ 20-30%

### 6.2 å®éªŒè®¾è®¡

```bash
# å®éªŒ 1: åŸºç¡€å¯¹æ¯”
python run_main.py --model TimeLLM --data ETTh1 --pred_len 96
python run_main.py --model TimeLLM --data ETTh1 --pred_len 96 --use_hybrid_residual

# å®éªŒ 2: åˆ†æ®µèåˆ
python run_main.py --model TimeLLM --data ETTh1 --pred_len 96 --use_segment_adaptive

# å®éªŒ 3: å®Œæ•´æ–¹æ¡ˆ
python run_main.py --model TimeLLM --data ETTh1 --pred_len 96 \
  --use_hybrid_residual --use_segment_adaptive --use_distillation
```

### 6.3 é¢„æœŸç»“æœ

| æ–¹æ³• | ETTh1 MSE | ETTh1 MAE | å¯è§£é‡Šæ€§ |
|------|-----------|-----------|----------|
| Time-LLM (åŸºçº¿) | 0.375 | 0.400 | ä½ |
| + æ®‹å·®å­¦ä¹  | 0.330 | 0.365 | é«˜ |
| + åˆ†æ®µèåˆ | 0.315 | 0.350 | é«˜ |
| + çŸ¥è¯†è’¸é¦ | 0.305 | 0.340 | é«˜ |

---

## å‚è€ƒæ–‡çŒ®

1. [ES-RNN: M4 Competition Winner](https://www.uber.com/blog/m4-forecasting-competition/) - Uber Engineering Blog
2. [N-BEATS: Neural Basis Expansion Analysis](https://arxiv.org/abs/1905.10437) - ICLR 2020
3. [Temporal Fusion Transformer](https://arxiv.org/abs/1912.09363) - arXiv 2019
4. [Hybrid ARIMA-LSTM Residual Learning](https://link.springer.com/article/10.1007/s44196-025-00930-4) - 2025
5. [Ensemble Method for Residual Forecast](https://www.sciencedirect.com/science/article/abs/pii/S0020025523011994) - Information Sciences 2024
6. [Distillation Enhanced Time Series Forecasting](https://arxiv.org/html/2401.17802v1) - arXiv 2024
7. [Adaptive Multi-Scale Decomposition Framework](https://arxiv.org/html/2406.03751v1) - arXiv 2024
8. [STL Decomposition with Hybrid Models](https://arxiv.org/abs/2510.23668) - arXiv 2025

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**ç”Ÿæˆæ—¥æœŸ**: 2026-01-11
**ä½œè€…**: Claude Code Analysis

