# Time-LLM é¡¹ç›®æ–‡æ¡£ä½“ç³»æ€»è§ˆ (Project Documentation Overview)

## ğŸ é¡¹ç›®é‡Œç¨‹ç¢‘æ€»ç»“ (Executive Summary)

æˆªè‡³ **2024-12-05**ï¼Œé’ˆå¯¹ **Time-LLM (ICLR'24)** é¡¹ç›®çš„æ·±åº¦è§£æä¸ç¯å¢ƒé€‚é…å·¥ä½œå·²å…¨éƒ¨å®Œæˆã€‚æœ¬é¡¹ç›®ä¸ä»…å®Œæˆäº†ä»£ç çš„è·‘é€šï¼Œæ›´å®ç°äº†ä» **GPT-2** åˆ° **Qwen 2.5 3B** çš„å‡çº§ï¼Œå¹¶é€šè¿‡ **4-bit é‡åŒ–** æŠ€æœ¯ä½¿å…¶åœ¨ 6GB æ˜¾å­˜ä¸‹ç¨³å®šè¿è¡Œã€‚

æˆ‘ä»¬æˆåŠŸå®ç°äº†ä» **"æ€ä¹ˆè·‘ (How to Run)"** åˆ° **"æ€ä¹ˆæ‡‚ (How it Works)"** çš„è·¨è¶Šï¼Œå¹¶è§£å†³äº†ä»¥ä¸‹å…³é”®å·¥ç¨‹æŒ‘æˆ˜ï¼š

1. **ç¡¬ä»¶é€‚é…ï¼š** é€šè¿‡ 4-bit NF4 é‡åŒ–ï¼ŒæˆåŠŸåœ¨ 6GB æ˜¾å­˜ä¸‹è¿è¡Œ Qwen 2.5 3B æ¨¡å‹
2. **ç¯å¢ƒä¿®å¤ï¼š** è§£å†³äº† Windows ä¸‹çš„ GBK/UTF-8 ç¼–ç å†²çªä¸ç»ˆç«¯ä¹±ç é—®é¢˜
3. **çŸ¥è¯†æ²‰æ·€ï¼š** å°†éšæ€§çš„ä»£ç é€»è¾‘æ˜¾æ€§åŒ–ï¼Œè¾“å‡ºäº†å¯è§†åŒ–çš„æ•°æ®æµå’Œæ¶æ„å›¾

---

## ğŸ“‚ æ–‡æ¡£äº¤ä»˜ç‰©æ¸…å•

| æ–‡ä»¶å | ç±»å‹ | æ ¸å¿ƒä½œç”¨ | é€‚ç”¨åœºæ™¯ |
| :--- | :--- | :--- | :--- |
| **`work1.md`** | **å®æ“æŒ‡å—** | **è§£å†³"æ€ä¹ˆè·‘"**ã€‚Qwen 2.5 3B 4-bit é…ç½®ã€è¿è¡Œå‘½ä»¤ã€æ•…éšœæ’é™¤ã€‚ | æ–°æ‰‹å…¥é—¨ã€ç¯å¢ƒéƒ¨ç½²ã€å¤ç°ä»£ç  |
| **`work2.md`** | **æŠ€æœ¯ç™½çš®ä¹¦** | **è§£å†³"æ€ä¹ˆæ‡‚"**ã€‚æ¶æ„æ‹“æ‰‘ã€Patching/Reprogramming æœºåˆ¶è§£æã€å‚æ•°é‡åˆ†æã€æ•°æ®æµè¿½è¸ªã€‚ | æ·±åº¦å­¦ä¹ ã€ä»£ç ç ”è¯»ã€è®ºæ–‡æ’°å†™ |
| **`CLAUDE.md`** | **é¡¹ç›®è®°å¿†åº“** | **è§£å†³"æ€ä¹ˆè®°"**ã€‚é¡¹ç›®æ ¸å¿ƒè§„åˆ™ã€å¸¸æ•°é€ŸæŸ¥ã€AI äº¤äº’è§„èŒƒã€å†æ¬¡ä¿®æ”¹è®°å½•ã€‚ | æ—¥å¸¸å¼€å‘ã€AI è¾…åŠ©ã€å¿«é€ŸæŸ¥é˜… |
| **`work.md`** | **é¡¹ç›®æ€»è§ˆ** | **è§£å†³"æ˜¯ä»€ä¹ˆ"**ã€‚å³å½“å‰æ–‡ä»¶ï¼Œä½œä¸ºæ•´ä¸ªæ–‡æ¡£ä½“ç³»çš„ç´¢å¼•ä¸æ€»ç»“ã€‚ | é¡¹ç›®å½’æ¡£ã€æ±‡æŠ¥æ€»ç»“ |

---

## âœ… è¿è¡Œå‰æœ€ç»ˆæ£€æŸ¥æ¸…å• (Pre-Flight Checklist)

### 1. æ•°æ®é›†æ£€æŸ¥
| æ–‡ä»¶ | çŠ¶æ€ | è·¯å¾„ |
|------|------|------|
| `ETTh1.csv` | âœ… | `./dataset/ETT-small/ETTh1.csv` (2.5 MB) |
| `ETTh2.csv` | âœ… | `./dataset/ETT-small/ETTh2.csv` (2.4 MB) |
| `ETTm1.csv` | âœ… | `./dataset/ETT-small/ETTm1.csv` (10.4 MB) |
| `ETTm2.csv` | âœ… | `./dataset/ETT-small/ETTm2.csv` (9.7 MB) |

### 2. æ¨¡å‹æ–‡ä»¶æ£€æŸ¥
| æ–‡ä»¶ | çŠ¶æ€ | è·¯å¾„ |
|------|------|------|
| `config.json` | âœ… | `./base_models/Qwen2.5-3B/config.json` |
| `tokenizer.json` | âœ… | `./base_models/Qwen2.5-3B/tokenizer.json` (7 MB) |
| `tokenizer_config.json` | âœ… | `./base_models/Qwen2.5-3B/tokenizer_config.json` |
| `vocab.json` | âœ… | `./base_models/Qwen2.5-3B/vocab.json` (2.8 MB) |
| `merges.txt` | âœ… | `./base_models/Qwen2.5-3B/merges.txt` (1.7 MB) |
| `model.safetensors.index.json` | âœ… | `./base_models/Qwen2.5-3B/model.safetensors.index.json` |
| `model-00001-of-00002.safetensors` | â³ ä¸‹è½½ä¸­ | `./base_models/Qwen2.5-3B/` |
| `model-00002-of-00002.safetensors` | â³ ä¸‹è½½ä¸­ | `./base_models/Qwen2.5-3B/` |

### 3. ä»£ç ä¿®æ”¹æ£€æŸ¥
| æ–‡ä»¶ | çŠ¶æ€ | ä¿®æ”¹å†…å®¹ |
|------|------|----------|
| `run_main.py` | âœ… | ç¬¬ 82-84 è¡Œï¼šæ–°å¢ `--llm_model_path`, `--load_in_4bit` å‚æ•° |
| `models/TimeLLM.py` | âœ… | ç¬¬ 43-96 è¡Œï¼šæ–°å¢é€šç”¨æ¨¡å‹åŠ è½½ + 4-bit é‡åŒ–æ”¯æŒ |

### 4. ä¾èµ–æ£€æŸ¥
```bash
pip install bitsandbytes accelerate
```

---

## ğŸš€ è¿è¡Œå‘½ä»¤

```powershell
cd e:\timellm\Time-LLM

python run_main.py ^
  --task_name long_term_forecast ^
  --is_training 1 ^
  --root_path ./dataset/ETT-small/ ^
  --data_path ETTm1.csv ^
  --model_id ETTm1_512_96 ^
  --model_comment Qwen3B ^
  --model TimeLLM ^
  --data ETTm1 ^
  --features M ^
  --seq_len 512 ^
  --label_len 48 ^
  --pred_len 96 ^
  --e_layers 2 ^
  --d_layers 1 ^
  --factor 3 ^
  --enc_in 7 ^
  --dec_in 7 ^
  --c_out 7 ^
  --batch_size 8 ^
  --d_model 32 ^
  --d_ff 32 ^
  --llm_dim 2048 ^
  --dropout 0.1 ^
  --llm_model QWEN ^
  --llm_model_path "e:\timellm\Time-LLM\base_models\Qwen2.5-3B" ^
  --load_in_4bit
```

---

## ğŸ“Š å…³é”®æ¥å£è¯´æ˜

### 1. æ•°æ®æ¥å£ (`data_provider/data_factory.py`)
```python
data_dict = {
    'ETTh1': Dataset_ETT_hour,    # å°æ—¶çº§ ETT æ•°æ®
    'ETTh2': Dataset_ETT_hour,
    'ETTm1': Dataset_ETT_minute,  # åˆ†é’Ÿçº§ ETT æ•°æ®
    'ETTm2': Dataset_ETT_minute,
    'ECL': Dataset_Custom,        # ç”µåŠ›æ•°æ®
    'Traffic': Dataset_Custom,    # äº¤é€šæ•°æ®
    'Weather': Dataset_Custom,    # å¤©æ°”æ•°æ®
    'm4': Dataset_M4,             # M4 ç«èµ›æ•°æ®
}

# ä½¿ç”¨æ–¹å¼
train_data, train_loader = data_provider(args, 'train')
vali_data, vali_loader = data_provider(args, 'val')
test_data, test_loader = data_provider(args, 'test')
```

### 2. æ¨¡å‹æ¥å£ (`models/TimeLLM.py`)

**æ–°å¢å‚æ•°æ”¯æŒï¼š**
| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `configs.llm_model_path` | str | æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ– HuggingFace ID |
| `configs.load_in_4bit` | bool | æ˜¯å¦å¯ç”¨ 4-bit é‡åŒ– |
| `configs.llm_dim` | int | LLM éšè—å±‚ç»´åº¦ (Qwen 3B = 2048) |
| `configs.llm_layers` | int | ä½¿ç”¨çš„ LLM å±‚æ•° |

**æ¨¡å‹åŠ è½½é€»è¾‘ï¼ˆç¬¬ 43-96 è¡Œï¼‰ï¼š**
```python
if configs.llm_model_path:
    # é€šç”¨åŠ è½½ï¼šæ”¯æŒä»»æ„ HuggingFace æ¨¡å‹
    self.llm_model = AutoModel.from_pretrained(
        configs.llm_model_path,
        trust_remote_code=True,
        quantization_config=BitsAndBytesConfig(...) if configs.load_in_4bit else None,
        device_map="auto" if configs.load_in_4bit else None
    )
elif configs.llm_model == 'LLAMA':
    # åŸæœ‰ LLAMA åŠ è½½é€»è¾‘
elif configs.llm_model == 'GPT2':
    # åŸæœ‰ GPT2 åŠ è½½é€»è¾‘
elif configs.llm_model == 'BERT':
    # åŸæœ‰ BERT åŠ è½½é€»è¾‘
```

---

## ğŸ“ˆ æ˜¾å­˜ä¼°ç®—

| ç»„ä»¶ | æ˜¾å­˜å ç”¨ |
|------|----------|
| Qwen 2.5 3B (4-bit é‡åŒ–) | ~1.5 GB |
| Time-LLM å¯è®­ç»ƒå‚æ•° | ~0.5 GB |
| ä¸­é—´å˜é‡ & æ¢¯åº¦ (batch_size=8) | ~2.0 GB |
| ç³»ç»Ÿå ç”¨ | ~1.0 GB |
| **æ€»è®¡** | **~5.0 GB** âœ… |

> **ç»“è®ºï¼š** 6GB æ˜¾å­˜å®Œå…¨å¤Ÿç”¨ï¼

---

## ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„

```
Time-LLM/
â”œâ”€â”€ base_models/
â”‚   â””â”€â”€ Qwen2.5-3B/              # â˜… Qwen 2.5 3B æ¨¡å‹ (4-bit é‡åŒ–)
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors
â”‚       â””â”€â”€ model-00002-of-00002.safetensors
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ ETT-small/               # â˜… è®­ç»ƒæ•°æ®
â”‚   â”‚   â”œâ”€â”€ ETTh1.csv
â”‚   â”‚   â”œâ”€â”€ ETTh2.csv
â”‚   â”‚   â”œâ”€â”€ ETTm1.csv
â”‚   â”‚   â””â”€â”€ ETTm2.csv
â”‚   â””â”€â”€ prompt_bank/             # é¢†åŸŸæè¿°
â”œâ”€â”€ models/
â”‚   â””â”€â”€ TimeLLM.py               # â˜… å·²ä¿®æ”¹ï¼šæ”¯æŒ AutoModel + 4-bit
â”œâ”€â”€ data_provider/
â”‚   â”œâ”€â”€ data_factory.py          # æ•°æ®é›†è·¯ç”±
â”‚   â””â”€â”€ data_loader.py           # æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ run_main.py                  # â˜… å·²ä¿®æ”¹ï¼šæ–°å¢å‚æ•°
â”œâ”€â”€ CLAUDE.md                    # é¡¹ç›®è®°å¿†åº“
â”œâ”€â”€ work.md                      # æœ¬æ–‡æ¡£
â”œâ”€â”€ work1.md                     # å¿«é€Ÿä¸Šæ‰‹æŒ‡å—
â””â”€â”€ work2.md                     # æ·±åº¦æŠ€æœ¯è§£æ
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç­‰å¾…æ¨¡å‹ä¸‹è½½å®Œæˆ**ï¼šç¡®è®¤ `model-00001-of-00002.safetensors` å’Œ `model-00002-of-00002.safetensors` ä¸‹è½½å®Œæ¯•
2. **è¿è¡Œè®­ç»ƒå‘½ä»¤**ï¼šæ‰§è¡Œä¸Šæ–¹ PowerShell å‘½ä»¤
3. **ç›‘æ§è®­ç»ƒ**ï¼šè§‚å¯Ÿæ§åˆ¶å°è¾“å‡ºå’Œ GPU æ˜¾å­˜ä½¿ç”¨
4. **å¦‚é‡ OOM**ï¼šå°† `--batch_size` é™è‡³ 4

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€