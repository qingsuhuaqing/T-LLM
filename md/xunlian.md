# Time-LLM 词汇表与重编程机制深度解析

> **核心问题解答** —— 词汇表(Mapping Layer)与重编程(Reprogramming Layer)的关系与区别

---

## 目录

1. [问题概览](#一问题概览)
2. [词汇表(Mapping Layer)详解](#二词汇表mapping-layer详解)
3. [重编程(Reprogramming Layer)详解](#三重编程reprogramming-layer详解)
4. [二者关系与区别](#四二者关系与区别)
5. [完整数据流示意](#五完整数据流示意)
6. [总结](#六总结)

---

## 一、问题概览

您提出的问题：
1. 词汇表起到了什么作用？
2. 是将词汇表的输出再进行重映射？
3. 词汇表是以 batch 中的 patch 作为输入的？
4. 词汇表属于嵌入过程？
5. 不同的数据类型，词汇表是如何进行嵌入的？如何包罗住所有数据类型？
6. 训练词汇表是什么过程？是指训练这个嵌入映射（可能的权重）？
7. 词汇表和重编程有什么关系？二者的区别在哪里？

---

## 二、词汇表(Mapping Layer)详解

### 2.1 词汇表起到了什么作用？

**答案**：词汇表（Mapping Layer）的作用是**将 LLM 的大型词嵌入矩阵压缩为一个小型的"虚拟词表"**，作为重编程层的 Key 和 Value。

#### 代码位置：`models/TimeLLM.py` 第 233-236 行

```python
# 获取 LLM 的词嵌入矩阵（冻结的）
self.word_embeddings = self.llm_model.get_input_embeddings().weight
# 例如 GPT-2: [50257, 768]，Qwen 2.5 3B: [151936, 2048]

self.vocab_size = self.word_embeddings.shape[0]  # 词表大小
self.num_tokens = 1000  # 压缩后的虚拟词表大小（硬编码）

# Mapping Layer: 线性层进行词表压缩
self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)
# GPT-2: Linear(50257, 1000)
# Qwen: Linear(151936, 1000)
```

#### 词汇表的三大作用：

| 作用 | 说明 |
|------|------|
| **1. 词表压缩** | 将 50257 或 151936 个词压缩为 1000 个"虚拟词"，降低计算复杂度 |
| **2. 任务适配** | 通过可学习的线性层，筛选出与时序预测任务最相关的语义原型 |
| **3. 桥梁作用** | 连接 LLM 的文本知识空间与时序数据空间 |

---

### 2.2 是将词汇表的输出再进行重映射？

**答案**：**是的，但不是直接重映射词汇表的输出**。

词汇表的输出（source_embeddings）是作为**重编程层的 Key 和 Value**，而不是直接作为最终输出。

#### 代码位置：`models/TimeLLM.py` 第 294 行

```python
# Mapping Layer 的输出
source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)
# 输入: word_embeddings [vocab_size, llm_dim] = [50257, 768]
# 转置: [768, 50257]
# Linear: [768, 50257] @ [50257, 1000] → [768, 1000]
# 再转置: [1000, 768]
# 输出: source_embeddings [num_tokens, llm_dim] = [1000, 768]
```

#### 然后送入重编程层（第 299 行）：

```python
enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)
#                                  ↑ Query    ↑ Key            ↑ Value
#                                  Patch嵌入   压缩后的词表      压缩后的词表
```

**关键理解**：
- 词汇表的输出（1000 个虚拟词嵌入）**不是直接送给 LLM**
- 而是作为"语义锚点"，让时序 Patch 通过 Cross-Attention 去"查询"这些锚点
- 最终输出的是时序 Patch 在 LLM 空间中的新表示

---

### 2.3 词汇表是以 batch 中的 patch 作为输入的？

**答案**：**不是！这是一个常见误解。**

词汇表（Mapping Layer）的输入是 **LLM 的词嵌入矩阵**，而不是 Patch。

#### 数据流对比：

```
┌────────────────────────────────────────────────────────────────────┐
│                    两条独立的数据流                                  │
├────────────────────────────────────────────────────────────────────┤
│                                                                    │
│  数据流 A: Patch 路径（Query 来源）                                 │
│  ─────────────────────────────────                                 │
│  时序数据 [B, T, N]                                                 │
│      ↓                                                             │
│  PatchEmbedding                                                    │
│      ↓                                                             │
│  enc_out [B*N, num_patches, d_model] ← 这是 Query                  │
│                                                                    │
├────────────────────────────────────────────────────────────────────┤
│                                                                    │
│  数据流 B: 词表路径（Key/Value 来源）                               │
│  ─────────────────────────────────                                 │
│  LLM 词嵌入矩阵 [vocab_size, llm_dim]  ← 冻结的，来自预训练         │
│      ↓                                                             │
│  Mapping Layer (可学习)                                            │
│      ↓                                                             │
│  source_embeddings [num_tokens, llm_dim] ← 这是 Key/Value          │
│                                                                    │
└────────────────────────────────────────────────────────────────────┘
```

**重要区别**：
- **Patch 路径**：每个 batch 的数据不同，enc_out 随输入变化
- **词表路径**：与输入数据无关！无论输入什么时序数据，source_embeddings 都是从**同一个冻结的词嵌入矩阵**计算出来的

---

### 2.4 词汇表属于嵌入过程？

**答案**：**词汇表不属于传统意义上的"嵌入"，而是"词表压缩/适配"过程。**

#### 项目中的嵌入层对比：

| 模块 | 类型 | 输入 | 输出 | 是否嵌入？ |
|------|------|------|------|-----------|
| **PatchEmbedding** | 时序嵌入 | 时序 Patch [B*N, num_patches, patch_len] | [B*N, num_patches, d_model] | ✅ 是 |
| **LLM Tokenizer + Embedding** | 文本嵌入 | Prompt 文本 | [B*N, prompt_len, llm_dim] | ✅ 是 |
| **Mapping Layer** | 词表压缩 | LLM 词嵌入矩阵 [vocab_size, llm_dim] | [num_tokens, llm_dim] | ❌ 不是 |

**Mapping Layer 的本质**：
- 它是一个**降维/压缩**操作，而不是嵌入操作
- 嵌入（Embedding）：将离散 ID 映射为连续向量
- Mapping Layer：将大矩阵压缩为小矩阵（通过线性投影）

---

### 2.5 不同的数据类型，词汇表是如何进行嵌入的？如何包罗住所有数据类型？

**答案**：**词汇表（Mapping Layer）与数据类型无关！它只处理 LLM 的词嵌入矩阵。**

这是本项目最精妙的设计之一：

#### 设计思想：

```
┌─────────────────────────────────────────────────────────────────┐
│              "通用语义锚点"设计理念                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  LLM 词嵌入矩阵包含了丰富的语义知识：                            │
│  - "increase", "decrease", "trend", "peak", "valley" ...       │
│  - 数字概念: "one", "hundred", "million" ...                   │
│  - 时间概念: "daily", "weekly", "seasonal" ...                 │
│                                                                 │
│  Mapping Layer 学习的是：                                       │
│  "从 50257 个词中，选择/组合出 1000 个对时序预测最有用的语义原型" │
│                                                                 │
│  这 1000 个虚拟词是：                                           │
│  - 与具体数据类型无关                                           │
│  - 与具体数据集无关                                             │
│  - 是"通用时序语义"的表示                                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### 如何适配不同数据类型？

| 组件 | 如何适配不同数据 |
|------|------------------|
| **PatchEmbedding** | 通过 1D 卷积，任何连续数值都可以嵌入 |
| **Instance Normalization** | 将数据归一化到 [-1, 1] 范围，消除量纲差异 |
| **Prompt** | 动态提取统计特征（min/max/median/trend），适配不同分布 |
| **Mapping Layer** | **不需要适配**！它只是词表压缩，与输入数据无关 |
| **Reprogramming Layer** | 通过 Cross-Attention 的学习，自动找到时序与语义的对应关系 |

**核心洞察**：
- 不是词汇表去"包罗"所有数据类型
- 而是时序数据通过 Reprogramming Layer 去"查询"词汇表中最相关的语义
- 这种设计使得模型具有**数据无关性**——无论是电力、天气、交通，都使用同一套虚拟词表

---

### 2.6 训练词汇表是什么过程？是指训练这个嵌入映射（可能的权重）？

**答案**：**是的，训练词汇表就是训练 Mapping Layer 的权重矩阵。**

#### 训练的参数：

```python
self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)
# 例如 GPT-2: Linear(50257, 1000)
#
# 权重矩阵: W [50257, 1000]  ← 可训练！
# 偏置向量: b [1000]         ← 可训练！
#
# 参数量: 50257 × 1000 + 1000 ≈ 50.3M
```

#### 训练过程解析：

```
┌─────────────────────────────────────────────────────────────────┐
│                    Mapping Layer 训练过程                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  输入: LLM 词嵌入矩阵 E [vocab_size, llm_dim]                   │
│        ↓                                                        │
│  转置: E^T [llm_dim, vocab_size]                               │
│        ↓                                                        │
│  线性层: E^T × W + b                                            │
│        ↓                                                        │
│  结果: [llm_dim, num_tokens]                                   │
│        ↓                                                        │
│  再转置: [num_tokens, llm_dim] = source_embeddings             │
│                                                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  梯度传播方向：                                                  │
│                                                                 │
│  预测误差 (MSE Loss)                                            │
│       ↓                                                        │
│  FlattenHead                                                   │
│       ↓                                                        │
│  LLM 输出（冻结，梯度截断）                                      │
│       ↓                                                        │
│  Reprogramming Layer                                           │
│       ↓                                                        │
│  Mapping Layer (梯度更新 W 和 b)                                │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### 训练的物理意义：

Mapping Layer 学习的是一个"软选择"矩阵：

```python
# 假设 W 的某一列 W[:, i] 学到了:
W[:, 0] = [0.5, 0, 0, 0.3, 0, ..., 0.2, 0, ...]
#          ↑      ↑        ↑
#        "上升" "趋势"   "增长"
#
# 这意味着虚拟词 0 是 "上升"、"趋势"、"增长" 等词的加权组合
# 它代表了"上升趋势"这一时序语义
```

---

## 三、重编程(Reprogramming Layer)详解

### 3.1 重编程层的作用

**重编程层的作用**：通过 Cross-Attention 机制，将时序 Patch 嵌入"翻译"到 LLM 的词嵌入空间。

#### 代码位置：`models/TimeLLM.py` 第 325-363 行

```python
class ReprogrammingLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):
        super(ReprogrammingLayer, self).__init__()

        d_keys = d_keys or (d_model // n_heads)

        # Query 投影: 来自时序 Patch
        self.query_projection = nn.Linear(d_model, d_keys * n_heads)

        # Key 投影: 来自压缩词表
        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)

        # Value 投影: 来自压缩词表
        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)

        # 输出投影: 映射回 LLM 维度
        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)
```

### 3.2 重编程的计算过程

```python
def forward(self, target_embedding, source_embedding, value_embedding):
    """
    target_embedding: Patch 嵌入 [B*N, num_patches, d_model]  ← Query
    source_embedding: 压缩词表 [num_tokens, llm_dim]          ← Key
    value_embedding:  压缩词表 [num_tokens, llm_dim]          ← Value
    """
    B, L, _ = target_embedding.shape  # B*N, num_patches
    S, _ = source_embedding.shape      # num_tokens
    H = self.n_heads

    # Query 投影: [B*N, num_patches, d_model] → [B*N, num_patches, H, d_keys]
    target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)

    # Key 投影: [num_tokens, llm_dim] → [num_tokens, H, d_keys]
    source_embedding = self.key_projection(source_embedding).view(S, H, -1)

    # Value 投影: [num_tokens, llm_dim] → [num_tokens, H, d_keys]
    value_embedding = self.value_projection(value_embedding).view(S, H, -1)

    # Cross-Attention
    out = self.reprogramming(target_embedding, source_embedding, value_embedding)
    # out: [B*N, num_patches, H, d_keys]

    out = out.reshape(B, L, -1)  # [B*N, num_patches, H*d_keys]

    return self.out_projection(out)  # [B*N, num_patches, llm_dim]
```

### 3.3 Cross-Attention 的物理意义

```
┌─────────────────────────────────────────────────────────────────┐
│              Cross-Attention 物理意义                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Query (时序 Patch):                                            │
│  "我是一个时序片段，我想知道我在语义空间中代表什么意思"           │
│                                                                 │
│  Key (压缩词表):                                                 │
│  "我们是 1000 个语义原型，代表各种时序相关的概念"                │
│  - 虚拟词 0: 代表 "上升趋势"                                    │
│  - 虚拟词 1: 代表 "周期波动"                                    │
│  - 虚拟词 2: 代表 "平稳状态"                                    │
│  - ...                                                          │
│                                                                 │
│  Attention 权重:                                                │
│  "计算时序 Patch 与每个语义原型的相似度"                        │
│  - Patch 1 与 "上升趋势" 相似度 = 0.8                          │
│  - Patch 1 与 "周期波动" 相似度 = 0.1                          │
│  - Patch 1 与 "平稳状态" 相似度 = 0.1                          │
│                                                                 │
│  Value (压缩词表):                                               │
│  "用注意力权重加权聚合语义原型的表示"                           │
│  Patch 1 的新表示 = 0.8 × V["上升趋势"] + 0.1 × V["周期波动"] + ...│
│                                                                 │
│  输出:                                                          │
│  时序 Patch 在 LLM 语义空间中的表示                             │
│  - 具有 llm_dim 维度                                            │
│  - 可以直接送入冻结的 LLM 进行处理                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 四、二者关系与区别

### 4.1 关系：串联的两个阶段

```
┌──────────────────────────────────────────────────────────────────────────┐
│                    Mapping Layer 与 Reprogramming Layer 的关系            │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   阶段 1: Mapping Layer                                                  │
│   ─────────────────────                                                  │
│   输入: LLM 词嵌入矩阵 [vocab_size, llm_dim]                             │
│   输出: 压缩词表 [num_tokens, llm_dim]                                   │
│   作用: 词表压缩，生成"语义锚点"                                         │
│                                                                          │
│                         ↓ (输出作为 Key/Value)                           │
│                                                                          │
│   阶段 2: Reprogramming Layer                                            │
│   ───────────────────────                                                │
│   输入:                                                                  │
│     - Query: Patch 嵌入 [B*N, num_patches, d_model]                      │
│     - Key/Value: 压缩词表 [num_tokens, llm_dim]  ← 来自 Mapping Layer    │
│   输出: 重编程后的 Patch [B*N, num_patches, llm_dim]                     │
│   作用: 跨模态对齐，将时序"翻译"到 LLM 空间                              │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### 4.2 区别对比表

| 维度 | Mapping Layer (词汇表) | Reprogramming Layer (重编程) |
|------|----------------------|---------------------------|
| **输入** | LLM 词嵌入矩阵 [vocab_size, llm_dim] | Patch 嵌入 + 压缩词表 |
| **输出** | 压缩词表 [num_tokens, llm_dim] | 重编程后的 Patch [B*N, L, llm_dim] |
| **操作类型** | 线性投影（降维） | Cross-Attention |
| **与 batch 的关系** | **无关** —— 输出固定 | **有关** —— 每个 batch 输出不同 |
| **可训练参数** | ~50M (主要参数量) | ~6M |
| **物理意义** | 筛选/组合出有用的语义原型 | 将时序映射到语义空间 |
| **类比** | 编译字典（从大词典中选出常用词） | 翻译过程（用字典将时序翻译为语义） |

### 4.3 形象类比

```
┌─────────────────────────────────────────────────────────────────┐
│                        翻译员类比                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  假设您要将"时序语言"翻译成"文本语言"：                          │
│                                                                 │
│  1. Mapping Layer = 编写《时序-文本》词典                       │
│     ──────────────────────────────────                          │
│     - 从 50257 个英文词中选出 1000 个对时序最有用的词            │
│     - "上升"、"下降"、"周期"、"趋势"、"峰值"...                 │
│     - 这本词典只需编写一次，适用于所有翻译任务                   │
│                                                                 │
│  2. Reprogramming Layer = 查词典进行翻译                        │
│     ──────────────────────────────────                          │
│     - 拿到一个时序 Patch（原文）                                │
│     - 查词典，找到最相关的词（注意力权重）                       │
│     - 组合这些词的意思，得到翻译结果                            │
│     - 每个 Patch 都需要单独翻译                                 │
│                                                                 │
│  3. 关系:                                                       │
│     - 没有词典（Mapping Layer），就无法翻译                     │
│     - 没有翻译过程（Reprogramming Layer），词典就没用           │
│     - 二者缺一不可                                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 五、完整数据流示意

### 5.1 全景数据流图

```
┌──────────────────────────────────────────────────────────────────────────────────┐
│                           Time-LLM 完整数据流                                     │
├──────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │ 输入: x_enc [B, seq_len, N_vars] = [4, 512, 7]                          │    │
│  └───────────────────────────────────┬─────────────────────────────────────┘    │
│                                      │                                          │
│                                      ▼                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │ 1. Instance Normalization: 实例归一化                                    │    │
│  │    x_enc = (x_enc - mean) / std                                         │    │
│  └───────────────────────────────────┬─────────────────────────────────────┘    │
│                                      │                                          │
│          ┌───────────────────────────┼───────────────────────────┐              │
│          │                           │                           │              │
│          ▼                           ▼                           ▼              │
│  ┌───────────────────┐    ┌─────────────────────┐    ┌─────────────────────┐   │
│  │ 2. Prompt 构建     │    │ 3. PatchEmbedding   │    │ 4. Mapping Layer    │   │
│  │                    │    │                     │    │                     │   │
│  │ 提取统计特征:      │    │ 时序 → Patch:       │    │ 词表压缩:           │   │
│  │ - min, max, median │    │ [4,7,512]           │    │ [vocab_size,llm_dim]│   │
│  │ - trend, lags      │    │    ↓                │    │    ↓                │   │
│  │                    │    │ [28,64,16]          │    │ [1000, llm_dim]     │   │
│  │ Tokenizer:         │    │    ↓                │    │                     │   │
│  │ text → token IDs   │    │ Conv1d 嵌入         │    │ 与输入数据无关！    │   │
│  │    ↓               │    │    ↓                │    │                     │   │
│  │ LLM Embedding:     │    │ [28,64,d_model]     │    │                     │   │
│  │ IDs → vectors      │    │ = [28,64,32]        │    │                     │   │
│  │    ↓               │    │                     │    │                     │   │
│  │ [28,~128,llm_dim]  │    │  Query 来源         │    │  Key/Value 来源     │   │
│  └─────────┬──────────┘    └─────────┬───────────┘    └─────────┬───────────┘   │
│            │                         │                           │              │
│            │                         └─────────────┬─────────────┘              │
│            │                                       │                            │
│            │                                       ▼                            │
│            │               ┌─────────────────────────────────────────────┐      │
│            │               │ 5. Reprogramming Layer (Cross-Attention)    │      │
│            │               │                                             │      │
│            │               │ Q: Patch 嵌入 [28, 64, 32]                  │      │
│            │               │ K: 压缩词表  [1000, llm_dim]               │      │
│            │               │ V: 压缩词表  [1000, llm_dim]               │      │
│            │               │                                             │      │
│            │               │ Attention = softmax(Q × K^T / √d) × V       │      │
│            │               │                                             │      │
│            │               │ 输出: [28, 64, llm_dim]                     │      │
│            │               └─────────────────────┬───────────────────────┘      │
│            │                                     │                              │
│            └──────────────────┬──────────────────┘                              │
│                               │                                                  │
│                               ▼                                                  │
│            ┌─────────────────────────────────────────────────────┐              │
│            │ 6. Concat: Prompt + Reprogrammed Patches            │              │
│            │    [28, ~128+64, llm_dim] = [28, ~192, llm_dim]     │              │
│            └─────────────────────────┬───────────────────────────┘              │
│                                      │                                          │
│                                      ▼                                          │
│            ┌─────────────────────────────────────────────────────┐              │
│            │ 7. Frozen LLM Forward                                │              │
│            │    输入: [28, ~192, llm_dim]                         │              │
│            │    输出: [28, ~192, llm_dim]                         │              │
│            └─────────────────────────┬───────────────────────────┘              │
│                                      │                                          │
│                                      ▼                                          │
│            ┌─────────────────────────────────────────────────────┐              │
│            │ 8. FlattenHead                                       │              │
│            │    截取 Patch 部分 + Flatten + Linear                │              │
│            │    [4, 7, d_ff, 64] → [4, 7, pred_len] → [4, 96, 7] │              │
│            └─────────────────────────┬───────────────────────────┘              │
│                                      │                                          │
│                                      ▼                                          │
│            ┌─────────────────────────────────────────────────────┐              │
│            │ 9. Denormalization: 反归一化                         │              │
│            │    dec_out = dec_out * std + mean                    │              │
│            └─────────────────────────┬───────────────────────────┘              │
│                                      │                                          │
│                                      ▼                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │ 输出: dec_out [B, pred_len, N_vars] = [4, 96, 7]                        │    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
└──────────────────────────────────────────────────────────────────────────────────┘
```

### 5.2 可训练参数位置标注

```
┌────────────────────────────────────────────────────────────────┐
│                    可训练 vs 冻结                               │
├────────────────────────────────────────────────────────────────┤
│                                                                │
│  ❄️ 冻结部分 (约 124M-3B 参数):                                 │
│     - LLM Backbone (GPT-2/Qwen/LLAMA)                         │
│     - LLM 词嵌入矩阵 (word_embeddings)                         │
│                                                                │
│  🔥 可训练部分 (约 56-158M 参数):                               │
│     1. PatchEmbedding: Conv1d           ~1.5K                  │
│     2. Mapping Layer: Linear            ~50-152M  ← 主要参数   │
│     3. Reprogramming Layer: 4个Linear   ~6M                    │
│     4. FlattenHead: Linear              ~200K                  │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

---

## 六、总结

### 6.1 问题回答总结

| 问题 | 答案 |
|------|------|
| **词汇表起到什么作用？** | 将 LLM 大词表(50K+)压缩为 1000 个"虚拟词"，作为跨模态对齐的语义锚点 |
| **是将词汇表的输出再进行重映射？** | 是的，词汇表的输出作为 Reprogramming Layer 的 Key/Value |
| **词汇表是以 batch 中的 patch 作为输入的？** | **不是！** 词汇表的输入是 LLM 的词嵌入矩阵（固定的），与 batch 数据无关 |
| **词汇表属于嵌入过程？** | **不属于。** 它是词表压缩/适配过程，而非嵌入（离散→连续）过程 |
| **如何包罗住所有数据类型？** | 词汇表与数据类型无关，它只处理 LLM 词嵌入；适配不同数据靠 PatchEmbedding 和 Normalization |
| **训练词汇表是什么过程？** | 训练 Mapping Layer 的权重矩阵 W，学习如何从大词表中"软选择"出有用的语义原型 |
| **词汇表和重编程的关系？** | 串联关系：词汇表生成语义锚点 → 重编程用这些锚点对齐时序 |
| **二者的区别？** | 词汇表是静态的词表压缩；重编程是动态的跨模态对齐 |

### 6.2 关键洞察

1. **Mapping Layer 是"词典编写者"**：从大词表中筛选/组合出对时序任务最有用的 1000 个语义原型

2. **Reprogramming Layer 是"翻译员"**：用这本词典，将每个时序 Patch 翻译成 LLM 能理解的语义表示

3. **设计精髓**：通过这种"词典+翻译"的双层设计，实现了：
   - **参数高效**：无需微调 LLM，只训练适配层
   - **数据无关**：同一套虚拟词表适用于所有时序数据集
   - **知识迁移**：利用 LLM 预训练的语义理解能力

---

**文档生成时间**: 2026-02-02
**适用版本**: Time-LLM v1.0 (基于 ICLR'24 论文实现)
**作者**: Claude Code Technical Analysis
