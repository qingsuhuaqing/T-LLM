#!/bin/bash

# ========================================================================
# Time-LLM Training Script for ETTh1 Dataset (Optimized for Speed)
# ========================================================================
# 模型: Qwen 2.5 3B + 4-bit NF4 量化
# 硬件: 6GB VRAM GPU
# 数据集: ETTh1 (1小时级别, 7变量) - 比 ETTm1 小 4 倍
# 任务: 长期预测 (512步输入 → 96步预测)
# 目标训练时间: ~2.9 小时 (3 epochs)
# ========================================================================

# 切换到项目目录 (WSL 路径)
cd /mnt/e/timellm-chuangxin/Time-LLM

# 设置显存优化
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:64"

# ========================================================================
# 时间估算 (ETTh1 数据集)
# ========================================================================
# ETTh1 总行数: ~17,420
# 训练集: ~70% = ~12,194 行
# batch_size=8: ~1,524 迭代/epoch
# 训练速度: ~1.15 秒/迭代
# 每个 epoch: ~1,524 × 1.15 = ~29 分钟
# 6 个 epoch: ~174 分钟 = ~2.9 小时 ✅
# ========================================================================

python run_main.py \
  `# 基础配置` \
  --task_name long_term_forecast \
  --is_training 1 \
  --model_id ETTh1_512_96 \
  --model_comment Qwen3B \
  --model TimeLLM \
  \
  `# 数据配置 (使用 ETTh1 - 小时级数据)` \
  --data ETTh1 \
  --root_path ./dataset/ETT-small/ \
  --data_path ETTh1.csv \
  --features M \
  \
  `# 时序长度配置` \
  --seq_len 512 \
  --label_len 48 \
  --pred_len 96 \
  \
  `# 模型结构配置` \
  --enc_in 7 \
  --dec_in 7 \
  --c_out 7 \
  --d_model 32 \
  --n_heads 8 \
  --e_layers 2 \
  --d_layers 1 \
  --d_ff 32 \
  --factor 3 \
  --dropout 0.1 \
  \
  `# LLM配置` \
  --llm_model QWEN \
  --llm_dim 2048 \
  --llm_model_path /mnt/e/timellm-chuangxin/Time-LLM/base_models/Qwen2.5-3B \
  --llm_layers 6 \
  --load_in_4bit \
  \
  `# 训练优化配置 (优化后参数)` \
  --batch_size 8 \
  --num_workers 2 \
  --train_epochs 6 \
  --itr 1 \
  --prompt_domain 1

# ========================================================================
# 显存占用估算 (与 ETTm1 相同)
# ========================================================================
# Qwen 2.5 3B (4-bit, 6层):  ~1.5 GB
# Time-LLM 可训练参数:       ~0.5 GB
# 中间变量 (batch_size=8):   ~2.0 GB
# 梯度缓存:                  ~0.5 GB
# 系统占用:                  ~0.5 GB
# ----------------------------------------
# 总计:                      ~5.0 GB ✅
# ========================================================================

# ========================================================================
# 生成的 Checkpoint 路径
# ========================================================================
# checkpoints/long_term_forecast_ETTh1_512_96_TimeLLM_ETTh1_ftM_sl512_ll48_pl96_dm32_nh8_el2_dl1_df32_fc3_ebtimeF_test_0-Qwen3B/checkpoint
# ========================================================================

# ========================================================================
# 为什么选择 ETTh1？
# ========================================================================
# 1. 数据量对比:
#    - ETTm1: 69,680 行 (15分钟级) → ~12,000 迭代/epoch → ~3.8小时/epoch
#    - ETTh1: 17,420 行 (1小时级)  → ~1,524 迭代/epoch → ~29分钟/epoch
#
# 2. 训练时间对比 (6 epochs):
#    - ETTm1: ~38 小时
#    - ETTh1: ~2.9 小时 ✅
#
# 3. 性能影响:
#    - 数据粒度不同，但模型架构相同
#    - 适合快速验证模型是否正常工作
#    - 训练完成后可切换回 ETTm1 进行完整训练
# ========================================================================

# ========================================================================
# 常见问题与解决
# ========================================================================
# 1. 如果仍然 OOM (显存不足):
#    - 降低 --batch_size 为 4
#    - 降低 --seq_len 为 256
#
# 2. 如果想进一步加速:
#    - 减少 --train_epochs 为 3 (~1.5 小时)
#
# 3. 训练完成后想用 ETTm1:
#    - 运行 ./scripts/TimeLLM_ETTm1_2.sh
#    - 增加 --train_epochs 为 20-50（需要更长时间）
# ========================================================================
