通过"重编程"(Reprogramming) 技术，(权重冻结,只进行可学习的轻量级层)
将预训练的大语言模型 (LLM) 适配为时间序列预测模型，
无需从头训练即可利用 LLM 的模式识别能力。
传统的时序预测过于薄弱,无法处理长文本和复杂多模态,现有的大模型十分适用于nlp技术的处理,
我们将输入的时序信息映射为相应的文本信息,再利用llm的能力,进行相应领域的相关预测,可以极大的扩展我们预测的准确性,模型的适配性,与此同时我们涉及到垂直领域与大预言模型的融合,有助于通用大语言模型(agi)的发展,也为具身智能在确切场景下的应用提供了相关的可能性.
time-llm在论文中提到了两种优势:
1.输入重编程 (Input Reprogramming):它不直接把数字 0.5, 0.8, 0.9 扔给 LLM。它使用了一个Patching（分块）和 Projection（投影）层。它把时间序列切片，通过一个小的线性层（Linear Layer），把这些数字映射到 LLM 的 词向量空间 (Word Embedding Space)。目的:使LLM认为这些时间序列数据是某种"特殊的文本特征。
2.提示重编程 (Prompt Reprogramming):它会在输入前加上特定的自然语言提示（Prompt），比如：“作为一名专家，请预测未来的趋势...”.这些提示不仅包含指令，还包含了时间序列的统计特征（比如数据的均值、方差等，被转化为文本描述）。目的： 激活 LLM 内部原本就有的“趋势识别”和“模式匹配”能力。
其实很有意思,一方面输入重编程,利用了llm对于nlp的优势,直接映射数字肯定有效果,但是不如已有的文本数据更全面.另一方面,提示重编程,我们在大模型的基础希望由相关性向因果性靠拢,使之有迹可循.而且提示词本身也可以用文字描述文本信息,进一步增强模型输出的准确性.而且我认为这也是多模态应用的一种可能,比如用文字信息详细的描述出图像的表达内容,比如在人脸识别-情绪识别中,这个"标签"越详细,我们的模型效果在一定程度上就会有所进步,多模态应用中将模态之间互相映射进而最后融合,使得模型可以正确高效的同时处理多模态输入,会对今后的工作很有帮助.
